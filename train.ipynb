{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "import  numpy as np\n",
    "from    config          import *\n",
    "from    agent           import Agent\n",
    "from    enviroment      import Enviroment\n",
    "from    IPython.display import clear_output\n",
    "from    matplotlib      import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = GRID_SIZE\n",
    "num_col = grid_size\n",
    "\n",
    "possibleActions = POSSIBLE_ACTIONS\n",
    "\n",
    "action_space_dict = {\n",
    "    \"U\" : 0,\n",
    "    \"D\" : 1,\n",
    "    \"L\" : 2,\n",
    "    \"R\" : 3,\n",
    "    \"S\" : 4\n",
    "}\n",
    "\n",
    "n_agents          = N_AGENTS\n",
    "allplayerpos      = PLAYER_POS[: n_agents]\n",
    "enemy_list_pos    = ENEMY_POS[: n_agents]\n",
    "batch_size        = BATCH_SIZE\n",
    "replay_memory_len = REPLAY_MEMORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(state_num):\n",
    "    return int(state_num/num_col), state_num%num_col\n",
    "\n",
    "def state_encode(row,col):\n",
    "    return row*num_col + col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    total_step = 0\n",
    "    rewards_list = []\n",
    "    timesteps_list = []\n",
    "\n",
    "    for episode in range(1, EPISODES):\n",
    "        print(\"Episode number: \", episode)\n",
    "\n",
    "        reward_all = 0\n",
    "        time_step = 0\n",
    "        i = 0\n",
    "\n",
    "        for agent in all_agents:\n",
    "            agent.terminal = False\n",
    "        \n",
    "        states = env.reset()\n",
    "\n",
    "        for agent in all_agents:\n",
    "            agent.set_pos(allplayerpos[i])\n",
    "            i = i + 1\n",
    "\n",
    "        done = [False for _ in range(n_agents)]\n",
    "\n",
    "        while not any(done):\n",
    "\n",
    "            # env.render(clear=True)\n",
    "            actions = []\n",
    "\n",
    "            for agent in all_agents:\n",
    "                actions.append(agent.act(states, possibleActions))\n",
    "\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            for agent in all_agents:\n",
    "\n",
    "                next_states = np.array(next_states)\n",
    "                next_states = next_states.ravel()\n",
    "\n",
    "                agent.set_pos(decode_state(next_states[agent.index]))\n",
    "                agent.store(np.concatenate((states,enemy_states)).ravel(), action_space_dict[actions[agent.index]], \\\n",
    "                rewards[agent.index], np.concatenate((next_states,enemy_states)).ravel(), done[agent.index])\n",
    "\n",
    "                if done[agent.index] == True:\n",
    "                    agent.terminal = True\n",
    "                    print(\"agent reached landmark\")\n",
    "                    print(\"updating target model\")\n",
    "                    agent.alighn_target_model()\n",
    "            print(\"actions\", actions)\n",
    "\n",
    "            print(\"agent epsilon \", all_agents[0].epsilon, \"agent memory len\",\\\n",
    "                len(all_agents[0].expirience_replay), \"steps \", time_step,\\\n",
    "                \"reward\", rewards[0], \"next state \", next_states[0], \"agent position \", all_agents[0].return_coordinates())\n",
    "\n",
    "            if time_step >= TIME_STEPS:\n",
    "                print(\"max steps reached\")\n",
    "                break\n",
    "\n",
    "\n",
    "            total_step += 1\n",
    "            time_step += 1\n",
    "            states = next_states\n",
    "            reward_all += sum(rewards)\n",
    "\n",
    "            if time_step % REPLAY_STEPS == 0:\n",
    "                for agent in all_agents:\n",
    "                    agent.retrain()\n",
    "\n",
    "        print(\"total rewards\", reward_all)\n",
    "        for agent in all_agents:\n",
    "            agent.decay_epsilon(episode)\n",
    "\n",
    "        rewards_list.append(reward_all)\n",
    "        timesteps_list.append(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "-\t-\t-\t-\tP\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\tX\t\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_agents= []\n",
    "\n",
    "for index in range(n_agents):\n",
    "\n",
    "    all_agents.append(Agent(index, allplayerpos[index]))\n",
    "\n",
    "\n",
    "initial_states = []\n",
    "for agent in all_agents:\n",
    "    initial_states.append(state_encode(agent.x, agent.y))\n",
    "\n",
    "enemy_states = []\n",
    "for enemy_pos in enemy_list_pos:\n",
    "    enemy_states.append(state_encode(enemy_pos[0], enemy_pos[1]))\n",
    "\n",
    "env = Enviroment(initial_states = initial_states, enemy_states = enemy_states)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  1\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 1 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 2 steps  1 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 3 steps  2 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 4 steps  3 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 5 steps  4 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 6 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 7 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 8 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 9 steps  8 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 10 steps  9 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 11 steps  10 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 12 steps  11 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 13 steps  12 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 14 steps  13 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 15 steps  14 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 16 steps  15 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 17 steps  16 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 18 steps  17 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 19 steps  18 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 20 steps  19 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 21 steps  20 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 22 steps  21 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 23 steps  22 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 24 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 25 steps  24 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 26 steps  25 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 27 steps  26 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 28 steps  27 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 29 steps  28 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 30 steps  29 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 31 steps  30 reward -2 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 32 steps  31 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 33 steps  32 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 34 steps  33 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 35 steps  34 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 36 steps  35 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 37 steps  36 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 38 steps  37 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 39 steps  38 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 40 steps  39 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 41 steps  40 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 42 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 43 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 44 steps  43 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 45 steps  44 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 46 steps  45 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 47 steps  46 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 48 steps  47 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 49 steps  48 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 50 steps  49 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 51 steps  50 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -60\n",
      "Episode number:  2\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 52 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 53 steps  1 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 54 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 55 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 56 steps  4 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 57 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 58 steps  6 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 59 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 60 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 61 steps  9 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 62 steps  10 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 63 steps  11 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 64 steps  12 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 65 steps  13 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 66 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 67 steps  15 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 68 steps  16 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 69 steps  17 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 70 steps  18 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 71 steps  19 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 72 steps  20 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 73 steps  21 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 74 steps  22 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 75 steps  23 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 76 steps  24 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 77 steps  25 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 78 steps  26 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 79 steps  27 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 80 steps  28 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 81 steps  29 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 82 steps  30 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 83 steps  31 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 84 steps  32 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 85 steps  33 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 86 steps  34 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 87 steps  35 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 88 steps  36 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 89 steps  37 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 90 steps  38 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 91 steps  39 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 92 steps  40 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 93 steps  41 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 94 steps  42 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 95 steps  43 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 96 steps  44 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 97 steps  45 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 98 steps  46 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 99 steps  47 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 100 steps  48 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 101 steps  49 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 102 steps  50 reward -1 next state  8 agent position  (1, 3)\n",
      "max steps reached\n",
      "total rewards -61\n",
      "Episode number:  3\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 103 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 104 steps  1 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 105 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 106 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 107 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 108 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 109 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 110 steps  7 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 111 steps  8 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 112 steps  9 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 113 steps  10 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 114 steps  11 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 115 steps  12 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 116 steps  13 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 117 steps  14 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 118 steps  15 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 119 steps  16 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 120 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 121 steps  18 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 122 steps  19 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 123 steps  20 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 124 steps  21 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 125 steps  22 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 126 steps  23 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 127 steps  24 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 128 steps  25 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 129 steps  26 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 130 steps  27 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 131 steps  28 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 132 steps  29 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 133 steps  30 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 134 steps  31 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 135 steps  32 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 136 steps  33 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 137 steps  34 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 138 steps  35 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 139 steps  36 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 140 steps  37 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 141 steps  38 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 142 steps  39 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 143 steps  40 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 144 steps  41 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 145 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 146 steps  43 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 147 steps  44 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 148 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 149 steps  46 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 150 steps  47 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 151 steps  48 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 152 steps  49 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 153 steps  50 reward -1 next state  21 agent position  (4, 1)\n",
      "max steps reached\n",
      "total rewards -58\n",
      "Episode number:  4\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 154 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 155 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 156 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 157 steps  3 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 158 steps  4 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.8621008966608072 agent memory len 159 steps  5 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 160 steps  6 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 161 steps  7 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.8621008966608072 agent memory len 162 steps  8 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 163 steps  9 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 164 steps  10 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 165 steps  11 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 166 steps  12 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 167 steps  13 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 168 steps  14 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 169 steps  15 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 170 steps  16 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 171 steps  17 reward -2 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 172 steps  18 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -8\n",
      "Episode number:  5\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 173 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 174 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 175 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 176 steps  3 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 177 steps  4 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 178 steps  5 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 179 steps  6 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 180 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 181 steps  8 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 182 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 183 steps  10 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 184 steps  11 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 185 steps  12 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 186 steps  13 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 187 steps  14 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 188 steps  15 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 189 steps  16 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 190 steps  17 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 191 steps  18 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 192 steps  19 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 193 steps  20 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 194 steps  21 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 195 steps  22 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 196 steps  23 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 197 steps  24 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 198 steps  25 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 199 steps  26 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 200 steps  27 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 201 steps  28 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 202 steps  29 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 203 steps  30 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 204 steps  31 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 205 steps  32 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 206 steps  33 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 207 steps  34 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 208 steps  35 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 209 steps  36 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 210 steps  37 reward -2 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 211 steps  38 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -38\n",
      "Episode number:  6\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 212 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 213 steps  1 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 214 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 215 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 216 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 217 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 218 steps  6 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 219 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 220 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 221 steps  9 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 222 steps  10 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 223 steps  11 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 224 steps  12 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 225 steps  13 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 226 steps  14 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 227 steps  15 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 228 steps  16 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 229 steps  17 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 230 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 231 steps  19 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 232 steps  20 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 233 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 234 steps  22 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 235 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 236 steps  24 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 237 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 238 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 239 steps  27 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 240 steps  28 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 241 steps  29 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 242 steps  30 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 243 steps  31 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 244 steps  32 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 245 steps  33 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 246 steps  34 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 247 steps  35 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 248 steps  36 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 249 steps  37 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 250 steps  38 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 251 steps  39 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 252 steps  40 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 253 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 254 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 255 steps  43 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 256 steps  44 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 257 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 258 steps  46 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 259 steps  47 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 260 steps  48 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 261 steps  49 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 262 steps  50 reward -2 next state  20 agent position  (4, 0)\n",
      "max steps reached\n",
      "total rewards -60\n",
      "Episode number:  7\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 263 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 264 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 265 steps  2 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 266 steps  3 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 267 steps  4 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 268 steps  5 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 269 steps  6 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 270 steps  7 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 271 steps  8 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 272 steps  9 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 273 steps  10 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 274 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 275 steps  12 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 276 steps  13 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 277 steps  14 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 278 steps  15 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -4\n",
      "Episode number:  8\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 279 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 280 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 281 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 282 steps  3 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 283 steps  4 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 284 steps  5 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 285 steps  6 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 286 steps  7 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 287 steps  8 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 288 steps  9 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 289 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 290 steps  11 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 291 steps  12 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 292 steps  13 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 293 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 294 steps  15 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 295 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 296 steps  17 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 297 steps  18 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 298 steps  19 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 299 steps  20 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 300 steps  21 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 301 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 302 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 303 steps  24 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 304 steps  25 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 305 steps  26 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 306 steps  27 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 307 steps  28 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 308 steps  29 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 309 steps  30 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 310 steps  31 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 311 steps  32 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 312 steps  33 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 313 steps  34 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 314 steps  35 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 315 steps  36 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 316 steps  37 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 317 steps  38 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 318 steps  39 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 319 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 320 steps  41 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 321 steps  42 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 322 steps  43 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 323 steps  44 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 324 steps  45 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 325 steps  46 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 326 steps  47 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 327 steps  48 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 328 steps  49 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 329 steps  50 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -63\n",
      "Episode number:  9\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 330 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 331 steps  1 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 332 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 333 steps  3 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 334 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 335 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 336 steps  6 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 337 steps  7 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 338 steps  8 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 339 steps  9 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 340 steps  10 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 341 steps  11 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 342 steps  12 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 343 steps  13 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 344 steps  14 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 345 steps  15 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 346 steps  16 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 347 steps  17 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 348 steps  18 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 349 steps  19 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 350 steps  20 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 351 steps  21 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 352 steps  22 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 353 steps  23 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 354 steps  24 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 355 steps  25 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 356 steps  26 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 357 steps  27 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 358 steps  28 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 359 steps  29 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 360 steps  30 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 361 steps  31 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 362 steps  32 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 363 steps  33 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 364 steps  34 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 365 steps  35 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 366 steps  36 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 367 steps  37 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 368 steps  38 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 369 steps  39 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 370 steps  40 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 371 steps  41 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 372 steps  42 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 373 steps  43 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 374 steps  44 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 375 steps  45 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 376 steps  46 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 377 steps  47 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 378 steps  48 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 379 steps  49 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 380 steps  50 reward -1 next state  6 agent position  (1, 1)\n",
      "max steps reached\n",
      "total rewards -65\n",
      "Episode number:  10\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 381 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 382 steps  1 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 383 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 384 steps  3 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 385 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 386 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 387 steps  6 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 388 steps  7 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 389 steps  8 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 2\n",
      "Episode number:  11\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 390 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 391 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 392 steps  2 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 393 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 394 steps  4 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 395 steps  5 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 396 steps  6 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 397 steps  7 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 398 steps  8 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 399 steps  9 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 400 steps  10 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 401 steps  11 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 402 steps  12 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 403 steps  13 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 404 steps  14 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 405 steps  15 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 406 steps  16 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 407 steps  17 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 408 steps  18 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 409 steps  19 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 410 steps  20 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 411 steps  21 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 412 steps  22 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 413 steps  23 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 414 steps  24 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 415 steps  25 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 416 steps  26 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 417 steps  27 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 418 steps  28 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 419 steps  29 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 420 steps  30 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 421 steps  31 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 422 steps  32 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 423 steps  33 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 424 steps  34 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 425 steps  35 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 426 steps  36 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 427 steps  37 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 428 steps  38 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 429 steps  39 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 430 steps  40 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 431 steps  41 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 432 steps  42 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 433 steps  43 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 434 steps  44 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 435 steps  45 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 436 steps  46 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 437 steps  47 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 438 steps  48 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 439 steps  49 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 440 steps  50 reward -2 next state  23 agent position  (4, 3)\n",
      "max steps reached\n",
      "total rewards -63\n",
      "Episode number:  12\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 441 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 442 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 443 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 444 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  13\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 445 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 446 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 447 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 448 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  14\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 449 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 450 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 451 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 452 steps  3 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 453 steps  4 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 454 steps  5 reward -2 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 455 steps  6 reward -2 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 456 steps  7 reward -2 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 457 steps  8 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 458 steps  9 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 459 steps  10 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 460 steps  11 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 461 steps  12 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 462 steps  13 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 463 steps  14 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 464 steps  15 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 465 steps  16 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 466 steps  17 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 467 steps  18 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 468 steps  19 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 469 steps  20 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 470 steps  21 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 471 steps  22 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 472 steps  23 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -18\n",
      "Episode number:  15\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 473 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 474 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 475 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 476 steps  3 reward -2 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 477 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  16\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 478 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 479 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 480 steps  2 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 481 steps  3 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 482 steps  4 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 483 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 484 steps  6 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 485 steps  7 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 0\n",
      "Episode number:  17\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 486 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 487 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 488 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 489 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 490 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 491 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 492 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 493 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 494 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 495 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 496 steps  10 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 497 steps  11 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 498 steps  12 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 499 steps  13 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 500 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 501 steps  15 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 502 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 503 steps  17 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 504 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 505 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 506 steps  20 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 507 steps  21 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 508 steps  22 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 509 steps  23 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 510 steps  24 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 511 steps  25 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 512 steps  26 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -28\n",
      "Episode number:  18\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 513 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 514 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 515 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 516 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  19\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 517 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 518 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 519 steps  2 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 520 steps  3 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 521 steps  4 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 522 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 523 steps  6 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 524 steps  7 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 525 steps  8 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 526 steps  9 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 527 steps  10 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 528 steps  11 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 529 steps  12 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 530 steps  13 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 531 steps  14 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 532 steps  15 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 533 steps  16 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 534 steps  17 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 535 steps  18 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 536 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 537 steps  20 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 538 steps  21 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 539 steps  22 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 540 steps  23 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 541 steps  24 reward -2 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 542 steps  25 reward -2 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 543 steps  26 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -27\n",
      "Episode number:  20\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 544 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 545 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 546 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 547 steps  3 reward -2 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 548 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  21\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 549 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 550 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 551 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 552 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  22\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 553 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 554 steps  1 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 555 steps  2 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 556 steps  3 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 557 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  23\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 558 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 559 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 560 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 561 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  24\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 562 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 563 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 564 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 565 steps  3 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 566 steps  4 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 567 steps  5 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 568 steps  6 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 569 steps  7 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 570 steps  8 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 571 steps  9 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 572 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.32347040168526264 agent memory len 573 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 574 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 575 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 576 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 577 steps  15 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.32347040168526264 agent memory len 578 steps  16 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 579 steps  17 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 580 steps  18 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 581 steps  19 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 582 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 583 steps  21 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 584 steps  22 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -18\n",
      "Episode number:  25\n",
      "actions ['R']\n",
      "agent epsilon  0.30818226979308 agent memory len 585 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.30818226979308 agent memory len 586 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.30818226979308 agent memory len 587 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.30818226979308 agent memory len 588 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.30818226979308 agent memory len 589 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 590 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.30818226979308 agent memory len 591 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 592 steps  7 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 593 steps  8 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 594 steps  9 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 595 steps  10 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 596 steps  11 reward -2 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.30818226979308 agent memory len 597 steps  12 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -5\n",
      "Episode number:  26\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 598 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.29363974889158817 agent memory len 599 steps  1 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 600 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 601 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 602 steps  4 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 603 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 604 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 605 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 606 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 607 steps  9 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 608 steps  10 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 609 steps  11 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 610 steps  12 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 611 steps  13 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 612 steps  14 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 613 steps  15 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 614 steps  16 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -8\n",
      "Episode number:  27\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 615 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 616 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 617 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 618 steps  3 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 619 steps  4 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 620 steps  5 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  28\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 621 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 622 steps  1 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 623 steps  2 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 624 steps  3 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 625 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 626 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 627 steps  6 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 628 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 629 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 630 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 631 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 632 steps  11 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 633 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 634 steps  13 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 635 steps  14 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 636 steps  15 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 637 steps  16 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 638 steps  17 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 639 steps  18 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 640 steps  19 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 641 steps  20 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 642 steps  21 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 643 steps  22 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 644 steps  23 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 645 steps  24 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 646 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 647 steps  26 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 648 steps  27 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 649 steps  28 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 650 steps  29 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 651 steps  30 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 652 steps  31 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 653 steps  32 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 654 steps  33 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 655 steps  34 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 656 steps  35 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 657 steps  36 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 658 steps  37 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 659 steps  38 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 660 steps  39 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 661 steps  40 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 662 steps  41 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 663 steps  42 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 664 steps  43 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 665 steps  44 reward -2 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 666 steps  45 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -52\n",
      "Episode number:  29\n",
      "actions ['R']\n",
      "agent epsilon  0.2541309943021904 agent memory len 667 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.2541309943021904 agent memory len 668 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.2541309943021904 agent memory len 669 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.2541309943021904 agent memory len 670 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.2541309943021904 agent memory len 671 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 672 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 673 steps  6 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 674 steps  7 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 675 steps  8 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 676 steps  9 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.2541309943021904 agent memory len 677 steps  10 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.2541309943021904 agent memory len 678 steps  11 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.2541309943021904 agent memory len 679 steps  12 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.2541309943021904 agent memory len 680 steps  13 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.2541309943021904 agent memory len 681 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 682 steps  15 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 683 steps  16 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 684 steps  17 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 685 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 686 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.2541309943021904 agent memory len 687 steps  20 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.2541309943021904 agent memory len 688 steps  21 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -13\n",
      "Episode number:  30\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 689 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 690 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 691 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 692 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 693 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 694 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 695 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 696 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 697 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 698 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 699 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 700 steps  11 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 701 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 702 steps  13 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 703 steps  14 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.24222458521285967 agent memory len 704 steps  15 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 705 steps  16 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 706 steps  17 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 707 steps  18 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 708 steps  19 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 709 steps  20 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 710 steps  21 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 711 steps  22 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.24222458521285967 agent memory len 712 steps  23 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -23\n",
      "Episode number:  31\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 713 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 714 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 715 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 716 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 717 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 718 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 719 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 720 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 721 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 722 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 723 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 724 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 725 steps  12 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 726 steps  13 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 727 steps  14 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 728 steps  15 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 729 steps  16 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 730 steps  17 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 731 steps  18 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 732 steps  19 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 733 steps  20 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 734 steps  21 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 735 steps  22 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 736 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 737 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 738 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 739 steps  26 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 740 steps  27 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 741 steps  28 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 742 steps  29 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 743 steps  30 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 744 steps  31 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 745 steps  32 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 746 steps  33 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 747 steps  34 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 748 steps  35 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 749 steps  36 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 750 steps  37 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 751 steps  38 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 752 steps  39 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 753 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 754 steps  41 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 755 steps  42 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 756 steps  43 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 757 steps  44 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 758 steps  45 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 759 steps  46 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 760 steps  47 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 761 steps  48 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.23089885854694553 agent memory len 762 steps  49 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 763 steps  50 reward -1 next state  5 agent position  (1, 0)\n",
      "max steps reached\n",
      "total rewards -81\n",
      "Episode number:  32\n",
      "actions ['D']\n",
      "agent epsilon  0.22012549408847562 agent memory len 764 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.22012549408847562 agent memory len 765 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.22012549408847562 agent memory len 766 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.22012549408847562 agent memory len 767 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  33\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 768 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 769 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 770 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 771 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  34\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 772 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 773 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 774 steps  2 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 775 steps  3 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 776 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  35\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 777 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 778 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 779 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 780 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 781 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 782 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 783 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 784 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 785 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 786 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 787 steps  10 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 788 steps  11 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 789 steps  12 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 790 steps  13 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 791 steps  14 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 792 steps  15 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 793 steps  16 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 794 steps  17 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 795 steps  18 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 796 steps  19 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -24\n",
      "Episode number:  36\n",
      "actions ['U']\n",
      "agent epsilon  0.1820362040159407 agent memory len 797 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 798 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 799 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 800 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 801 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 802 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 803 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 804 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 805 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 806 steps  9 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 807 steps  10 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 808 steps  11 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 809 steps  12 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 810 steps  13 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -6\n",
      "Episode number:  37\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 811 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 812 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 813 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 814 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  38\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 815 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 816 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 817 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.16566479465049133 agent memory len 818 steps  3 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 819 steps  4 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 820 steps  5 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 7\n",
      "Episode number:  39\n",
      "actions ['D']\n",
      "agent epsilon  0.1580729330304087 agent memory len 821 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.1580729330304087 agent memory len 822 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.1580729330304087 agent memory len 823 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.1580729330304087 agent memory len 824 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  40\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 825 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 826 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 827 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 828 steps  3 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 829 steps  4 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 830 steps  5 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 831 steps  6 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 832 steps  7 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 833 steps  8 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 834 steps  9 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 835 steps  10 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 836 steps  11 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 837 steps  12 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 838 steps  13 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 839 steps  14 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 840 steps  15 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 841 steps  16 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 842 steps  17 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 843 steps  18 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 844 steps  19 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 845 steps  20 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 846 steps  21 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 847 steps  22 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 848 steps  23 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 849 steps  24 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 850 steps  25 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 851 steps  26 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 852 steps  27 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 853 steps  28 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 854 steps  29 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 855 steps  30 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 856 steps  31 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 857 steps  32 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 858 steps  33 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 859 steps  34 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 860 steps  35 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 861 steps  36 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 862 steps  37 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 863 steps  38 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 864 steps  39 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 865 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 866 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 867 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 868 steps  43 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 869 steps  44 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 870 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 871 steps  46 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 872 steps  47 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 873 steps  48 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 874 steps  49 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 875 steps  50 reward -1 next state  10 agent position  (2, 0)\n",
      "max steps reached\n",
      "total rewards -79\n",
      "Episode number:  41\n",
      "actions ['U']\n",
      "agent epsilon  0.1439819304042466 agent memory len 876 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.1439819304042466 agent memory len 877 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.1439819304042466 agent memory len 878 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.1439819304042466 agent memory len 879 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 880 steps  4 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 881 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 882 steps  6 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 883 steps  7 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 884 steps  8 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 885 steps  9 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 886 steps  10 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 887 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 888 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 889 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 890 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 891 steps  15 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 892 steps  16 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 893 steps  17 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 894 steps  18 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 895 steps  19 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 896 steps  20 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 897 steps  21 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 898 steps  22 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 899 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 900 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 901 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.1439819304042466 agent memory len 902 steps  26 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 903 steps  27 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 904 steps  28 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 905 steps  29 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.1439819304042466 agent memory len 906 steps  30 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.1439819304042466 agent memory len 907 steps  31 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.1439819304042466 agent memory len 908 steps  32 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -42\n",
      "Episode number:  42\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 909 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 910 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 911 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 912 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 913 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 914 steps  5 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 915 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 916 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 917 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 918 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 919 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 920 steps  11 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 921 steps  12 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 922 steps  13 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 923 steps  14 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 924 steps  15 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 925 steps  16 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 926 steps  17 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 927 steps  18 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 928 steps  19 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 929 steps  20 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 930 steps  21 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 931 steps  22 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 932 steps  23 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 933 steps  24 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 934 steps  25 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 935 steps  26 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 936 steps  27 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 937 steps  28 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13744755455192614 agent memory len 938 steps  29 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 939 steps  30 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 940 steps  31 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 941 steps  32 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 942 steps  33 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 943 steps  34 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 944 steps  35 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 945 steps  36 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 946 steps  37 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 947 steps  38 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 948 steps  39 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 949 steps  40 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 950 steps  41 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 951 steps  42 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 952 steps  43 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 953 steps  44 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 954 steps  45 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 955 steps  46 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 956 steps  47 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 957 steps  48 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 958 steps  49 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 959 steps  50 reward -1 next state  20 agent position  (4, 0)\n",
      "max steps reached\n",
      "total rewards -66\n",
      "Episode number:  43\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 960 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 961 steps  1 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 962 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 963 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 964 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 965 steps  5 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 966 steps  6 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 967 steps  7 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 968 steps  8 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 969 steps  9 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 970 steps  10 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 971 steps  11 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 972 steps  12 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 973 steps  13 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 974 steps  14 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 975 steps  15 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 976 steps  16 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 977 steps  17 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 978 steps  18 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 979 steps  19 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 980 steps  20 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 981 steps  21 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 982 steps  22 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 983 steps  23 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 984 steps  24 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 985 steps  25 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 986 steps  26 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 987 steps  27 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 988 steps  28 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 989 steps  29 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 990 steps  30 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 991 steps  31 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 992 steps  32 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 993 steps  33 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 994 steps  34 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 995 steps  35 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 996 steps  36 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 997 steps  37 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 998 steps  38 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 999 steps  39 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1000 steps  40 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1001 steps  41 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1002 steps  42 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1003 steps  43 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1004 steps  44 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1005 steps  45 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1006 steps  46 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1007 steps  47 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1008 steps  48 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1009 steps  49 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 1010 steps  50 reward 0 next state  23 agent position  (4, 3)\n",
      "max steps reached\n",
      "total rewards -76\n",
      "Episode number:  44\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1011 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1012 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1013 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1014 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1015 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1016 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1017 steps  6 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1018 steps  7 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1019 steps  8 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1020 steps  9 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1021 steps  10 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1022 steps  11 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1023 steps  12 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1024 steps  13 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1025 steps  14 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1026 steps  15 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1027 steps  16 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1028 steps  17 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1029 steps  18 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1030 steps  19 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1031 steps  20 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1032 steps  21 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1033 steps  22 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1034 steps  23 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1035 steps  24 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1036 steps  25 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1037 steps  26 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1038 steps  27 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1039 steps  28 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1040 steps  29 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1041 steps  30 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1042 steps  31 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1043 steps  32 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1044 steps  33 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1045 steps  34 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1046 steps  35 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1047 steps  36 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1048 steps  37 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1049 steps  38 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 1050 steps  39 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1051 steps  40 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1052 steps  41 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1053 steps  42 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1054 steps  43 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1055 steps  44 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 1056 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1057 steps  46 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1058 steps  47 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1059 steps  48 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 1060 steps  49 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 1061 steps  50 reward -1 next state  1 agent position  (0, 1)\n",
      "max steps reached\n",
      "total rewards -72\n",
      "Episode number:  45\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1062 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1063 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1064 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1065 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1066 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1067 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1068 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1069 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1070 steps  8 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 1071 steps  9 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -4\n",
      "Episode number:  46\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 1072 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 1073 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 1074 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 1075 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  47\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 1076 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 1077 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 1078 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 1079 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  48\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 1080 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 1081 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 1082 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 1083 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  49\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 1084 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 1085 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 1086 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 1087 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  50\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 1088 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 1089 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 1090 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 1091 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  51\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 1092 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 1093 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 1094 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 1095 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  52\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 1096 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 1097 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 1098 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 1099 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  53\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 1100 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 1101 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 1102 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 1103 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  54\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 1104 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 1105 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 1106 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 1107 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  55\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 1108 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 1109 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 1110 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 1111 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  56\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 1112 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 1113 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 1114 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 1115 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  57\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1116 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1117 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1118 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1119 steps  3 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1120 steps  4 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1121 steps  5 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1122 steps  6 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1123 steps  7 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1124 steps  8 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1125 steps  9 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1126 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1127 steps  11 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1128 steps  12 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1129 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1130 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1131 steps  15 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1132 steps  16 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1133 steps  17 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1134 steps  18 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1135 steps  19 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1136 steps  20 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1137 steps  21 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1138 steps  22 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1139 steps  23 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1140 steps  24 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1141 steps  25 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1142 steps  26 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1143 steps  27 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1144 steps  28 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1145 steps  29 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1146 steps  30 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1147 steps  31 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1148 steps  32 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1149 steps  33 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1150 steps  34 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1151 steps  35 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1152 steps  36 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1153 steps  37 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1154 steps  38 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1155 steps  39 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1156 steps  40 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1157 steps  41 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1158 steps  42 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1159 steps  43 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1160 steps  44 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1161 steps  45 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1162 steps  46 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1163 steps  47 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1164 steps  48 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1165 steps  49 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 1166 steps  50 reward 10 next state  24 agent position  (4, 4)\n",
      "max steps reached\n",
      "total rewards -59\n",
      "Episode number:  58\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1167 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1168 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1169 steps  2 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1170 steps  3 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1171 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  59\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1172 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1173 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1174 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1175 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1176 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1177 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1178 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1179 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1180 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1181 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1182 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1183 steps  11 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1184 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1185 steps  13 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1186 steps  14 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1187 steps  15 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1188 steps  16 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1189 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1190 steps  18 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1191 steps  19 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1192 steps  20 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1193 steps  21 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1194 steps  22 reward 0 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1195 steps  23 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -24\n",
      "Episode number:  60\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1196 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1197 steps  1 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1198 steps  2 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1199 steps  3 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1200 steps  4 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1201 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1202 steps  6 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1203 steps  7 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 0\n",
      "Episode number:  61\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1204 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1205 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1206 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1207 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  62\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1208 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1209 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1210 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1211 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  63\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1212 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1213 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1214 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1215 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  64\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1216 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1217 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1218 steps  2 reward -2 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1219 steps  3 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1220 steps  4 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 6\n",
      "Episode number:  65\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1221 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1222 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1223 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1224 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1225 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1226 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1227 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1228 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1229 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1230 steps  9 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1231 steps  10 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1232 steps  11 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1233 steps  12 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1234 steps  13 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1235 steps  14 reward -2 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1236 steps  15 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1237 steps  16 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1238 steps  17 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1239 steps  18 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1240 steps  19 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1241 steps  20 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1242 steps  21 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1243 steps  22 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1244 steps  23 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1245 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1246 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1247 steps  26 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1248 steps  27 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1249 steps  28 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1250 steps  29 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1251 steps  30 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1252 steps  31 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1253 steps  32 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1254 steps  33 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1255 steps  34 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1256 steps  35 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1257 steps  36 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1258 steps  37 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1259 steps  38 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1260 steps  39 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1261 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1262 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1263 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1264 steps  43 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1265 steps  44 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1266 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1267 steps  46 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1268 steps  47 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1269 steps  48 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1270 steps  49 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1271 steps  50 reward -2 next state  20 agent position  (4, 0)\n",
      "max steps reached\n",
      "total rewards -88\n",
      "Episode number:  66\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1272 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1273 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1274 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1275 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  67\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1276 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1277 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1278 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1279 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  68\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1280 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1281 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1282 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1283 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  69\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1284 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1285 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1286 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1287 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  70\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1288 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1289 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1290 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1291 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  71\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1292 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1293 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1294 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1295 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  72\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1296 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1297 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1298 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1299 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  73\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1300 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1301 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1302 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1303 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  74\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1304 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1305 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1306 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1307 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  75\n",
      "actions ['D']\n",
      "agent epsilon  0.034476291205635994 agent memory len 1308 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.034476291205635994 agent memory len 1309 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.034476291205635994 agent memory len 1310 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.034476291205635994 agent memory len 1311 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  76\n",
      "actions ['D']\n",
      "agent epsilon  0.03328256839744902 agent memory len 1312 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03328256839744902 agent memory len 1313 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03328256839744902 agent memory len 1314 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03328256839744902 agent memory len 1315 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  77\n",
      "actions ['D']\n",
      "agent epsilon  0.03214706413760394 agent memory len 1316 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03214706413760394 agent memory len 1317 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03214706413760394 agent memory len 1318 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03214706413760394 agent memory len 1319 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  78\n",
      "actions ['D']\n",
      "agent epsilon  0.031066939073993396 agent memory len 1320 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.031066939073993396 agent memory len 1321 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.031066939073993396 agent memory len 1322 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.031066939073993396 agent memory len 1323 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  79\n",
      "actions ['D']\n",
      "agent epsilon  0.03003949233134634 agent memory len 1324 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03003949233134634 agent memory len 1325 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.03003949233134634 agent memory len 1326 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.03003949233134634 agent memory len 1327 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  80\n",
      "actions ['D']\n",
      "agent epsilon  0.029062154757633053 agent memory len 1328 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.029062154757633053 agent memory len 1329 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.029062154757633053 agent memory len 1330 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.029062154757633053 agent memory len 1331 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  81\n",
      "actions ['D']\n",
      "agent epsilon  0.028132482499846838 agent memory len 1332 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.028132482499846838 agent memory len 1333 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.028132482499846838 agent memory len 1334 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.028132482499846838 agent memory len 1335 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  82\n",
      "actions ['D']\n",
      "agent epsilon  0.02724815089309858 agent memory len 1336 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02724815089309858 agent memory len 1337 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02724815089309858 agent memory len 1338 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.02724815089309858 agent memory len 1339 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  83\n",
      "actions ['D']\n",
      "agent epsilon  0.026406948647743622 agent memory len 1340 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.026406948647743622 agent memory len 1341 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.026406948647743622 agent memory len 1342 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.026406948647743622 agent memory len 1343 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  84\n",
      "actions ['D']\n",
      "agent epsilon  0.025606772320005942 agent memory len 1344 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.025606772320005942 agent memory len 1345 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.025606772320005942 agent memory len 1346 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.025606772320005942 agent memory len 1347 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  85\n",
      "actions ['D']\n",
      "agent epsilon  0.024845621052272927 agent memory len 1348 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.024845621052272927 agent memory len 1349 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.024845621052272927 agent memory len 1350 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.024845621052272927 agent memory len 1351 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  86\n",
      "actions ['D']\n",
      "agent epsilon  0.024121591569909263 agent memory len 1352 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.024121591569909263 agent memory len 1353 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.024121591569909263 agent memory len 1354 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.024121591569909263 agent memory len 1355 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  87\n",
      "actions ['D']\n",
      "agent epsilon  0.023432873422078924 agent memory len 1356 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.023432873422078924 agent memory len 1357 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.023432873422078924 agent memory len 1358 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.023432873422078924 agent memory len 1359 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  88\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1360 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1361 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1362 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1363 steps  3 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1364 steps  4 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1365 steps  5 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1366 steps  6 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1367 steps  7 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1368 steps  8 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1369 steps  9 reward -2 next state  23 agent position  (4, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.022777744454675064 agent memory len 1370 steps  10 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -3\n",
      "Episode number:  89\n",
      "actions ['R']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1371 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1372 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1373 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1374 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1375 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1376 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1377 steps  6 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1378 steps  7 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.02215456650403775 agent memory len 1379 steps  8 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -2\n",
      "Episode number:  90\n",
      "actions ['D']\n",
      "agent epsilon  0.021561781300691488 agent memory len 1380 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.021561781300691488 agent memory len 1381 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.021561781300691488 agent memory len 1382 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.021561781300691488 agent memory len 1383 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  91\n",
      "actions ['D']\n",
      "agent epsilon  0.02099790657285988 agent memory len 1384 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02099790657285988 agent memory len 1385 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02099790657285988 agent memory len 1386 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.02099790657285988 agent memory len 1387 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  92\n",
      "actions ['D']\n",
      "agent epsilon  0.02046153234001413 agent memory len 1388 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02046153234001413 agent memory len 1389 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.02046153234001413 agent memory len 1390 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.02046153234001413 agent memory len 1391 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  93\n",
      "actions ['D']\n",
      "agent epsilon  0.01995131738718724 agent memory len 1392 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01995131738718724 agent memory len 1393 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01995131738718724 agent memory len 1394 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01995131738718724 agent memory len 1395 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  94\n",
      "actions ['D']\n",
      "agent epsilon  0.01946598591123807 agent memory len 1396 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01946598591123807 agent memory len 1397 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01946598591123807 agent memory len 1398 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01946598591123807 agent memory len 1399 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  95\n",
      "actions ['D']\n",
      "agent epsilon  0.019004324330678855 agent memory len 1400 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.019004324330678855 agent memory len 1401 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.019004324330678855 agent memory len 1402 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.019004324330678855 agent memory len 1403 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  96\n",
      "actions ['D']\n",
      "agent epsilon  0.01856517825108943 agent memory len 1404 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01856517825108943 agent memory len 1405 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01856517825108943 agent memory len 1406 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01856517825108943 agent memory len 1407 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  97\n",
      "actions ['D']\n",
      "agent epsilon  0.018147449578529824 agent memory len 1408 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.018147449578529824 agent memory len 1409 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.018147449578529824 agent memory len 1410 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.018147449578529824 agent memory len 1411 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  98\n",
      "actions ['D']\n",
      "agent epsilon  0.01775009377373351 agent memory len 1412 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01775009377373351 agent memory len 1413 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01775009377373351 agent memory len 1414 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01775009377373351 agent memory len 1415 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  99\n",
      "actions ['D']\n",
      "agent epsilon  0.017372117240215094 agent memory len 1416 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.017372117240215094 agent memory len 1417 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.017372117240215094 agent memory len 1418 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.017372117240215094 agent memory len 1419 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  100\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1420 steps  0 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1421 steps  1 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1422 steps  2 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1423 steps  3 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1424 steps  4 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1425 steps  5 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1426 steps  6 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1427 steps  7 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1428 steps  8 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1429 steps  9 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1430 steps  10 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1431 steps  11 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1432 steps  12 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1433 steps  13 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1434 steps  14 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1435 steps  15 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1436 steps  16 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1437 steps  17 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1438 steps  18 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1439 steps  19 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1440 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1441 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1442 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1443 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1444 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1445 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1446 steps  26 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1447 steps  27 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1448 steps  28 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1449 steps  29 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1450 steps  30 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1451 steps  31 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1452 steps  32 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1453 steps  33 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1454 steps  34 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1455 steps  35 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1456 steps  36 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1457 steps  37 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1458 steps  38 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1459 steps  39 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1460 steps  40 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1461 steps  41 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1462 steps  42 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1463 steps  43 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1464 steps  44 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1465 steps  45 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1466 steps  46 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1467 steps  47 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1468 steps  48 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1469 steps  49 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.017012574839761596 agent memory len 1470 steps  50 reward -2 next state  4 agent position  (0, 4)\n",
      "max steps reached\n",
      "total rewards -71\n",
      "Episode number:  101\n",
      "actions ['U']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1471 steps  0 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1472 steps  1 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1473 steps  2 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1474 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1475 steps  4 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1476 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1477 steps  6 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1478 steps  7 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.016670567529094613 agent memory len 1479 steps  8 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -2\n",
      "Episode number:  102\n",
      "actions ['D']\n",
      "agent epsilon  0.016345240111793814 agent memory len 1480 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016345240111793814 agent memory len 1481 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016345240111793814 agent memory len 1482 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.016345240111793814 agent memory len 1483 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  103\n",
      "actions ['D']\n",
      "agent epsilon  0.016035779099860478 agent memory len 1484 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016035779099860478 agent memory len 1485 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.016035779099860478 agent memory len 1486 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.016035779099860478 agent memory len 1487 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  104\n",
      "actions ['D']\n",
      "agent epsilon  0.01574141067957372 agent memory len 1488 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01574141067957372 agent memory len 1489 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01574141067957372 agent memory len 1490 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01574141067957372 agent memory len 1491 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  105\n",
      "actions ['D']\n",
      "agent epsilon  0.015461398776553163 agent memory len 1492 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.015461398776553163 agent memory len 1493 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.015461398776553163 agent memory len 1494 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.015461398776553163 agent memory len 1495 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  106\n",
      "actions ['D']\n",
      "agent epsilon  0.015195043215189571 agent memory len 1496 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.015195043215189571 agent memory len 1497 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.015195043215189571 agent memory len 1498 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.015195043215189571 agent memory len 1499 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  107\n",
      "actions ['D']\n",
      "agent epsilon  0.01494167796784111 agent memory len 1500 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01494167796784111 agent memory len 1501 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01494167796784111 agent memory len 1502 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01494167796784111 agent memory len 1503 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  108\n",
      "actions ['D']\n",
      "agent epsilon  0.014700669489417359 agent memory len 1504 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014700669489417359 agent memory len 1505 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014700669489417359 agent memory len 1506 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.014700669489417359 agent memory len 1507 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  109\n",
      "actions ['D']\n",
      "agent epsilon  0.01447141513318654 agent memory len 1508 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01447141513318654 agent memory len 1509 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01447141513318654 agent memory len 1510 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01447141513318654 agent memory len 1511 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  110\n",
      "actions ['D']\n",
      "agent epsilon  0.014253341643844817 agent memory len 1512 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014253341643844817 agent memory len 1513 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014253341643844817 agent memory len 1514 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.014253341643844817 agent memory len 1515 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  111\n",
      "actions ['D']\n",
      "agent epsilon  0.014045903724079427 agent memory len 1516 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014045903724079427 agent memory len 1517 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.014045903724079427 agent memory len 1518 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.014045903724079427 agent memory len 1519 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  112\n",
      "actions ['D']\n",
      "agent epsilon  0.013848582671041366 agent memory len 1520 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.013848582671041366 agent memory len 1521 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.013848582671041366 agent memory len 1522 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.013848582671041366 agent memory len 1523 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  113\n",
      "actions ['D']\n",
      "agent epsilon  0.0136608850793181 agent memory len 1524 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0136608850793181 agent memory len 1525 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.0136608850793181 agent memory len 1526 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.0136608850793181 agent memory len 1527 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  114\n",
      "actions ['D']\n",
      "agent epsilon  0.013482341607163007 agent memory len 1528 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.013482341607163007 agent memory len 1529 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.013482341607163007 agent memory len 1530 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.013482341607163007 agent memory len 1531 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  115\n",
      "actions ['D']\n",
      "agent epsilon  0.01331250580289656 agent memory len 1532 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01331250580289656 agent memory len 1533 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01331250580289656 agent memory len 1534 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01331250580289656 agent memory len 1535 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  116\n",
      "actions ['D']\n",
      "agent epsilon  0.01315095298854457 agent memory len 1536 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01315095298854457 agent memory len 1537 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01315095298854457 agent memory len 1538 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01315095298854457 agent memory len 1539 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  117\n",
      "actions ['D']\n",
      "agent epsilon  0.012997279197922054 agent memory len 1540 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012997279197922054 agent memory len 1541 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012997279197922054 agent memory len 1542 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012997279197922054 agent memory len 1543 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  118\n",
      "actions ['D']\n",
      "agent epsilon  0.012851100166507357 agent memory len 1544 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012851100166507357 agent memory len 1545 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012851100166507357 agent memory len 1546 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012851100166507357 agent memory len 1547 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  119\n",
      "actions ['D']\n",
      "agent epsilon  0.012712050370580685 agent memory len 1548 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012712050370580685 agent memory len 1549 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012712050370580685 agent memory len 1550 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012712050370580685 agent memory len 1551 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  120\n",
      "actions ['D']\n",
      "agent epsilon  0.012579782113224414 agent memory len 1552 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012579782113224414 agent memory len 1553 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012579782113224414 agent memory len 1554 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012579782113224414 agent memory len 1555 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  121\n",
      "actions ['D']\n",
      "agent epsilon  0.012453964654899695 agent memory len 1556 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012453964654899695 agent memory len 1557 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012453964654899695 agent memory len 1558 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012453964654899695 agent memory len 1559 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  122\n",
      "actions ['D']\n",
      "agent epsilon  0.012334283386425329 agent memory len 1560 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012334283386425329 agent memory len 1561 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012334283386425329 agent memory len 1562 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012334283386425329 agent memory len 1563 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  123\n",
      "actions ['D']\n",
      "agent epsilon  0.012220439042290943 agent memory len 1564 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012220439042290943 agent memory len 1565 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012220439042290943 agent memory len 1566 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012220439042290943 agent memory len 1567 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  124\n",
      "actions ['D']\n",
      "agent epsilon  0.012112146952337331 agent memory len 1568 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012112146952337331 agent memory len 1569 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012112146952337331 agent memory len 1570 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012112146952337331 agent memory len 1571 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  125\n",
      "actions ['D']\n",
      "agent epsilon  0.012009136329932777 agent memory len 1572 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012009136329932777 agent memory len 1573 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.012009136329932777 agent memory len 1574 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.012009136329932777 agent memory len 1575 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  126\n",
      "actions ['D']\n",
      "agent epsilon  0.011911149594865432 agent memory len 1576 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011911149594865432 agent memory len 1577 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011911149594865432 agent memory len 1578 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011911149594865432 agent memory len 1579 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  127\n",
      "actions ['D']\n",
      "agent epsilon  0.011817941729258617 agent memory len 1580 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011817941729258617 agent memory len 1581 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011817941729258617 agent memory len 1582 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011817941729258617 agent memory len 1583 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  128\n",
      "actions ['D']\n",
      "agent epsilon  0.011729279664898506 agent memory len 1584 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011729279664898506 agent memory len 1585 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011729279664898506 agent memory len 1586 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011729279664898506 agent memory len 1587 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  129\n",
      "actions ['D']\n",
      "agent epsilon  0.011644941700442194 agent memory len 1588 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011644941700442194 agent memory len 1589 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011644941700442194 agent memory len 1590 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011644941700442194 agent memory len 1591 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  130\n",
      "actions ['D']\n",
      "agent epsilon  0.011564716947048855 agent memory len 1592 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011564716947048855 agent memory len 1593 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011564716947048855 agent memory len 1594 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011564716947048855 agent memory len 1595 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  131\n",
      "actions ['D']\n",
      "agent epsilon  0.011488404801047796 agent memory len 1596 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011488404801047796 agent memory len 1597 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011488404801047796 agent memory len 1598 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011488404801047796 agent memory len 1599 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  132\n",
      "actions ['D']\n",
      "agent epsilon  0.011415814442324795 agent memory len 1600 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011415814442324795 agent memory len 1601 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011415814442324795 agent memory len 1602 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011415814442324795 agent memory len 1603 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  133\n",
      "actions ['D']\n",
      "agent epsilon  0.011346764357172414 agent memory len 1604 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011346764357172414 agent memory len 1605 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011346764357172414 agent memory len 1606 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011346764357172414 agent memory len 1607 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  134\n",
      "actions ['D']\n",
      "agent epsilon  0.01128108188441119 agent memory len 1608 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01128108188441119 agent memory len 1609 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01128108188441119 agent memory len 1610 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01128108188441119 agent memory len 1611 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  135\n",
      "actions ['D']\n",
      "agent epsilon  0.011218602783646746 agent memory len 1612 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011218602783646746 agent memory len 1613 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011218602783646746 agent memory len 1614 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011218602783646746 agent memory len 1615 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  136\n",
      "actions ['D']\n",
      "agent epsilon  0.011159170824583262 agent memory len 1616 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011159170824583262 agent memory len 1617 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011159170824583262 agent memory len 1618 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011159170824583262 agent memory len 1619 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  137\n",
      "actions ['D']\n",
      "agent epsilon  0.011102637396366355 agent memory len 1620 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011102637396366355 agent memory len 1621 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011102637396366355 agent memory len 1622 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011102637396366355 agent memory len 1623 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  138\n",
      "actions ['D']\n",
      "agent epsilon  0.011048861135978534 agent memory len 1624 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011048861135978534 agent memory len 1625 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.011048861135978534 agent memory len 1626 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.011048861135978534 agent memory len 1627 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  139\n",
      "actions ['D']\n",
      "agent epsilon  0.010997707574758025 agent memory len 1628 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010997707574758025 agent memory len 1629 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010997707574758025 agent memory len 1630 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010997707574758025 agent memory len 1631 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  140\n",
      "actions ['D']\n",
      "agent epsilon  0.01094904880215708 agent memory len 1632 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01094904880215708 agent memory len 1633 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01094904880215708 agent memory len 1634 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01094904880215708 agent memory len 1635 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  141\n",
      "actions ['D']\n",
      "agent epsilon  0.010902763145898971 agent memory len 1636 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010902763145898971 agent memory len 1637 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010902763145898971 agent memory len 1638 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010902763145898971 agent memory len 1639 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  142\n",
      "actions ['D']\n",
      "agent epsilon  0.010858734867733932 agent memory len 1640 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010858734867733932 agent memory len 1641 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010858734867733932 agent memory len 1642 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010858734867733932 agent memory len 1643 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  143\n",
      "actions ['D']\n",
      "agent epsilon  0.010816853874033245 agent memory len 1644 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010816853874033245 agent memory len 1645 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010816853874033245 agent memory len 1646 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010816853874033245 agent memory len 1647 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  144\n",
      "actions ['D']\n",
      "agent epsilon  0.010777015440497823 agent memory len 1648 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010777015440497823 agent memory len 1649 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010777015440497823 agent memory len 1650 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010777015440497823 agent memory len 1651 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  145\n",
      "actions ['D']\n",
      "agent epsilon  0.010739119950292912 agent memory len 1652 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010739119950292912 agent memory len 1653 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010739119950292912 agent memory len 1654 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010739119950292912 agent memory len 1655 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  146\n",
      "actions ['D']\n",
      "agent epsilon  0.010703072644954124 agent memory len 1656 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010703072644954124 agent memory len 1657 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010703072644954124 agent memory len 1658 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010703072644954124 agent memory len 1659 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  147\n",
      "actions ['D']\n",
      "agent epsilon  0.010668783387441906 agent memory len 1660 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010668783387441906 agent memory len 1661 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010668783387441906 agent memory len 1662 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010668783387441906 agent memory len 1663 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  148\n",
      "actions ['D']\n",
      "agent epsilon  0.010636166436752002 agent memory len 1664 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010636166436752002 agent memory len 1665 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010636166436752002 agent memory len 1666 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010636166436752002 agent memory len 1667 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  149\n",
      "actions ['D']\n",
      "agent epsilon  0.010605140233518277 agent memory len 1668 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010605140233518277 agent memory len 1669 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010605140233518277 agent memory len 1670 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010605140233518277 agent memory len 1671 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  150\n",
      "actions ['D']\n",
      "agent epsilon  0.01057562719607182 agent memory len 1672 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01057562719607182 agent memory len 1673 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01057562719607182 agent memory len 1674 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01057562719607182 agent memory len 1675 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  151\n",
      "actions ['D']\n",
      "agent epsilon  0.010547553526446355 agent memory len 1676 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010547553526446355 agent memory len 1677 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010547553526446355 agent memory len 1678 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010547553526446355 agent memory len 1679 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  152\n",
      "actions ['D']\n",
      "agent epsilon  0.010520849025844903 agent memory len 1680 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010520849025844903 agent memory len 1681 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010520849025844903 agent memory len 1682 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010520849025844903 agent memory len 1683 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  153\n",
      "actions ['D']\n",
      "agent epsilon  0.010495446919106205 agent memory len 1684 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010495446919106205 agent memory len 1685 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010495446919106205 agent memory len 1686 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010495446919106205 agent memory len 1687 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  154\n",
      "actions ['D']\n",
      "agent epsilon  0.010471283687732046 agent memory len 1688 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010471283687732046 agent memory len 1689 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010471283687732046 agent memory len 1690 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010471283687732046 agent memory len 1691 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  155\n",
      "actions ['D']\n",
      "agent epsilon  0.010448298911057929 agent memory len 1692 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010448298911057929 agent memory len 1693 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010448298911057929 agent memory len 1694 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010448298911057929 agent memory len 1695 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  156\n",
      "actions ['D']\n",
      "agent epsilon  0.01042643511516993 agent memory len 1696 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01042643511516993 agent memory len 1697 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01042643511516993 agent memory len 1698 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01042643511516993 agent memory len 1699 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  157\n",
      "actions ['D']\n",
      "agent epsilon  0.010405637629189989 agent memory len 1700 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010405637629189989 agent memory len 1701 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010405637629189989 agent memory len 1702 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010405637629189989 agent memory len 1703 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  158\n",
      "actions ['D']\n",
      "agent epsilon  0.010385854448570228 agent memory len 1704 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010385854448570228 agent memory len 1705 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010385854448570228 agent memory len 1706 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010385854448570228 agent memory len 1707 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  159\n",
      "actions ['D']\n",
      "agent epsilon  0.010367036105054498 agent memory len 1708 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010367036105054498 agent memory len 1709 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010367036105054498 agent memory len 1710 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010367036105054498 agent memory len 1711 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  160\n",
      "actions ['D']\n",
      "agent epsilon  0.010349135542981974 agent memory len 1712 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010349135542981974 agent memory len 1713 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010349135542981974 agent memory len 1714 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010349135542981974 agent memory len 1715 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  161\n",
      "actions ['D']\n",
      "agent epsilon  0.010332108001623487 agent memory len 1716 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010332108001623487 agent memory len 1717 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010332108001623487 agent memory len 1718 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010332108001623487 agent memory len 1719 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  162\n",
      "actions ['D']\n",
      "agent epsilon  0.010315910903256391 agent memory len 1720 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010315910903256391 agent memory len 1721 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010315910903256391 agent memory len 1722 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010315910903256391 agent memory len 1723 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  163\n",
      "actions ['D']\n",
      "agent epsilon  0.01030050374669808 agent memory len 1724 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01030050374669808 agent memory len 1725 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01030050374669808 agent memory len 1726 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01030050374669808 agent memory len 1727 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  164\n",
      "actions ['D']\n",
      "agent epsilon  0.010285848006031922 agent memory len 1728 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010285848006031922 agent memory len 1729 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010285848006031922 agent memory len 1730 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010285848006031922 agent memory len 1731 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  165\n",
      "actions ['D']\n",
      "agent epsilon  0.01027190703427242 agent memory len 1732 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01027190703427242 agent memory len 1733 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01027190703427242 agent memory len 1734 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01027190703427242 agent memory len 1735 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  166\n",
      "actions ['D']\n",
      "agent epsilon  0.010258645971728651 agent memory len 1736 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010258645971728651 agent memory len 1737 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010258645971728651 agent memory len 1738 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010258645971728651 agent memory len 1739 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  167\n",
      "actions ['D']\n",
      "agent epsilon  0.010246031658836873 agent memory len 1740 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010246031658836873 agent memory len 1741 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010246031658836873 agent memory len 1742 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010246031658836873 agent memory len 1743 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  168\n",
      "actions ['D']\n",
      "agent epsilon  0.010234032553244354 agent memory len 1744 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010234032553244354 agent memory len 1745 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010234032553244354 agent memory len 1746 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010234032553244354 agent memory len 1747 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  169\n",
      "actions ['D']\n",
      "agent epsilon  0.01022261865093706 agent memory len 1748 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01022261865093706 agent memory len 1749 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01022261865093706 agent memory len 1750 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01022261865093706 agent memory len 1751 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  170\n",
      "actions ['D']\n",
      "agent epsilon  0.010211761411213984 agent memory len 1752 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010211761411213984 agent memory len 1753 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010211761411213984 agent memory len 1754 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010211761411213984 agent memory len 1755 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  171\n",
      "actions ['D']\n",
      "agent epsilon  0.010201433685320538 agent memory len 1756 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010201433685320538 agent memory len 1757 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010201433685320538 agent memory len 1758 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010201433685320538 agent memory len 1759 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  172\n",
      "actions ['D']\n",
      "agent epsilon  0.010191609648562514 agent memory len 1760 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010191609648562514 agent memory len 1761 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010191609648562514 agent memory len 1762 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010191609648562514 agent memory len 1763 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  173\n",
      "actions ['D']\n",
      "agent epsilon  0.010182264735730903 agent memory len 1764 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010182264735730903 agent memory len 1765 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010182264735730903 agent memory len 1766 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010182264735730903 agent memory len 1767 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  174\n",
      "actions ['D']\n",
      "agent epsilon  0.010173375579676082 agent memory len 1768 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010173375579676082 agent memory len 1769 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010173375579676082 agent memory len 1770 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010173375579676082 agent memory len 1771 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  175\n",
      "actions ['D']\n",
      "agent epsilon  0.010164919952877757 agent memory len 1772 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010164919952877757 agent memory len 1773 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010164919952877757 agent memory len 1774 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010164919952877757 agent memory len 1775 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  176\n",
      "actions ['D']\n",
      "agent epsilon  0.010156876711864594 agent memory len 1776 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010156876711864594 agent memory len 1777 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010156876711864594 agent memory len 1778 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010156876711864594 agent memory len 1779 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  177\n",
      "actions ['D']\n",
      "agent epsilon  0.010149225744344521 agent memory len 1780 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010149225744344521 agent memory len 1781 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010149225744344521 agent memory len 1782 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010149225744344521 agent memory len 1783 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  178\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1784 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1785 steps  1 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1786 steps  2 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1787 steps  3 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1788 steps  4 reward 0 next state  19 agent position  (3, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1789 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1790 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1791 steps  7 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1792 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1793 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1794 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1795 steps  11 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1796 steps  12 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01014194791891353 agent memory len 1797 steps  13 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards -3\n",
      "Episode number:  179\n",
      "actions ['D']\n",
      "agent epsilon  0.010135025037217192 agent memory len 1798 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010135025037217192 agent memory len 1799 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010135025037217192 agent memory len 1800 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010135025037217192 agent memory len 1801 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  180\n",
      "actions ['D']\n",
      "agent epsilon  0.010128439788445296 agent memory len 1802 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010128439788445296 agent memory len 1803 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010128439788445296 agent memory len 1804 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010128439788445296 agent memory len 1805 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  181\n",
      "actions ['D']\n",
      "agent epsilon  0.010122175706045813 agent memory len 1806 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010122175706045813 agent memory len 1807 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010122175706045813 agent memory len 1808 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010122175706045813 agent memory len 1809 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  182\n",
      "actions ['D']\n",
      "agent epsilon  0.010116217126549927 agent memory len 1810 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010116217126549927 agent memory len 1811 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010116217126549927 agent memory len 1812 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010116217126549927 agent memory len 1813 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  183\n",
      "actions ['D']\n",
      "agent epsilon  0.010110549150405214 agent memory len 1814 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010110549150405214 agent memory len 1815 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010110549150405214 agent memory len 1816 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010110549150405214 agent memory len 1817 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  184\n",
      "actions ['D']\n",
      "agent epsilon  0.010105157604718994 agent memory len 1818 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010105157604718994 agent memory len 1819 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010105157604718994 agent memory len 1820 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010105157604718994 agent memory len 1821 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  185\n",
      "actions ['D']\n",
      "agent epsilon  0.010100029007818723 agent memory len 1822 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010100029007818723 agent memory len 1823 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010100029007818723 agent memory len 1824 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010100029007818723 agent memory len 1825 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  186\n",
      "actions ['D']\n",
      "agent epsilon  0.01009515053554078 agent memory len 1826 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01009515053554078 agent memory len 1827 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01009515053554078 agent memory len 1828 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01009515053554078 agent memory len 1829 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  187\n",
      "actions ['D']\n",
      "agent epsilon  0.010090509989163391 agent memory len 1830 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010090509989163391 agent memory len 1831 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010090509989163391 agent memory len 1832 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010090509989163391 agent memory len 1833 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  188\n",
      "actions ['D']\n",
      "agent epsilon  0.01008609576490346 agent memory len 1834 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01008609576490346 agent memory len 1835 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01008609576490346 agent memory len 1836 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01008609576490346 agent memory len 1837 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  189\n",
      "actions ['D']\n",
      "agent epsilon  0.010081896824901066 agent memory len 1838 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010081896824901066 agent memory len 1839 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010081896824901066 agent memory len 1840 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010081896824901066 agent memory len 1841 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  190\n",
      "actions ['D']\n",
      "agent epsilon  0.010077902669619077 agent memory len 1842 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010077902669619077 agent memory len 1843 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010077902669619077 agent memory len 1844 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010077902669619077 agent memory len 1845 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  191\n",
      "actions ['D']\n",
      "agent epsilon  0.010074103311588823 agent memory len 1846 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010074103311588823 agent memory len 1847 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010074103311588823 agent memory len 1848 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010074103311588823 agent memory len 1849 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  192\n",
      "actions ['D']\n",
      "agent epsilon  0.010070489250436234 agent memory len 1850 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010070489250436234 agent memory len 1851 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010070489250436234 agent memory len 1852 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010070489250436234 agent memory len 1853 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  193\n",
      "actions ['D']\n",
      "agent epsilon  0.010067051449125945 agent memory len 1854 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010067051449125945 agent memory len 1855 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010067051449125945 agent memory len 1856 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010067051449125945 agent memory len 1857 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  194\n",
      "actions ['D']\n",
      "agent epsilon  0.010063781311364012 agent memory len 1858 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010063781311364012 agent memory len 1859 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010063781311364012 agent memory len 1860 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010063781311364012 agent memory len 1861 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  195\n",
      "actions ['D']\n",
      "agent epsilon  0.01006067066010269 agent memory len 1862 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01006067066010269 agent memory len 1863 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01006067066010269 agent memory len 1864 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01006067066010269 agent memory len 1865 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  196\n",
      "actions ['D']\n",
      "agent epsilon  0.01005771171709356 agent memory len 1866 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01005771171709356 agent memory len 1867 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01005771171709356 agent memory len 1868 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01005771171709356 agent memory len 1869 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  197\n",
      "actions ['D']\n",
      "agent epsilon  0.010054897083437856 agent memory len 1870 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010054897083437856 agent memory len 1871 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010054897083437856 agent memory len 1872 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010054897083437856 agent memory len 1873 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  198\n",
      "actions ['D']\n",
      "agent epsilon  0.01005221972108536 agent memory len 1874 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01005221972108536 agent memory len 1875 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.01005221972108536 agent memory len 1876 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.01005221972108536 agent memory len 1877 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n",
      "Episode number:  199\n",
      "actions ['D']\n",
      "agent epsilon  0.010049672935235614 agent memory len 1878 steps  0 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010049672935235614 agent memory len 1879 steps  1 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.010049672935235614 agent memory len 1880 steps  2 reward 0 next state  19 agent position  (3, 4)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.010049672935235614 agent memory len 1881 steps  3 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 8\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.step(\"D\"))\n",
    "# env.render()\n",
    "# EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agents[0].save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
