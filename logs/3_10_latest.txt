Episode number:  1
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 1 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 1 steps  1 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 1 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 2 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 2 steps  2 reward -1 next state  0 agent position  (0, 8)
agent epsilon  1.0 agent memory len 2 steps  2 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'R']
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  0 agent position  (2, 7)
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'R']
agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  0 agent position  (3, 7)
agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  0 agent position  (3, 7)
agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 8 steps  8 reward -2 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 8 steps  8 reward 0 next state  0 agent position  (3, 6)
agent epsilon  1.0 agent memory len 8 steps  8 reward -1 next state  3 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 9 steps  9 reward -2 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 9 steps  9 reward 0 next state  0 agent position  (3, 5)
agent epsilon  1.0 agent memory len 9 steps  9 reward -1 next state  3 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 10 steps  10 reward 0 next state  0 agent position  (3, 4)
agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  3 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'R']
agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 11 steps  11 reward 0 next state  1 agent position  (3, 3)
agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 12 steps  12 reward 0 next state  2 agent position  (3, 4)
agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 13 steps  13 reward 10 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 14 steps  14 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 13 steps  14 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 14 steps  14 reward -2 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 13 steps  15 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 13 steps  16 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  4 agent position  (9, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 13 steps  17 reward 0 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 18 steps  18 reward -2 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 13 steps  18 reward 0 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 18 steps  18 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 13 steps  19 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 20 steps  20 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 13 steps  20 reward 0 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 20 steps  20 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 13 steps  21 reward 0 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  4 agent position  (9, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 13 steps  22 reward 0 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 22 steps  22 reward -2 next state  4 agent position  (9, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 13 steps  23 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 13 steps  24 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  4 agent position  (9, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 13 steps  25 reward 0 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  4 agent position  (9, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 13 steps  26 reward 0 next state  2 agent position  (4, 4)
agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 13 steps  27 reward 0 next state  2 agent position  (4, 4)
agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  4 agent position  (8, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 28 steps  28 reward 0 next state  4 agent position  (4, 3)
agent epsilon  1.0 agent memory len 13 steps  28 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 28 steps  28 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 13 steps  29 reward 0 next state  2 agent position  (4, 4)
agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 13 steps  30 reward 0 next state  2 agent position  (4, 4)
agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 31 steps  31 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 13 steps  31 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 31 steps  31 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 13 steps  32 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 33 steps  33 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 13 steps  33 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 33 steps  33 reward -2 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 13 steps  34 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  4 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  1.0 agent memory len 35 steps  35 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 13 steps  35 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 35 steps  35 reward -1 next state  4 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 36 steps  36 reward 0 next state  3 agent position  (3, 4)
agent epsilon  1.0 agent memory len 13 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 36 steps  36 reward -2 next state  4 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 13 steps  37 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  4 agent position  (6, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 38 steps  38 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 13 steps  38 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 38 steps  38 reward -1 next state  4 agent position  (5, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 39 steps  39 reward 0 next state  4 agent position  (4, 3)
agent epsilon  1.0 agent memory len 13 steps  39 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 39 steps  39 reward -1 next state  4 agent position  (5, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  1.0 agent memory len 40 steps  40 reward 0 next state  5 agent position  (5, 3)
agent epsilon  1.0 agent memory len 13 steps  40 reward 0 next state  3 agent position  (4, 4)
agent epsilon  1.0 agent memory len 40 steps  40 reward -1 next state  4 agent position  (6, 8)
 is_terminal [False, True, False]
random action 0
random action 2
landmark captured 2
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  41 reward 10 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 41 steps  41 reward -1 next state  4 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  42 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 42 steps  42 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  43 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 43 steps  43 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  44 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 44 steps  44 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  45 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 45 steps  45 reward -1 next state  4 agent position  (3, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  46 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 46 steps  46 reward -1 next state  4 agent position  (3, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 41 steps  47 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 47 steps  47 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  48 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 48 steps  48 reward -1 next state  4 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  49 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 49 steps  49 reward -1 next state  4 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  50 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 50 steps  50 reward -1 next state  4 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  51 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 51 steps  51 reward -1 next state  4 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 41 steps  52 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 52 steps  52 reward -1 next state  4 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 41 steps  53 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 53 steps  53 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  54 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 54 steps  54 reward -1 next state  4 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 41 steps  55 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 55 steps  55 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  56 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 56 steps  56 reward -1 next state  4 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 41 steps  57 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 57 steps  57 reward -1 next state  4 agent position  (2, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  58 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 58 steps  58 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  59 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 59 steps  59 reward -1 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  60 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 60 steps  60 reward -1 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  61 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 61 steps  61 reward -1 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 41 steps  62 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 62 steps  62 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  63 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 63 steps  63 reward -1 next state  4 agent position  (1, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  64 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 64 steps  64 reward -1 next state  4 agent position  (1, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  65 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 65 steps  65 reward -1 next state  4 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  66 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 66 steps  66 reward -1 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 41 steps  67 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 67 steps  67 reward -1 next state  4 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  68 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 68 steps  68 reward -1 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  69 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 69 steps  69 reward -1 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  70 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 70 steps  70 reward -1 next state  4 agent position  (0, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 41 steps  71 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 71 steps  71 reward -1 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  72 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 72 steps  72 reward -2 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 41 steps  73 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 73 steps  73 reward -1 next state  4 agent position  (0, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 41 steps  74 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 74 steps  74 reward -1 next state  4 agent position  (0, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 41 steps  75 reward 0 next state  5 agent position  (5, 4)
agent epsilon  1.0 agent memory len 13 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 75 steps  75 reward -2 next state  4 agent position  (0, 8)
max steps reached
total rewards -107
Episode number:  2
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 42 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 14 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  1.0 agent memory len 76 steps  1 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 43 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 15 steps  2 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 77 steps  2 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 44 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 16 steps  3 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 78 steps  3 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  1.0 agent memory len 45 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 17 steps  4 reward -1 next state  2 agent position  (2, 9)
agent epsilon  1.0 agent memory len 79 steps  4 reward -1 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 46 steps  5 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 18 steps  5 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 80 steps  5 reward -1 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 47 steps  6 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 19 steps  6 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 81 steps  6 reward -2 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  1.0 agent memory len 48 steps  7 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 20 steps  7 reward -1 next state  2 agent position  (2, 9)
agent epsilon  1.0 agent memory len 82 steps  7 reward -1 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 49 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 21 steps  8 reward -1 next state  1 agent position  (2, 9)
agent epsilon  1.0 agent memory len 83 steps  8 reward -1 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  1.0 agent memory len 50 steps  9 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 22 steps  9 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 84 steps  9 reward -2 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 51 steps  10 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 23 steps  10 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 85 steps  10 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  1.0 agent memory len 52 steps  11 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 24 steps  11 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 86 steps  11 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 53 steps  12 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 25 steps  12 reward -2 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 87 steps  12 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  1.0 agent memory len 54 steps  13 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 26 steps  13 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 88 steps  13 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 55 steps  14 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 27 steps  14 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 89 steps  14 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  1.0 agent memory len 56 steps  15 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 28 steps  15 reward -1 next state  0 agent position  (1, 9)
agent epsilon  1.0 agent memory len 90 steps  15 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  1.0 agent memory len 57 steps  16 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 29 steps  16 reward -1 next state  0 agent position  (1, 8)
agent epsilon  1.0 agent memory len 91 steps  16 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  1.0 agent memory len 58 steps  17 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 30 steps  17 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 92 steps  17 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  1.0 agent memory len 59 steps  18 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 31 steps  18 reward -1 next state  1 agent position  (2, 7)
agent epsilon  1.0 agent memory len 93 steps  18 reward -2 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 60 steps  19 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 32 steps  19 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 94 steps  19 reward -1 next state  2 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  1.0 agent memory len 61 steps  20 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 33 steps  20 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 95 steps  20 reward -1 next state  3 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 62 steps  21 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 34 steps  21 reward -1 next state  3 agent position  (3, 8)
agent epsilon  1.0 agent memory len 96 steps  21 reward -1 next state  3 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  1.0 agent memory len 63 steps  22 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 35 steps  22 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 97 steps  22 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 64 steps  23 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 36 steps  23 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 98 steps  23 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 65 steps  24 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 37 steps  24 reward -1 next state  1 agent position  (4, 8)
agent epsilon  1.0 agent memory len 99 steps  24 reward -1 next state  4 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 66 steps  25 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 38 steps  25 reward -1 next state  0 agent position  (4, 7)
agent epsilon  1.0 agent memory len 100 steps  25 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'D']
agent epsilon  1.0 agent memory len 67 steps  26 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 39 steps  26 reward -1 next state  0 agent position  (5, 7)
agent epsilon  1.0 agent memory len 101 steps  26 reward -2 next state  5 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  1.0 agent memory len 68 steps  27 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 40 steps  27 reward -1 next state  1 agent position  (6, 7)
agent epsilon  1.0 agent memory len 102 steps  27 reward -2 next state  6 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'L']
agent epsilon  1.0 agent memory len 69 steps  28 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 41 steps  28 reward -1 next state  0 agent position  (5, 7)
agent epsilon  1.0 agent memory len 103 steps  28 reward -1 next state  5 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  1.0 agent memory len 70 steps  29 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 42 steps  29 reward 0 next state  0 agent position  (5, 6)
agent epsilon  1.0 agent memory len 104 steps  29 reward -1 next state  5 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 71 steps  30 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 43 steps  30 reward 0 next state  0 agent position  (5, 6)
agent epsilon  1.0 agent memory len 105 steps  30 reward -2 next state  5 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  1.0 agent memory len 72 steps  31 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 44 steps  31 reward -1 next state  0 agent position  (6, 6)
agent epsilon  1.0 agent memory len 106 steps  31 reward -1 next state  6 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  1.0 agent memory len 73 steps  32 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 45 steps  32 reward -1 next state  0 agent position  (7, 6)
agent epsilon  1.0 agent memory len 107 steps  32 reward -1 next state  7 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  1.0 agent memory len 74 steps  33 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 46 steps  33 reward -1 next state  1 agent position  (7, 7)
agent epsilon  1.0 agent memory len 108 steps  33 reward -1 next state  7 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  1.0 agent memory len 75 steps  34 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 47 steps  34 reward -1 next state  1 agent position  (7, 6)
agent epsilon  1.0 agent memory len 109 steps  34 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  1.0 agent memory len 76 steps  35 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 48 steps  35 reward -1 next state  0 agent position  (8, 6)
agent epsilon  1.0 agent memory len 110 steps  35 reward -1 next state  8 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 77 steps  36 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 49 steps  36 reward -1 next state  0 agent position  (7, 6)
agent epsilon  1.0 agent memory len 111 steps  36 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  1.0 agent memory len 78 steps  37 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 50 steps  37 reward -1 next state  0 agent position  (6, 6)
agent epsilon  1.0 agent memory len 112 steps  37 reward -2 next state  6 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'S']
agent epsilon  1.0 agent memory len 79 steps  38 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 51 steps  38 reward 0 next state  0 agent position  (6, 5)
agent epsilon  1.0 agent memory len 113 steps  38 reward -1 next state  6 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 80 steps  39 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 52 steps  39 reward -1 next state  0 agent position  (7, 5)
agent epsilon  1.0 agent memory len 114 steps  39 reward -1 next state  7 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 81 steps  40 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 53 steps  40 reward -1 next state  0 agent position  (8, 5)
agent epsilon  1.0 agent memory len 115 steps  40 reward -1 next state  8 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 82 steps  41 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 54 steps  41 reward -1 next state  1 agent position  (9, 5)
agent epsilon  1.0 agent memory len 116 steps  41 reward -1 next state  9 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 83 steps  42 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 55 steps  42 reward -1 next state  1 agent position  (9, 5)
agent epsilon  1.0 agent memory len 117 steps  42 reward -1 next state  9 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'S']
agent epsilon  1.0 agent memory len 84 steps  43 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 56 steps  43 reward -1 next state  1 agent position  (8, 5)
agent epsilon  1.0 agent memory len 118 steps  43 reward -1 next state  8 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  1.0 agent memory len 85 steps  44 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 57 steps  44 reward -1 next state  1 agent position  (9, 5)
agent epsilon  1.0 agent memory len 119 steps  44 reward -1 next state  9 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 86 steps  45 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 58 steps  45 reward -2 next state  0 agent position  (9, 5)
agent epsilon  1.0 agent memory len 120 steps  45 reward -1 next state  9 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 87 steps  46 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 59 steps  46 reward -1 next state  1 agent position  (9, 6)
agent epsilon  1.0 agent memory len 121 steps  46 reward -1 next state  9 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  1.0 agent memory len 88 steps  47 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 60 steps  47 reward -2 next state  0 agent position  (9, 6)
agent epsilon  1.0 agent memory len 122 steps  47 reward -1 next state  9 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 89 steps  48 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 61 steps  48 reward -1 next state  0 agent position  (9, 5)
agent epsilon  1.0 agent memory len 123 steps  48 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  1.0 agent memory len 90 steps  49 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 62 steps  49 reward -1 next state  1 agent position  (9, 4)
agent epsilon  1.0 agent memory len 124 steps  49 reward -1 next state  9 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  1.0 agent memory len 91 steps  50 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 63 steps  50 reward -1 next state  1 agent position  (9, 3)
agent epsilon  1.0 agent memory len 125 steps  50 reward -1 next state  9 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  1.0 agent memory len 92 steps  51 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 64 steps  51 reward -1 next state  2 agent position  (9, 4)
agent epsilon  1.0 agent memory len 126 steps  51 reward -1 next state  9 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 93 steps  52 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 65 steps  52 reward -2 next state  2 agent position  (9, 4)
agent epsilon  1.0 agent memory len 127 steps  52 reward -1 next state  9 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  1.0 agent memory len 94 steps  53 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 66 steps  53 reward -1 next state  1 agent position  (9, 5)
agent epsilon  1.0 agent memory len 128 steps  53 reward -1 next state  9 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 95 steps  54 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 67 steps  54 reward -1 next state  1 agent position  (9, 6)
agent epsilon  1.0 agent memory len 129 steps  54 reward -1 next state  9 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'R']
agent epsilon  1.0 agent memory len 96 steps  55 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 68 steps  55 reward -2 next state  0 agent position  (9, 6)
agent epsilon  1.0 agent memory len 130 steps  55 reward -1 next state  9 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 97 steps  56 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 69 steps  56 reward -1 next state  1 agent position  (9, 6)
agent epsilon  1.0 agent memory len 131 steps  56 reward -1 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 98 steps  57 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 70 steps  57 reward -1 next state  1 agent position  (9, 6)
agent epsilon  1.0 agent memory len 132 steps  57 reward -2 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  1.0 agent memory len 99 steps  58 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 71 steps  58 reward -1 next state  0 agent position  (8, 6)
agent epsilon  1.0 agent memory len 133 steps  58 reward -1 next state  8 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 100 steps  59 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 72 steps  59 reward -1 next state  1 agent position  (7, 6)
agent epsilon  1.0 agent memory len 134 steps  59 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 101 steps  60 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 73 steps  60 reward -1 next state  1 agent position  (7, 7)
agent epsilon  1.0 agent memory len 135 steps  60 reward -1 next state  7 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  1.0 agent memory len 102 steps  61 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 74 steps  61 reward -1 next state  1 agent position  (7, 8)
agent epsilon  1.0 agent memory len 136 steps  61 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 103 steps  62 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 75 steps  62 reward -1 next state  1 agent position  (7, 8)
agent epsilon  1.0 agent memory len 137 steps  62 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'L']
agent epsilon  1.0 agent memory len 104 steps  63 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 76 steps  63 reward -1 next state  0 agent position  (6, 8)
agent epsilon  1.0 agent memory len 138 steps  63 reward -1 next state  6 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 105 steps  64 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 77 steps  64 reward -1 next state  0 agent position  (5, 8)
agent epsilon  1.0 agent memory len 139 steps  64 reward -1 next state  5 agent position  (8, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 106 steps  65 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 78 steps  65 reward -1 next state  0 agent position  (6, 8)
agent epsilon  1.0 agent memory len 140 steps  65 reward -1 next state  6 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 107 steps  66 reward -2 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 79 steps  66 reward -1 next state  0 agent position  (6, 8)
agent epsilon  1.0 agent memory len 141 steps  66 reward -1 next state  6 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 108 steps  67 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 80 steps  67 reward -1 next state  0 agent position  (6, 8)
agent epsilon  1.0 agent memory len 142 steps  67 reward -1 next state  6 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 109 steps  68 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 81 steps  68 reward -1 next state  0 agent position  (7, 8)
agent epsilon  1.0 agent memory len 143 steps  68 reward -1 next state  7 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  1.0 agent memory len 110 steps  69 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 82 steps  69 reward -1 next state  0 agent position  (7, 9)
agent epsilon  1.0 agent memory len 144 steps  69 reward -1 next state  7 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  1.0 agent memory len 111 steps  70 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 83 steps  70 reward -1 next state  0 agent position  (7, 8)
agent epsilon  1.0 agent memory len 145 steps  70 reward -1 next state  7 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 112 steps  71 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 84 steps  71 reward -1 next state  0 agent position  (7, 8)
agent epsilon  1.0 agent memory len 146 steps  71 reward -1 next state  7 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 113 steps  72 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 85 steps  72 reward -1 next state  0 agent position  (6, 8)
agent epsilon  1.0 agent memory len 147 steps  72 reward -1 next state  6 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 114 steps  73 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 86 steps  73 reward -1 next state  0 agent position  (6, 9)
agent epsilon  1.0 agent memory len 148 steps  73 reward -1 next state  6 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 115 steps  74 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 87 steps  74 reward -1 next state  1 agent position  (5, 9)
agent epsilon  1.0 agent memory len 149 steps  74 reward -1 next state  5 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 116 steps  75 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 88 steps  75 reward -1 next state  0 agent position  (5, 9)
agent epsilon  1.0 agent memory len 150 steps  75 reward -1 next state  5 agent position  (9, 3)
max steps reached
total rewards -246
Episode number:  3
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 117 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 89 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 151 steps  1 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 118 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 90 steps  2 reward -1 next state  1 agent position  (0, 9)
agent epsilon  1.0 agent memory len 152 steps  2 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'L']
agent epsilon  1.0 agent memory len 119 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 91 steps  3 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 153 steps  3 reward -2 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  1.0 agent memory len 120 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 92 steps  4 reward -1 next state  1 agent position  (0, 8)
agent epsilon  1.0 agent memory len 154 steps  4 reward -1 next state  0 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 121 steps  5 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 93 steps  5 reward -1 next state  1 agent position  (0, 8)
agent epsilon  1.0 agent memory len 155 steps  5 reward -2 next state  0 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 122 steps  6 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 94 steps  6 reward -1 next state  2 agent position  (1, 8)
agent epsilon  1.0 agent memory len 156 steps  6 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'R']
agent epsilon  1.0 agent memory len 123 steps  7 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 95 steps  7 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 157 steps  7 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  1.0 agent memory len 124 steps  8 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 96 steps  8 reward -1 next state  2 agent position  (1, 8)
agent epsilon  1.0 agent memory len 158 steps  8 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 125 steps  9 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 97 steps  9 reward -1 next state  2 agent position  (0, 8)
agent epsilon  1.0 agent memory len 159 steps  9 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  1.0 agent memory len 126 steps  10 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 98 steps  10 reward -1 next state  3 agent position  (0, 7)
agent epsilon  1.0 agent memory len 160 steps  10 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  1.0 agent memory len 127 steps  11 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 99 steps  11 reward -1 next state  2 agent position  (0, 8)
agent epsilon  1.0 agent memory len 161 steps  11 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 128 steps  12 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 100 steps  12 reward -1 next state  3 agent position  (0, 8)
agent epsilon  1.0 agent memory len 162 steps  12 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  1.0 agent memory len 129 steps  13 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 101 steps  13 reward -1 next state  3 agent position  (0, 7)
agent epsilon  1.0 agent memory len 163 steps  13 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 130 steps  14 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 102 steps  14 reward -2 next state  4 agent position  (0, 7)
agent epsilon  1.0 agent memory len 164 steps  14 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 131 steps  15 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 103 steps  15 reward -1 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 165 steps  15 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  1.0 agent memory len 132 steps  16 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 104 steps  16 reward -2 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 166 steps  16 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  1.0 agent memory len 133 steps  17 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 105 steps  17 reward -2 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 167 steps  17 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 134 steps  18 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 106 steps  18 reward -1 next state  4 agent position  (1, 8)
agent epsilon  1.0 agent memory len 168 steps  18 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 135 steps  19 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 107 steps  19 reward -1 next state  4 agent position  (1, 8)
agent epsilon  1.0 agent memory len 169 steps  19 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  1.0 agent memory len 136 steps  20 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 108 steps  20 reward -1 next state  5 agent position  (2, 8)
agent epsilon  1.0 agent memory len 170 steps  20 reward -2 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'R']
agent epsilon  1.0 agent memory len 137 steps  21 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 109 steps  21 reward -1 next state  5 agent position  (1, 8)
agent epsilon  1.0 agent memory len 171 steps  21 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 138 steps  22 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 110 steps  22 reward -1 next state  5 agent position  (0, 8)
agent epsilon  1.0 agent memory len 172 steps  22 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'U']
agent epsilon  1.0 agent memory len 139 steps  23 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 111 steps  23 reward -1 next state  5 agent position  (1, 8)
agent epsilon  1.0 agent memory len 173 steps  23 reward -1 next state  1 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  1.0 agent memory len 140 steps  24 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 112 steps  24 reward -1 next state  6 agent position  (1, 9)
agent epsilon  1.0 agent memory len 174 steps  24 reward -1 next state  1 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 141 steps  25 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 113 steps  25 reward -1 next state  5 agent position  (1, 9)
agent epsilon  1.0 agent memory len 175 steps  25 reward -1 next state  1 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 142 steps  26 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 114 steps  26 reward -1 next state  5 agent position  (0, 9)
agent epsilon  1.0 agent memory len 176 steps  26 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 143 steps  27 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 115 steps  27 reward -1 next state  4 agent position  (0, 9)
agent epsilon  1.0 agent memory len 177 steps  27 reward -1 next state  0 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  1.0 agent memory len 144 steps  28 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 116 steps  28 reward -2 next state  5 agent position  (0, 9)
agent epsilon  1.0 agent memory len 178 steps  28 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  1.0 agent memory len 145 steps  29 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 117 steps  29 reward -1 next state  5 agent position  (0, 8)
agent epsilon  1.0 agent memory len 179 steps  29 reward -1 next state  0 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'U']
agent epsilon  1.0 agent memory len 146 steps  30 reward 0 next state  3 agent position  (3, 5)
agent epsilon  1.0 agent memory len 118 steps  30 reward -1 next state  5 agent position  (0, 8)
agent epsilon  1.0 agent memory len 180 steps  30 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'R', 'L']
agent epsilon  1.0 agent memory len 147 steps  31 reward 10 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 119 steps  31 reward -1 next state  5 agent position  (0, 9)
agent epsilon  1.0 agent memory len 181 steps  31 reward -2 next state  0 agent position  (5, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 147 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 120 steps  32 reward -1 next state  5 agent position  (1, 9)
agent epsilon  1.0 agent memory len 182 steps  32 reward -1 next state  1 agent position  (6, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  1.0 agent memory len 147 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 121 steps  33 reward -1 next state  5 agent position  (0, 9)
agent epsilon  1.0 agent memory len 183 steps  33 reward -1 next state  0 agent position  (6, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 147 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 122 steps  34 reward -1 next state  5 agent position  (0, 8)
agent epsilon  1.0 agent memory len 184 steps  34 reward -1 next state  0 agent position  (6, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 123 steps  35 reward -1 next state  5 agent position  (1, 8)
agent epsilon  1.0 agent memory len 185 steps  35 reward 0 next state  1 agent position  (6, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 147 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 124 steps  36 reward -1 next state  5 agent position  (1, 9)
agent epsilon  1.0 agent memory len 186 steps  36 reward 0 next state  1 agent position  (6, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 125 steps  37 reward -1 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 187 steps  37 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 147 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 126 steps  38 reward -2 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 188 steps  38 reward 0 next state  2 agent position  (5, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 147 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 127 steps  39 reward -1 next state  5 agent position  (2, 8)
agent epsilon  1.0 agent memory len 189 steps  39 reward 0 next state  2 agent position  (5, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 147 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 128 steps  40 reward -1 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 190 steps  40 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 147 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 129 steps  41 reward -2 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 191 steps  41 reward -1 next state  2 agent position  (7, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 147 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 130 steps  42 reward -2 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 192 steps  42 reward -1 next state  2 agent position  (8, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 147 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 131 steps  43 reward -2 next state  5 agent position  (2, 9)
agent epsilon  1.0 agent memory len 193 steps  43 reward -1 next state  2 agent position  (8, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 132 steps  44 reward -1 next state  5 agent position  (3, 9)
agent epsilon  1.0 agent memory len 194 steps  44 reward -1 next state  3 agent position  (8, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 147 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 133 steps  45 reward -1 next state  5 agent position  (4, 9)
agent epsilon  1.0 agent memory len 195 steps  45 reward -1 next state  4 agent position  (7, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 147 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 134 steps  46 reward -1 next state  5 agent position  (5, 9)
agent epsilon  1.0 agent memory len 196 steps  46 reward -1 next state  5 agent position  (7, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 135 steps  47 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 197 steps  47 reward -1 next state  6 agent position  (7, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 147 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 136 steps  48 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 198 steps  48 reward -1 next state  6 agent position  (7, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 147 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 137 steps  49 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 199 steps  49 reward -1 next state  6 agent position  (6, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 147 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 138 steps  50 reward -2 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 200 steps  50 reward -1 next state  6 agent position  (7, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 147 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 139 steps  51 reward -1 next state  5 agent position  (6, 8)
agent epsilon  1.0 agent memory len 201 steps  51 reward -1 next state  6 agent position  (6, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 147 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 140 steps  52 reward -1 next state  5 agent position  (6, 8)
agent epsilon  1.0 agent memory len 202 steps  52 reward -1 next state  6 agent position  (5, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 147 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 141 steps  53 reward -1 next state  5 agent position  (6, 7)
agent epsilon  1.0 agent memory len 203 steps  53 reward -1 next state  6 agent position  (4, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 147 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 142 steps  54 reward -1 next state  5 agent position  (6, 8)
agent epsilon  1.0 agent memory len 204 steps  54 reward -1 next state  6 agent position  (4, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 147 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 143 steps  55 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 205 steps  55 reward -1 next state  6 agent position  (4, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 147 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 144 steps  56 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 206 steps  56 reward -1 next state  6 agent position  (4, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 147 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 145 steps  57 reward -1 next state  5 agent position  (6, 9)
agent epsilon  1.0 agent memory len 207 steps  57 reward -1 next state  6 agent position  (4, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 147 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 146 steps  58 reward -1 next state  5 agent position  (7, 9)
agent epsilon  1.0 agent memory len 208 steps  58 reward -1 next state  7 agent position  (4, 6)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 147 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 147 steps  59 reward -1 next state  5 agent position  (7, 8)
agent epsilon  1.0 agent memory len 209 steps  59 reward -1 next state  7 agent position  (4, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 147 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 148 steps  60 reward -1 next state  5 agent position  (8, 8)
agent epsilon  1.0 agent memory len 210 steps  60 reward -1 next state  8 agent position  (5, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 147 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 149 steps  61 reward -1 next state  5 agent position  (9, 8)
agent epsilon  1.0 agent memory len 211 steps  61 reward -1 next state  9 agent position  (4, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 147 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 150 steps  62 reward -1 next state  5 agent position  (9, 7)
agent epsilon  1.0 agent memory len 212 steps  62 reward -1 next state  9 agent position  (4, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 147 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 151 steps  63 reward -1 next state  5 agent position  (9, 7)
agent epsilon  1.0 agent memory len 213 steps  63 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 147 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 152 steps  64 reward -1 next state  5 agent position  (9, 8)
agent epsilon  1.0 agent memory len 214 steps  64 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 147 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 153 steps  65 reward -1 next state  5 agent position  (9, 9)
agent epsilon  1.0 agent memory len 215 steps  65 reward -1 next state  9 agent position  (5, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 154 steps  66 reward -2 next state  5 agent position  (9, 9)
agent epsilon  1.0 agent memory len 216 steps  66 reward -2 next state  9 agent position  (5, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  1.0 agent memory len 147 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 155 steps  67 reward -1 next state  5 agent position  (8, 9)
agent epsilon  1.0 agent memory len 217 steps  67 reward -1 next state  8 agent position  (5, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 147 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 156 steps  68 reward -1 next state  5 agent position  (9, 9)
agent epsilon  1.0 agent memory len 218 steps  68 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 147 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 157 steps  69 reward -1 next state  5 agent position  (8, 9)
agent epsilon  1.0 agent memory len 219 steps  69 reward -1 next state  8 agent position  (6, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 147 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 158 steps  70 reward -1 next state  5 agent position  (8, 8)
agent epsilon  1.0 agent memory len 220 steps  70 reward -1 next state  8 agent position  (7, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 147 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 159 steps  71 reward -1 next state  5 agent position  (8, 7)
agent epsilon  1.0 agent memory len 221 steps  71 reward -1 next state  8 agent position  (7, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 147 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 160 steps  72 reward -1 next state  5 agent position  (8, 6)
agent epsilon  1.0 agent memory len 222 steps  72 reward -1 next state  8 agent position  (8, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 147 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 161 steps  73 reward -1 next state  5 agent position  (8, 7)
agent epsilon  1.0 agent memory len 223 steps  73 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 147 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 162 steps  74 reward -1 next state  5 agent position  (9, 7)
agent epsilon  1.0 agent memory len 224 steps  74 reward -1 next state  9 agent position  (7, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 147 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  1.0 agent memory len 163 steps  75 reward -2 next state  5 agent position  (9, 7)
agent epsilon  1.0 agent memory len 225 steps  75 reward -1 next state  9 agent position  (7, 9)
max steps reached
total rewards -178
Episode number:  4
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 148 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 164 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 226 steps  1 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 149 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 165 steps  2 reward -1 next state  1 agent position  (0, 9)
agent epsilon  1.0 agent memory len 227 steps  2 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'S']
agent epsilon  1.0 agent memory len 150 steps  3 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 166 steps  3 reward -1 next state  1 agent position  (0, 8)
agent epsilon  1.0 agent memory len 228 steps  3 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  1.0 agent memory len 151 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 167 steps  4 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 229 steps  4 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 152 steps  5 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 168 steps  5 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 230 steps  5 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'S']
agent epsilon  1.0 agent memory len 153 steps  6 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 169 steps  6 reward -1 next state  1 agent position  (0, 8)
agent epsilon  1.0 agent memory len 231 steps  6 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 154 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 170 steps  7 reward -1 next state  1 agent position  (0, 7)
agent epsilon  1.0 agent memory len 232 steps  7 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 155 steps  8 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 171 steps  8 reward -1 next state  1 agent position  (0, 7)
agent epsilon  1.0 agent memory len 233 steps  8 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 156 steps  9 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 172 steps  9 reward -1 next state  1 agent position  (0, 6)
agent epsilon  1.0 agent memory len 234 steps  9 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 157 steps  10 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 173 steps  10 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 235 steps  10 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 158 steps  11 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 174 steps  11 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 236 steps  11 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  1.0 agent memory len 159 steps  12 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 175 steps  12 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 237 steps  12 reward -1 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 160 steps  13 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 176 steps  13 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 238 steps  13 reward -2 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'U']
agent epsilon  1.0 agent memory len 161 steps  14 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 177 steps  14 reward -1 next state  0 agent position  (1, 3)
agent epsilon  1.0 agent memory len 239 steps  14 reward -1 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  1.0 agent memory len 162 steps  15 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 178 steps  15 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 240 steps  15 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  1.0 agent memory len 163 steps  16 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 179 steps  16 reward -1 next state  1 agent position  (0, 4)
agent epsilon  1.0 agent memory len 241 steps  16 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  1.0 agent memory len 164 steps  17 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 180 steps  17 reward -1 next state  2 agent position  (0, 3)
agent epsilon  1.0 agent memory len 242 steps  17 reward -1 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  1.0 agent memory len 165 steps  18 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 181 steps  18 reward -1 next state  3 agent position  (0, 4)
agent epsilon  1.0 agent memory len 243 steps  18 reward -2 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'R']
agent epsilon  1.0 agent memory len 166 steps  19 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 182 steps  19 reward -1 next state  4 agent position  (0, 3)
agent epsilon  1.0 agent memory len 244 steps  19 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 167 steps  20 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 183 steps  20 reward -1 next state  3 agent position  (0, 2)
agent epsilon  1.0 agent memory len 245 steps  20 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 168 steps  21 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 184 steps  21 reward -1 next state  4 agent position  (1, 2)
agent epsilon  1.0 agent memory len 246 steps  21 reward -1 next state  1 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'D']
agent epsilon  1.0 agent memory len 169 steps  22 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 185 steps  22 reward -1 next state  4 agent position  (0, 2)
agent epsilon  1.0 agent memory len 247 steps  22 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 170 steps  23 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 186 steps  23 reward -1 next state  4 agent position  (0, 2)
agent epsilon  1.0 agent memory len 248 steps  23 reward 0 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  1.0 agent memory len 171 steps  24 reward 0 next state  3 agent position  (3, 4)
agent epsilon  1.0 agent memory len 187 steps  24 reward -1 next state  4 agent position  (0, 3)
agent epsilon  1.0 agent memory len 249 steps  24 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 172 steps  25 reward 0 next state  3 agent position  (3, 4)
agent epsilon  1.0 agent memory len 188 steps  25 reward -1 next state  4 agent position  (0, 3)
agent epsilon  1.0 agent memory len 250 steps  25 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'U', 'U']
agent epsilon  1.0 agent memory len 173 steps  26 reward 10 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 189 steps  26 reward -2 next state  4 agent position  (0, 3)
agent epsilon  1.0 agent memory len 251 steps  26 reward 0 next state  0 agent position  (5, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 173 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 190 steps  27 reward -1 next state  4 agent position  (1, 3)
agent epsilon  1.0 agent memory len 252 steps  27 reward -1 next state  1 agent position  (5, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 191 steps  28 reward -1 next state  4 agent position  (1, 2)
agent epsilon  1.0 agent memory len 253 steps  28 reward -1 next state  1 agent position  (5, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 173 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 192 steps  29 reward -1 next state  4 agent position  (1, 3)
agent epsilon  1.0 agent memory len 254 steps  29 reward -1 next state  1 agent position  (5, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 173 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 193 steps  30 reward -1 next state  4 agent position  (1, 3)
agent epsilon  1.0 agent memory len 255 steps  30 reward -1 next state  1 agent position  (5, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 173 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 194 steps  31 reward -1 next state  4 agent position  (2, 3)
agent epsilon  1.0 agent memory len 256 steps  31 reward -1 next state  2 agent position  (5, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 173 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 195 steps  32 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 257 steps  32 reward -1 next state  2 agent position  (5, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 196 steps  33 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 258 steps  33 reward -1 next state  2 agent position  (5, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 173 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 197 steps  34 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 259 steps  34 reward 0 next state  2 agent position  (5, 3)
 is_terminal [True, False, False]
random action 1
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 173 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 198 steps  35 reward -1 next state  4 agent position  (2, 3)
agent epsilon  1.0 agent memory len 260 steps  35 reward 10 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 199 steps  36 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 260 steps  36 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 200 steps  37 reward -1 next state  4 agent position  (2, 3)
agent epsilon  1.0 agent memory len 260 steps  37 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 201 steps  38 reward -1 next state  4 agent position  (3, 3)
agent epsilon  1.0 agent memory len 260 steps  38 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 202 steps  39 reward -1 next state  4 agent position  (2, 3)
agent epsilon  1.0 agent memory len 260 steps  39 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 203 steps  40 reward -1 next state  4 agent position  (2, 3)
agent epsilon  1.0 agent memory len 260 steps  40 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 204 steps  41 reward -1 next state  4 agent position  (1, 3)
agent epsilon  1.0 agent memory len 260 steps  41 reward 0 next state  1 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 205 steps  42 reward -1 next state  4 agent position  (1, 2)
agent epsilon  1.0 agent memory len 260 steps  42 reward 0 next state  1 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 206 steps  43 reward -1 next state  4 agent position  (1, 3)
agent epsilon  1.0 agent memory len 260 steps  43 reward 0 next state  1 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 207 steps  44 reward -1 next state  4 agent position  (1, 2)
agent epsilon  1.0 agent memory len 260 steps  44 reward 0 next state  1 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 208 steps  45 reward -1 next state  4 agent position  (0, 2)
agent epsilon  1.0 agent memory len 260 steps  45 reward 0 next state  0 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 209 steps  46 reward -1 next state  4 agent position  (0, 1)
agent epsilon  1.0 agent memory len 260 steps  46 reward 0 next state  0 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 210 steps  47 reward -1 next state  4 agent position  (0, 2)
agent epsilon  1.0 agent memory len 260 steps  47 reward 0 next state  0 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 211 steps  48 reward -2 next state  4 agent position  (0, 2)
agent epsilon  1.0 agent memory len 260 steps  48 reward 0 next state  0 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 212 steps  49 reward -1 next state  4 agent position  (1, 2)
agent epsilon  1.0 agent memory len 260 steps  49 reward 0 next state  1 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 213 steps  50 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 260 steps  50 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 214 steps  51 reward -1 next state  4 agent position  (2, 1)
agent epsilon  1.0 agent memory len 260 steps  51 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 215 steps  52 reward -1 next state  4 agent position  (2, 0)
agent epsilon  1.0 agent memory len 260 steps  52 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 216 steps  53 reward -2 next state  4 agent position  (2, 0)
agent epsilon  1.0 agent memory len 260 steps  53 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 217 steps  54 reward -1 next state  4 agent position  (2, 1)
agent epsilon  1.0 agent memory len 260 steps  54 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 218 steps  55 reward -1 next state  4 agent position  (2, 2)
agent epsilon  1.0 agent memory len 260 steps  55 reward 0 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 219 steps  56 reward -1 next state  4 agent position  (3, 2)
agent epsilon  1.0 agent memory len 260 steps  56 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 220 steps  57 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 260 steps  57 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 221 steps  58 reward -1 next state  4 agent position  (5, 2)
agent epsilon  1.0 agent memory len 260 steps  58 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 222 steps  59 reward -1 next state  4 agent position  (5, 3)
agent epsilon  1.0 agent memory len 260 steps  59 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 223 steps  60 reward -1 next state  4 agent position  (5, 3)
agent epsilon  1.0 agent memory len 260 steps  60 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 224 steps  61 reward -1 next state  4 agent position  (4, 3)
agent epsilon  1.0 agent memory len 260 steps  61 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 173 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 225 steps  62 reward -1 next state  4 agent position  (3, 3)
agent epsilon  1.0 agent memory len 260 steps  62 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 173 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 226 steps  63 reward 0 next state  4 agent position  (3, 4)
agent epsilon  1.0 agent memory len 260 steps  63 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 227 steps  64 reward -1 next state  4 agent position  (3, 3)
agent epsilon  1.0 agent memory len 260 steps  64 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 228 steps  65 reward -1 next state  4 agent position  (3, 3)
agent epsilon  1.0 agent memory len 260 steps  65 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 229 steps  66 reward -1 next state  4 agent position  (4, 3)
agent epsilon  1.0 agent memory len 260 steps  66 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 230 steps  67 reward -1 next state  4 agent position  (4, 3)
agent epsilon  1.0 agent memory len 260 steps  67 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 231 steps  68 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 260 steps  68 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 232 steps  69 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 260 steps  69 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 233 steps  70 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 260 steps  70 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 173 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 234 steps  71 reward -1 next state  4 agent position  (5, 2)
agent epsilon  1.0 agent memory len 260 steps  71 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 173 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 235 steps  72 reward -1 next state  4 agent position  (5, 1)
agent epsilon  1.0 agent memory len 260 steps  72 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 236 steps  73 reward -1 next state  4 agent position  (5, 1)
agent epsilon  1.0 agent memory len 260 steps  73 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 237 steps  74 reward -1 next state  4 agent position  (5, 1)
agent epsilon  1.0 agent memory len 260 steps  74 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 173 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  1.0 agent memory len 238 steps  75 reward -1 next state  4 agent position  (5, 1)
agent epsilon  1.0 agent memory len 260 steps  75 reward 0 next state  5 agent position  (5, 4)
max steps reached
total rewards -120
epsilon  0.820543445547202
Episode number:  5
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 174 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 239 steps  1 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.820543445547202 agent memory len 261 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  1.0 agent memory len 175 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 240 steps  2 reward -2 next state  1 agent position  (0, 9)
agent epsilon  0.820543445547202 agent memory len 262 steps  2 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  1.0 agent memory len 176 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 241 steps  3 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 263 steps  3 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  1.0 agent memory len 177 steps  4 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 242 steps  4 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.820543445547202 agent memory len 264 steps  4 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 178 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 243 steps  5 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.820543445547202 agent memory len 265 steps  5 reward -2 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 179 steps  6 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 244 steps  6 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.820543445547202 agent memory len 266 steps  6 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  1.0 agent memory len 180 steps  7 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 245 steps  7 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 267 steps  7 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  1.0 agent memory len 181 steps  8 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 246 steps  8 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 268 steps  8 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 182 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 247 steps  9 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 269 steps  9 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  1.0 agent memory len 183 steps  10 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 248 steps  10 reward -1 next state  2 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 270 steps  10 reward -2 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 184 steps  11 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 249 steps  11 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 271 steps  11 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 185 steps  12 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 250 steps  12 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 272 steps  12 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 186 steps  13 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 251 steps  13 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 273 steps  13 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 187 steps  14 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 252 steps  14 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 274 steps  14 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 188 steps  15 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 253 steps  15 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 275 steps  15 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  1.0 agent memory len 189 steps  16 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 254 steps  16 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 276 steps  16 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 190 steps  17 reward -2 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 255 steps  17 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.820543445547202 agent memory len 277 steps  17 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'R', 'R']
agent epsilon  1.0 agent memory len 191 steps  18 reward -2 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 256 steps  18 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.820543445547202 agent memory len 278 steps  18 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  1.0 agent memory len 192 steps  19 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 257 steps  19 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 279 steps  19 reward -1 next state  0 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'U']
agent epsilon  1.0 agent memory len 193 steps  20 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 258 steps  20 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 280 steps  20 reward -1 next state  0 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 194 steps  21 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 259 steps  21 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.820543445547202 agent memory len 281 steps  21 reward -1 next state  1 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 195 steps  22 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 260 steps  22 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.820543445547202 agent memory len 282 steps  22 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 196 steps  23 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 261 steps  23 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.820543445547202 agent memory len 283 steps  23 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  1.0 agent memory len 197 steps  24 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 262 steps  24 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.820543445547202 agent memory len 284 steps  24 reward -1 next state  1 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 198 steps  25 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 263 steps  25 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.820543445547202 agent memory len 285 steps  25 reward -1 next state  1 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 199 steps  26 reward -1 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 264 steps  26 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.820543445547202 agent memory len 286 steps  26 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 200 steps  27 reward -1 next state  5 agent position  (5, 1)
agent epsilon  1.0 agent memory len 265 steps  27 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 287 steps  27 reward -2 next state  0 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 201 steps  28 reward -1 next state  5 agent position  (5, 1)
agent epsilon  1.0 agent memory len 266 steps  28 reward -2 next state  1 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 288 steps  28 reward -1 next state  0 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 202 steps  29 reward -1 next state  6 agent position  (6, 1)
agent epsilon  1.0 agent memory len 267 steps  29 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.820543445547202 agent memory len 289 steps  29 reward -1 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 203 steps  30 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 268 steps  30 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 290 steps  30 reward -2 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'S']
agent epsilon  1.0 agent memory len 204 steps  31 reward -1 next state  7 agent position  (7, 2)
agent epsilon  1.0 agent memory len 269 steps  31 reward -2 next state  2 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 291 steps  31 reward -1 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 205 steps  32 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 270 steps  32 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 292 steps  32 reward -2 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 206 steps  33 reward -1 next state  7 agent position  (7, 0)
agent epsilon  1.0 agent memory len 271 steps  33 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 293 steps  33 reward -1 next state  0 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 207 steps  34 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 272 steps  34 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 294 steps  34 reward -1 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  1.0 agent memory len 208 steps  35 reward -1 next state  6 agent position  (6, 1)
agent epsilon  1.0 agent memory len 273 steps  35 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 295 steps  35 reward -1 next state  0 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  1.0 agent memory len 209 steps  36 reward -1 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 274 steps  36 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 296 steps  36 reward -1 next state  0 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 210 steps  37 reward -1 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 275 steps  37 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 297 steps  37 reward -1 next state  0 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 211 steps  38 reward -1 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 276 steps  38 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.820543445547202 agent memory len 298 steps  38 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  1.0 agent memory len 212 steps  39 reward -2 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 277 steps  39 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.820543445547202 agent memory len 299 steps  39 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'U']
agent epsilon  1.0 agent memory len 213 steps  40 reward -2 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 278 steps  40 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.820543445547202 agent memory len 300 steps  40 reward -1 next state  1 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  1.0 agent memory len 214 steps  41 reward -1 next state  7 agent position  (7, 0)
agent epsilon  1.0 agent memory len 279 steps  41 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 301 steps  41 reward -1 next state  0 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'S']
agent epsilon  1.0 agent memory len 215 steps  42 reward -1 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 280 steps  42 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.820543445547202 agent memory len 302 steps  42 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  1.0 agent memory len 216 steps  43 reward -2 next state  6 agent position  (6, 0)
agent epsilon  1.0 agent memory len 281 steps  43 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 303 steps  43 reward -1 next state  0 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  1.0 agent memory len 217 steps  44 reward -1 next state  7 agent position  (7, 0)
agent epsilon  1.0 agent memory len 282 steps  44 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 304 steps  44 reward -1 next state  0 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 218 steps  45 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 283 steps  45 reward -2 next state  1 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 305 steps  45 reward -1 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 219 steps  46 reward -1 next state  7 agent position  (7, 2)
agent epsilon  1.0 agent memory len 284 steps  46 reward -1 next state  2 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 306 steps  46 reward -2 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 220 steps  47 reward -1 next state  8 agent position  (8, 2)
agent epsilon  1.0 agent memory len 285 steps  47 reward -1 next state  2 agent position  (0, 7)
agent epsilon  0.820543445547202 agent memory len 307 steps  47 reward -2 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  1.0 agent memory len 221 steps  48 reward -1 next state  8 agent position  (8, 3)
agent epsilon  1.0 agent memory len 286 steps  48 reward -1 next state  3 agent position  (0, 6)
agent epsilon  0.820543445547202 agent memory len 308 steps  48 reward -1 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  1.0 agent memory len 222 steps  49 reward -1 next state  9 agent position  (9, 3)
agent epsilon  1.0 agent memory len 287 steps  49 reward -1 next state  3 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 309 steps  49 reward -2 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 223 steps  50 reward -1 next state  9 agent position  (9, 4)
agent epsilon  1.0 agent memory len 288 steps  50 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 310 steps  50 reward -1 next state  0 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 224 steps  51 reward -1 next state  9 agent position  (9, 3)
agent epsilon  1.0 agent memory len 289 steps  51 reward -1 next state  3 agent position  (1, 5)
agent epsilon  0.820543445547202 agent memory len 311 steps  51 reward -1 next state  1 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  1.0 agent memory len 225 steps  52 reward -1 next state  9 agent position  (9, 4)
agent epsilon  1.0 agent memory len 290 steps  52 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 312 steps  52 reward -2 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  1.0 agent memory len 226 steps  53 reward -1 next state  9 agent position  (9, 5)
agent epsilon  1.0 agent memory len 291 steps  53 reward -1 next state  5 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 313 steps  53 reward -1 next state  0 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  1.0 agent memory len 227 steps  54 reward -1 next state  8 agent position  (8, 5)
agent epsilon  1.0 agent memory len 292 steps  54 reward -1 next state  5 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 314 steps  54 reward -1 next state  0 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 228 steps  55 reward -1 next state  9 agent position  (9, 5)
agent epsilon  1.0 agent memory len 293 steps  55 reward -1 next state  5 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 315 steps  55 reward -1 next state  0 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'S']
agent epsilon  1.0 agent memory len 229 steps  56 reward -1 next state  8 agent position  (8, 5)
agent epsilon  1.0 agent memory len 294 steps  56 reward -1 next state  5 agent position  (1, 4)
agent epsilon  0.820543445547202 agent memory len 316 steps  56 reward -1 next state  1 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'R']
agent epsilon  1.0 agent memory len 230 steps  57 reward -1 next state  8 agent position  (8, 4)
agent epsilon  1.0 agent memory len 295 steps  57 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 317 steps  57 reward -1 next state  0 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 231 steps  58 reward -1 next state  7 agent position  (7, 4)
agent epsilon  1.0 agent memory len 296 steps  58 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 318 steps  58 reward -2 next state  0 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  1.0 agent memory len 232 steps  59 reward -1 next state  8 agent position  (8, 4)
agent epsilon  1.0 agent memory len 297 steps  59 reward -2 next state  4 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 319 steps  59 reward -2 next state  0 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  1.0 agent memory len 233 steps  60 reward -1 next state  9 agent position  (9, 4)
agent epsilon  1.0 agent memory len 298 steps  60 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.820543445547202 agent memory len 320 steps  60 reward -1 next state  0 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 234 steps  61 reward -1 next state  9 agent position  (9, 3)
agent epsilon  1.0 agent memory len 299 steps  61 reward -1 next state  3 agent position  (0, 4)
agent epsilon  0.820543445547202 agent memory len 321 steps  61 reward -1 next state  0 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 235 steps  62 reward -2 next state  9 agent position  (9, 3)
agent epsilon  1.0 agent memory len 300 steps  62 reward -1 next state  3 agent position  (1, 4)
agent epsilon  0.820543445547202 agent memory len 322 steps  62 reward -1 next state  1 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  1.0 agent memory len 236 steps  63 reward -1 next state  9 agent position  (9, 4)
agent epsilon  1.0 agent memory len 301 steps  63 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.820543445547202 agent memory len 323 steps  63 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'L']
agent epsilon  1.0 agent memory len 237 steps  64 reward -1 next state  9 agent position  (9, 3)
agent epsilon  1.0 agent memory len 302 steps  64 reward -1 next state  3 agent position  (2, 5)
agent epsilon  0.820543445547202 agent memory len 324 steps  64 reward -1 next state  2 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 238 steps  65 reward -1 next state  8 agent position  (8, 3)
agent epsilon  1.0 agent memory len 303 steps  65 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.820543445547202 agent memory len 325 steps  65 reward -1 next state  2 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  1.0 agent memory len 239 steps  66 reward -1 next state  8 agent position  (8, 4)
agent epsilon  1.0 agent memory len 304 steps  66 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.820543445547202 agent memory len 326 steps  66 reward -1 next state  2 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'R']
agent epsilon  1.0 agent memory len 240 steps  67 reward -1 next state  8 agent position  (8, 3)
agent epsilon  1.0 agent memory len 305 steps  67 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.820543445547202 agent memory len 327 steps  67 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  1.0 agent memory len 241 steps  68 reward -1 next state  7 agent position  (7, 3)
agent epsilon  1.0 agent memory len 306 steps  68 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.820543445547202 agent memory len 328 steps  68 reward -1 next state  3 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 242 steps  69 reward -1 next state  7 agent position  (7, 2)
agent epsilon  1.0 agent memory len 307 steps  69 reward 0 next state  2 agent position  (3, 6)
agent epsilon  0.820543445547202 agent memory len 329 steps  69 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 243 steps  70 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 308 steps  70 reward 0 next state  1 agent position  (4, 6)
agent epsilon  0.820543445547202 agent memory len 330 steps  70 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 244 steps  71 reward -1 next state  7 agent position  (7, 0)
agent epsilon  1.0 agent memory len 309 steps  71 reward 10 next state  0 agent position  (4, 5)
agent epsilon  0.820543445547202 agent memory len 331 steps  71 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 245 steps  72 reward -1 next state  7 agent position  (7, 1)
agent epsilon  1.0 agent memory len 309 steps  72 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.820543445547202 agent memory len 332 steps  72 reward -1 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 246 steps  73 reward -1 next state  6 agent position  (6, 1)
agent epsilon  1.0 agent memory len 309 steps  73 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.820543445547202 agent memory len 333 steps  73 reward -2 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 247 steps  74 reward -1 next state  5 agent position  (5, 1)
agent epsilon  1.0 agent memory len 309 steps  74 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.820543445547202 agent memory len 334 steps  74 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 248 steps  75 reward -1 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 309 steps  75 reward 0 next state  0 agent position  (4, 5)
agent epsilon  0.820543445547202 agent memory len 335 steps  75 reward -1 next state  4 agent position  (9, 9)
max steps reached
total rewards -232
epsilon  0.7810127752406908
epsilon  0.7810127752406908
Episode number:  6
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 249 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 310 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7810127752406908 agent memory len 336 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'D']
agent epsilon  1.0 agent memory len 250 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 311 steps  2 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.7810127752406908 agent memory len 337 steps  2 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'R', 'U']
agent epsilon  1.0 agent memory len 251 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 312 steps  3 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7810127752406908 agent memory len 338 steps  3 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  1.0 agent memory len 252 steps  4 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 313 steps  4 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7810127752406908 agent memory len 339 steps  4 reward -1 next state  0 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  1.0 agent memory len 253 steps  5 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 314 steps  5 reward -2 next state  1 agent position  (0, 8)
agent epsilon  0.7810127752406908 agent memory len 340 steps  5 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 254 steps  6 reward -2 next state  0 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 315 steps  6 reward -1 next state  1 agent position  (0, 9)
agent epsilon  0.7810127752406908 agent memory len 341 steps  6 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 255 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7810127752406908 agent memory len 316 steps  7 reward -2 next state  1 agent position  (0, 9)
agent epsilon  0.7810127752406908 agent memory len 342 steps  7 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 256 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 317 steps  8 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7810127752406908 agent memory len 343 steps  8 reward -1 next state  1 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  1.0 agent memory len 257 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.7810127752406908 agent memory len 318 steps  9 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.7810127752406908 agent memory len 344 steps  9 reward -1 next state  1 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 258 steps  10 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7810127752406908 agent memory len 319 steps  10 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7810127752406908 agent memory len 345 steps  10 reward -1 next state  2 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 259 steps  11 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.7810127752406908 agent memory len 320 steps  11 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.7810127752406908 agent memory len 346 steps  11 reward -1 next state  2 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 260 steps  12 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.7810127752406908 agent memory len 321 steps  12 reward -1 next state  2 agent position  (3, 8)
agent epsilon  0.7810127752406908 agent memory len 347 steps  12 reward -1 next state  3 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  1.0 agent memory len 261 steps  13 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.7810127752406908 agent memory len 322 steps  13 reward -1 next state  2 agent position  (3, 9)
agent epsilon  0.7810127752406908 agent memory len 348 steps  13 reward -1 next state  3 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  1.0 agent memory len 262 steps  14 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 323 steps  14 reward -1 next state  3 agent position  (4, 9)
agent epsilon  0.7810127752406908 agent memory len 349 steps  14 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  15 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 324 steps  15 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.7810127752406908 agent memory len 350 steps  15 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 263 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 325 steps  16 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.7810127752406908 agent memory len 351 steps  16 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 263 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 326 steps  17 reward -1 next state  4 agent position  (5, 8)
agent epsilon  0.7810127752406908 agent memory len 352 steps  17 reward -1 next state  5 agent position  (8, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 263 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 327 steps  18 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.7810127752406908 agent memory len 353 steps  18 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 263 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 328 steps  19 reward -1 next state  4 agent position  (3, 8)
agent epsilon  0.7810127752406908 agent memory len 354 steps  19 reward -1 next state  3 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 263 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 329 steps  20 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.7810127752406908 agent memory len 355 steps  20 reward -1 next state  3 agent position  (8, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 263 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 330 steps  21 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.7810127752406908 agent memory len 356 steps  21 reward -1 next state  3 agent position  (8, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 263 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 331 steps  22 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.7810127752406908 agent memory len 357 steps  22 reward -1 next state  3 agent position  (8, 4)
 is_terminal [True, False, False]
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 263 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  23 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 358 steps  23 reward -1 next state  4 agent position  (8, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 359 steps  24 reward -1 next state  4 agent position  (8, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 360 steps  25 reward -1 next state  4 agent position  (8, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 361 steps  26 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 362 steps  27 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 363 steps  28 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 364 steps  29 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 365 steps  30 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 366 steps  31 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 367 steps  32 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 368 steps  33 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 369 steps  34 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 370 steps  35 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 371 steps  36 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 372 steps  37 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 373 steps  38 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 374 steps  39 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 375 steps  40 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 376 steps  41 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 377 steps  42 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 378 steps  43 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 379 steps  44 reward -1 next state  4 agent position  (7, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 380 steps  45 reward -1 next state  4 agent position  (7, 1)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 381 steps  46 reward -1 next state  4 agent position  (7, 1)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 382 steps  47 reward -1 next state  4 agent position  (7, 1)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 383 steps  48 reward -1 next state  4 agent position  (6, 1)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 384 steps  49 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 385 steps  50 reward -1 next state  4 agent position  (7, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 386 steps  51 reward -1 next state  4 agent position  (7, 1)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 387 steps  52 reward -1 next state  4 agent position  (7, 0)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 388 steps  53 reward -1 next state  4 agent position  (7, 0)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 389 steps  54 reward -1 next state  4 agent position  (7, 0)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 390 steps  55 reward -1 next state  4 agent position  (7, 0)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 391 steps  56 reward -1 next state  4 agent position  (7, 1)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 392 steps  57 reward -1 next state  4 agent position  (7, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 393 steps  58 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 394 steps  59 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 395 steps  60 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 396 steps  61 reward -1 next state  4 agent position  (7, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 397 steps  62 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 398 steps  63 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 399 steps  64 reward 0 next state  4 agent position  (6, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 400 steps  65 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 401 steps  66 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 402 steps  67 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 263 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 403 steps  68 reward 0 next state  4 agent position  (6, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 263 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 404 steps  69 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 405 steps  70 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 406 steps  71 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 407 steps  72 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  1.0 agent memory len 263 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 408 steps  73 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 263 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 409 steps  74 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7810127752406908 agent memory len 332 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.7810127752406908 agent memory len 410 steps  75 reward -1 next state  4 agent position  (7, 3)
max steps reached
total rewards -73
epsilon  0.7434100384749007
epsilon  0.7434100384749007
epsilon  0.7434100384749007
Episode number:  7
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.7434100384749007 agent memory len 264 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7434100384749007 agent memory len 333 steps  1 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7434100384749007 agent memory len 411 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.7434100384749007 agent memory len 265 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 334 steps  2 reward -1 next state  1 agent position  (0, 9)
agent epsilon  0.7434100384749007 agent memory len 412 steps  2 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 266 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7434100384749007 agent memory len 335 steps  3 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.7434100384749007 agent memory len 413 steps  3 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.7434100384749007 agent memory len 267 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 336 steps  4 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 414 steps  4 reward -1 next state  0 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7434100384749007 agent memory len 268 steps  5 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 337 steps  5 reward -2 next state  1 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 415 steps  5 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 269 steps  6 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7434100384749007 agent memory len 338 steps  6 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 416 steps  6 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.7434100384749007 agent memory len 270 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 339 steps  7 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.7434100384749007 agent memory len 417 steps  7 reward -1 next state  1 agent position  (7, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 271 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7434100384749007 agent memory len 340 steps  8 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 418 steps  8 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 272 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.7434100384749007 agent memory len 341 steps  9 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 419 steps  9 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 273 steps  10 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7434100384749007 agent memory len 342 steps  10 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 420 steps  10 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'R']
agent epsilon  0.7434100384749007 agent memory len 274 steps  11 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7434100384749007 agent memory len 343 steps  11 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 421 steps  11 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.7434100384749007 agent memory len 275 steps  12 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7434100384749007 agent memory len 344 steps  12 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 422 steps  12 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 276 steps  13 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.7434100384749007 agent memory len 345 steps  13 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 423 steps  13 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.7434100384749007 agent memory len 277 steps  14 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7434100384749007 agent memory len 346 steps  14 reward -2 next state  1 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 424 steps  14 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 278 steps  15 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.7434100384749007 agent memory len 347 steps  15 reward -1 next state  1 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 425 steps  15 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 279 steps  16 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 348 steps  16 reward -1 next state  2 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 426 steps  16 reward -1 next state  0 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.7434100384749007 agent memory len 280 steps  17 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.7434100384749007 agent memory len 349 steps  17 reward -1 next state  2 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 427 steps  17 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 281 steps  18 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 350 steps  18 reward -2 next state  2 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 428 steps  18 reward -1 next state  0 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
actions ['U', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 282 steps  19 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 351 steps  19 reward -1 next state  2 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 429 steps  19 reward -1 next state  0 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.7434100384749007 agent memory len 283 steps  20 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 352 steps  20 reward -1 next state  2 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 430 steps  20 reward -1 next state  0 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.7434100384749007 agent memory len 284 steps  21 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 353 steps  21 reward -1 next state  2 agent position  (1, 3)
agent epsilon  0.7434100384749007 agent memory len 431 steps  21 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 285 steps  22 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7434100384749007 agent memory len 354 steps  22 reward -1 next state  3 agent position  (1, 4)
agent epsilon  0.7434100384749007 agent memory len 432 steps  22 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  0.7434100384749007 agent memory len 286 steps  23 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 355 steps  23 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.7434100384749007 agent memory len 433 steps  23 reward -2 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 287 steps  24 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 356 steps  24 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7434100384749007 agent memory len 434 steps  24 reward -1 next state  2 agent position  (8, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 288 steps  25 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 357 steps  25 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.7434100384749007 agent memory len 435 steps  25 reward -1 next state  2 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 289 steps  26 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.7434100384749007 agent memory len 358 steps  26 reward -1 next state  3 agent position  (1, 4)
agent epsilon  0.7434100384749007 agent memory len 436 steps  26 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 290 steps  27 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.7434100384749007 agent memory len 359 steps  27 reward -1 next state  3 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 437 steps  27 reward -1 next state  0 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.7434100384749007 agent memory len 291 steps  28 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.7434100384749007 agent memory len 360 steps  28 reward -1 next state  3 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 438 steps  28 reward -1 next state  0 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  29 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 361 steps  29 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 439 steps  29 reward -1 next state  0 agent position  (9, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 362 steps  30 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 440 steps  30 reward -1 next state  0 agent position  (9, 4)
 is_terminal [True, False, False]
actions ['S', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 363 steps  31 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 441 steps  31 reward -1 next state  0 agent position  (9, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 364 steps  32 reward -1 next state  4 agent position  (1, 5)
agent epsilon  0.7434100384749007 agent memory len 442 steps  32 reward -1 next state  1 agent position  (9, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 365 steps  33 reward -1 next state  4 agent position  (1, 6)
agent epsilon  0.7434100384749007 agent memory len 443 steps  33 reward -1 next state  1 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 366 steps  34 reward -1 next state  4 agent position  (1, 7)
agent epsilon  0.7434100384749007 agent memory len 444 steps  34 reward -1 next state  1 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7434100384749007 agent memory len 292 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 367 steps  35 reward -1 next state  4 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 445 steps  35 reward -1 next state  0 agent position  (9, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 368 steps  36 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 446 steps  36 reward -1 next state  0 agent position  (8, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 369 steps  37 reward -2 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 447 steps  37 reward -1 next state  0 agent position  (7, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 370 steps  38 reward -1 next state  4 agent position  (1, 6)
agent epsilon  0.7434100384749007 agent memory len 448 steps  38 reward -1 next state  1 agent position  (7, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 371 steps  39 reward -1 next state  4 agent position  (1, 5)
agent epsilon  0.7434100384749007 agent memory len 449 steps  39 reward -1 next state  1 agent position  (7, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 372 steps  40 reward -1 next state  4 agent position  (1, 6)
agent epsilon  0.7434100384749007 agent memory len 450 steps  40 reward -1 next state  1 agent position  (7, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 373 steps  41 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 451 steps  41 reward -1 next state  0 agent position  (8, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 374 steps  42 reward -1 next state  4 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 452 steps  42 reward -1 next state  0 agent position  (8, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 375 steps  43 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 453 steps  43 reward -1 next state  0 agent position  (8, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 376 steps  44 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 454 steps  44 reward -1 next state  0 agent position  (9, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 377 steps  45 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 455 steps  45 reward -2 next state  0 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 378 steps  46 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 456 steps  46 reward -1 next state  0 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 379 steps  47 reward -1 next state  4 agent position  (1, 4)
agent epsilon  0.7434100384749007 agent memory len 457 steps  47 reward -1 next state  1 agent position  (9, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 380 steps  48 reward -1 next state  4 agent position  (1, 4)
agent epsilon  0.7434100384749007 agent memory len 458 steps  48 reward -1 next state  1 agent position  (9, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 381 steps  49 reward -1 next state  4 agent position  (1, 3)
agent epsilon  0.7434100384749007 agent memory len 459 steps  49 reward -1 next state  1 agent position  (9, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 382 steps  50 reward -1 next state  4 agent position  (1, 4)
agent epsilon  0.7434100384749007 agent memory len 460 steps  50 reward -1 next state  1 agent position  (9, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7434100384749007 agent memory len 292 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 383 steps  51 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 461 steps  51 reward -1 next state  0 agent position  (9, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 384 steps  52 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 462 steps  52 reward -2 next state  0 agent position  (9, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 385 steps  53 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 463 steps  53 reward -2 next state  0 agent position  (9, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 386 steps  54 reward -2 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 464 steps  54 reward -2 next state  0 agent position  (9, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 387 steps  55 reward -1 next state  4 agent position  (1, 5)
agent epsilon  0.7434100384749007 agent memory len 465 steps  55 reward -1 next state  1 agent position  (9, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 388 steps  56 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 466 steps  56 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 389 steps  57 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 467 steps  57 reward -1 next state  0 agent position  (7, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 390 steps  58 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 468 steps  58 reward -1 next state  0 agent position  (7, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 391 steps  59 reward -2 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 469 steps  59 reward -1 next state  0 agent position  (7, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.7434100384749007 agent memory len 292 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 392 steps  60 reward -1 next state  4 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 470 steps  60 reward -1 next state  0 agent position  (7, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 393 steps  61 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 471 steps  61 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  0.7434100384749007 agent memory len 292 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 394 steps  62 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 472 steps  62 reward -1 next state  0 agent position  (8, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 395 steps  63 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 473 steps  63 reward -1 next state  0 agent position  (7, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 396 steps  64 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 474 steps  64 reward -1 next state  0 agent position  (8, 2)
 is_terminal [True, False, False]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 397 steps  65 reward -2 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 475 steps  65 reward -1 next state  0 agent position  (8, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 398 steps  66 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 476 steps  66 reward -1 next state  0 agent position  (7, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7434100384749007 agent memory len 292 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 399 steps  67 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 477 steps  67 reward -1 next state  0 agent position  (8, 2)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 400 steps  68 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 478 steps  68 reward -1 next state  0 agent position  (8, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.7434100384749007 agent memory len 292 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 401 steps  69 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 479 steps  69 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 402 steps  70 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 480 steps  70 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 403 steps  71 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 481 steps  71 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 404 steps  72 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 482 steps  72 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 405 steps  73 reward -2 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 483 steps  73 reward -1 next state  0 agent position  (8, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7434100384749007 agent memory len 292 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 406 steps  74 reward -1 next state  4 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 484 steps  74 reward -1 next state  0 agent position  (7, 1)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 292 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.7434100384749007 agent memory len 407 steps  75 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 485 steps  75 reward -1 next state  0 agent position  (7, 1)
max steps reached
total rewards -179
epsilon  0.7076412088215263
epsilon  0.7076412088215263
epsilon  0.7076412088215263
Episode number:  8
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 293 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7076412088215263 agent memory len 408 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 486 steps  1 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.7076412088215263 agent memory len 294 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7076412088215263 agent memory len 409 steps  2 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 487 steps  2 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.7076412088215263 agent memory len 295 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7076412088215263 agent memory len 410 steps  3 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 488 steps  3 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'R', 'U']
agent epsilon  0.7076412088215263 agent memory len 296 steps  4 reward -2 next state  0 agent position  (0, 1)
agent epsilon  0.7076412088215263 agent memory len 411 steps  4 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 489 steps  4 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  0.7076412088215263 agent memory len 297 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.7076412088215263 agent memory len 412 steps  5 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 490 steps  5 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'U']
agent epsilon  0.7076412088215263 agent memory len 298 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7076412088215263 agent memory len 413 steps  6 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 491 steps  6 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.7076412088215263 agent memory len 299 steps  7 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7076412088215263 agent memory len 414 steps  7 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 492 steps  7 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.7076412088215263 agent memory len 300 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7076412088215263 agent memory len 415 steps  8 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 493 steps  8 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.7076412088215263 agent memory len 301 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.7076412088215263 agent memory len 416 steps  9 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 494 steps  9 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.7076412088215263 agent memory len 302 steps  10 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7076412088215263 agent memory len 417 steps  10 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 495 steps  10 reward -1 next state  0 agent position  (4, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  0.7076412088215263 agent memory len 303 steps  11 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.7076412088215263 agent memory len 418 steps  11 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 496 steps  11 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'U']
agent epsilon  0.7076412088215263 agent memory len 304 steps  12 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7076412088215263 agent memory len 419 steps  12 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 497 steps  12 reward -1 next state  0 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 305 steps  13 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.7076412088215263 agent memory len 420 steps  13 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 498 steps  13 reward -1 next state  0 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 306 steps  14 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.7076412088215263 agent memory len 421 steps  14 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 499 steps  14 reward -1 next state  1 agent position  (2, 2)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.7076412088215263 agent memory len 307 steps  15 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.7076412088215263 agent memory len 422 steps  15 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.7076412088215263 agent memory len 500 steps  15 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 308 steps  16 reward -2 next state  5 agent position  (5, 0)
agent epsilon  0.7076412088215263 agent memory len 423 steps  16 reward -1 next state  0 agent position  (2, 6)
agent epsilon  0.7076412088215263 agent memory len 501 steps  16 reward -1 next state  2 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  0.7076412088215263 agent memory len 309 steps  17 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.7076412088215263 agent memory len 424 steps  17 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.7076412088215263 agent memory len 502 steps  17 reward -2 next state  2 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.7076412088215263 agent memory len 310 steps  18 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.7076412088215263 agent memory len 425 steps  18 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.7076412088215263 agent memory len 503 steps  18 reward -1 next state  2 agent position  (0, 1)
 is_terminal [False, False, False]
actions ['U', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 311 steps  19 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.7076412088215263 agent memory len 426 steps  19 reward -1 next state  0 agent position  (3, 8)
agent epsilon  0.7076412088215263 agent memory len 504 steps  19 reward -1 next state  3 agent position  (0, 0)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 312 steps  20 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7076412088215263 agent memory len 427 steps  20 reward -1 next state  0 agent position  (4, 8)
agent epsilon  0.7076412088215263 agent memory len 505 steps  20 reward -2 next state  4 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  0.7076412088215263 agent memory len 313 steps  21 reward -2 next state  3 agent position  (3, 0)
agent epsilon  0.7076412088215263 agent memory len 428 steps  21 reward -1 next state  0 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 506 steps  21 reward -1 next state  4 agent position  (0, 0)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 314 steps  22 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7076412088215263 agent memory len 429 steps  22 reward -1 next state  0 agent position  (5, 9)
agent epsilon  0.7076412088215263 agent memory len 507 steps  22 reward -1 next state  5 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  0.7076412088215263 agent memory len 315 steps  23 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7076412088215263 agent memory len 430 steps  23 reward -1 next state  0 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 508 steps  23 reward -1 next state  4 agent position  (2, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.7076412088215263 agent memory len 316 steps  24 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7076412088215263 agent memory len 431 steps  24 reward -2 next state  1 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 509 steps  24 reward -1 next state  4 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.7076412088215263 agent memory len 317 steps  25 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.7076412088215263 agent memory len 432 steps  25 reward -2 next state  2 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 510 steps  25 reward -1 next state  4 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7076412088215263 agent memory len 318 steps  26 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.7076412088215263 agent memory len 433 steps  26 reward -2 next state  2 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 511 steps  26 reward -1 next state  4 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 319 steps  27 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7076412088215263 agent memory len 434 steps  27 reward -1 next state  3 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 512 steps  27 reward -2 next state  4 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
actions ['S', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 320 steps  28 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7076412088215263 agent memory len 435 steps  28 reward -1 next state  3 agent position  (4, 9)
agent epsilon  0.7076412088215263 agent memory len 513 steps  28 reward -1 next state  4 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 321 steps  29 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7076412088215263 agent memory len 436 steps  29 reward -1 next state  3 agent position  (5, 9)
agent epsilon  0.7076412088215263 agent memory len 514 steps  29 reward -1 next state  5 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 322 steps  30 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.7076412088215263 agent memory len 437 steps  30 reward -1 next state  3 agent position  (6, 9)
agent epsilon  0.7076412088215263 agent memory len 515 steps  30 reward -1 next state  6 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 323 steps  31 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.7076412088215263 agent memory len 438 steps  31 reward -1 next state  3 agent position  (7, 9)
agent epsilon  0.7076412088215263 agent memory len 516 steps  31 reward -1 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.7076412088215263 agent memory len 324 steps  32 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.7076412088215263 agent memory len 439 steps  32 reward -1 next state  3 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 517 steps  32 reward -2 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.7076412088215263 agent memory len 325 steps  33 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.7076412088215263 agent memory len 440 steps  33 reward -1 next state  3 agent position  (6, 8)
agent epsilon  0.7076412088215263 agent memory len 518 steps  33 reward -1 next state  6 agent position  (4, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 326 steps  34 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.7076412088215263 agent memory len 441 steps  34 reward -1 next state  3 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 519 steps  34 reward -2 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 327 steps  35 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.7076412088215263 agent memory len 442 steps  35 reward -1 next state  3 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 520 steps  35 reward -1 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 328 steps  36 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.7076412088215263 agent memory len 443 steps  36 reward -1 next state  3 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 521 steps  36 reward -1 next state  7 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 329 steps  37 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.7076412088215263 agent memory len 444 steps  37 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 522 steps  37 reward -2 next state  8 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.7076412088215263 agent memory len 330 steps  38 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.7076412088215263 agent memory len 445 steps  38 reward -1 next state  3 agent position  (8, 7)
agent epsilon  0.7076412088215263 agent memory len 523 steps  38 reward -1 next state  8 agent position  (3, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  0.7076412088215263 agent memory len 331 steps  39 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.7076412088215263 agent memory len 446 steps  39 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 524 steps  39 reward -1 next state  8 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 332 steps  40 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.7076412088215263 agent memory len 447 steps  40 reward -1 next state  2 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 525 steps  40 reward -1 next state  8 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.7076412088215263 agent memory len 333 steps  41 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.7076412088215263 agent memory len 448 steps  41 reward -1 next state  3 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 526 steps  41 reward -1 next state  9 agent position  (3, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 334 steps  42 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.7076412088215263 agent memory len 449 steps  42 reward -2 next state  3 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 527 steps  42 reward -1 next state  9 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 335 steps  43 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.7076412088215263 agent memory len 450 steps  43 reward -2 next state  4 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 528 steps  43 reward -2 next state  9 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 336 steps  44 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 451 steps  44 reward -2 next state  4 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 529 steps  44 reward -2 next state  9 agent position  (2, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 337 steps  45 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 452 steps  45 reward -2 next state  4 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 530 steps  45 reward -1 next state  9 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'L']
agent epsilon  0.7076412088215263 agent memory len 338 steps  46 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.7076412088215263 agent memory len 453 steps  46 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 531 steps  46 reward -2 next state  8 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.7076412088215263 agent memory len 339 steps  47 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.7076412088215263 agent memory len 454 steps  47 reward -1 next state  5 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 532 steps  47 reward -1 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 340 steps  48 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.7076412088215263 agent memory len 455 steps  48 reward -1 next state  6 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 533 steps  48 reward -2 next state  7 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 341 steps  49 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  49 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 534 steps  49 reward -1 next state  8 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.7076412088215263 agent memory len 342 steps  50 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.7076412088215263 agent memory len 457 steps  50 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 535 steps  50 reward -1 next state  7 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'L']
agent epsilon  0.7076412088215263 agent memory len 343 steps  51 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.7076412088215263 agent memory len 458 steps  51 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.7076412088215263 agent memory len 536 steps  51 reward -1 next state  7 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.7076412088215263 agent memory len 344 steps  52 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.7076412088215263 agent memory len 459 steps  52 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 537 steps  52 reward -2 next state  7 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.7076412088215263 agent memory len 345 steps  53 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.7076412088215263 agent memory len 460 steps  53 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.7076412088215263 agent memory len 538 steps  53 reward -1 next state  7 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'L']
agent epsilon  0.7076412088215263 agent memory len 346 steps  54 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.7076412088215263 agent memory len 461 steps  54 reward -1 next state  7 agent position  (6, 7)
agent epsilon  0.7076412088215263 agent memory len 539 steps  54 reward -2 next state  6 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.7076412088215263 agent memory len 347 steps  55 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.7076412088215263 agent memory len 462 steps  55 reward -1 next state  7 agent position  (5, 7)
agent epsilon  0.7076412088215263 agent memory len 540 steps  55 reward -2 next state  5 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 348 steps  56 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.7076412088215263 agent memory len 463 steps  56 reward -1 next state  7 agent position  (6, 7)
agent epsilon  0.7076412088215263 agent memory len 541 steps  56 reward -2 next state  6 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7076412088215263 agent memory len 349 steps  57 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.7076412088215263 agent memory len 464 steps  57 reward -1 next state  7 agent position  (6, 6)
agent epsilon  0.7076412088215263 agent memory len 542 steps  57 reward -1 next state  6 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.7076412088215263 agent memory len 350 steps  58 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 465 steps  58 reward 0 next state  8 agent position  (5, 6)
agent epsilon  0.7076412088215263 agent memory len 543 steps  58 reward -2 next state  5 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'L']
agent epsilon  0.7076412088215263 agent memory len 351 steps  59 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 466 steps  59 reward 0 next state  8 agent position  (5, 5)
agent epsilon  0.7076412088215263 agent memory len 544 steps  59 reward -2 next state  5 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 352 steps  60 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 467 steps  60 reward 0 next state  8 agent position  (5, 5)
agent epsilon  0.7076412088215263 agent memory len 545 steps  60 reward -1 next state  5 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 353 steps  61 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 468 steps  61 reward 0 next state  8 agent position  (6, 5)
agent epsilon  0.7076412088215263 agent memory len 546 steps  61 reward -2 next state  6 agent position  (1, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 354 steps  62 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 469 steps  62 reward -1 next state  8 agent position  (7, 5)
agent epsilon  0.7076412088215263 agent memory len 547 steps  62 reward -2 next state  7 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.7076412088215263 agent memory len 355 steps  63 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 470 steps  63 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.7076412088215263 agent memory len 548 steps  63 reward -1 next state  8 agent position  (1, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'L']
agent epsilon  0.7076412088215263 agent memory len 356 steps  64 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.7076412088215263 agent memory len 471 steps  64 reward -1 next state  9 agent position  (8, 4)
agent epsilon  0.7076412088215263 agent memory len 549 steps  64 reward -1 next state  8 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 357 steps  65 reward -1 next state  9 agent position  (9, 9)
agent epsilon  0.7076412088215263 agent memory len 472 steps  65 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 550 steps  65 reward -2 next state  9 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 358 steps  66 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.7076412088215263 agent memory len 473 steps  66 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 551 steps  66 reward -2 next state  9 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.7076412088215263 agent memory len 359 steps  67 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.7076412088215263 agent memory len 474 steps  67 reward -1 next state  9 agent position  (8, 4)
agent epsilon  0.7076412088215263 agent memory len 552 steps  67 reward -1 next state  8 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 360 steps  68 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7076412088215263 agent memory len 475 steps  68 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 553 steps  68 reward -2 next state  9 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 361 steps  69 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.7076412088215263 agent memory len 476 steps  69 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 554 steps  69 reward -2 next state  9 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'R']
agent epsilon  0.7076412088215263 agent memory len 362 steps  70 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 477 steps  70 reward -2 next state  8 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 555 steps  70 reward -1 next state  9 agent position  (0, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.7076412088215263 agent memory len 363 steps  71 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 478 steps  71 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.7076412088215263 agent memory len 556 steps  71 reward -1 next state  8 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 364 steps  72 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.7076412088215263 agent memory len 479 steps  72 reward -1 next state  8 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 557 steps  72 reward -2 next state  9 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'L']
agent epsilon  0.7076412088215263 agent memory len 365 steps  73 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.7076412088215263 agent memory len 480 steps  73 reward -2 next state  8 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 558 steps  73 reward -1 next state  9 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.7076412088215263 agent memory len 366 steps  74 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 481 steps  74 reward -2 next state  8 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 559 steps  74 reward -2 next state  9 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 367 steps  75 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7076412088215263 agent memory len 482 steps  75 reward -1 next state  8 agent position  (9, 4)
agent epsilon  0.7076412088215263 agent memory len 560 steps  75 reward -2 next state  9 agent position  (0, 0)
max steps reached
total rewards -261
epsilon  0.6736168455752829
epsilon  0.6736168455752829
epsilon  0.6736168455752829
Episode number:  9
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.6736168455752829 agent memory len 368 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 483 steps  1 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 561 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.6736168455752829 agent memory len 369 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6736168455752829 agent memory len 484 steps  2 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.6736168455752829 agent memory len 562 steps  2 reward -1 next state  0 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6736168455752829 agent memory len 370 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6736168455752829 agent memory len 485 steps  3 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.6736168455752829 agent memory len 563 steps  3 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.6736168455752829 agent memory len 371 steps  4 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 486 steps  4 reward -1 next state  2 agent position  (0, 8)
agent epsilon  0.6736168455752829 agent memory len 564 steps  4 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.6736168455752829 agent memory len 372 steps  5 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 487 steps  5 reward -1 next state  3 agent position  (0, 8)
agent epsilon  0.6736168455752829 agent memory len 565 steps  5 reward -1 next state  0 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 373 steps  6 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  6 reward -1 next state  2 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 566 steps  6 reward -1 next state  1 agent position  (7, 2)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 374 steps  7 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 489 steps  7 reward -1 next state  3 agent position  (2, 8)
agent epsilon  0.6736168455752829 agent memory len 567 steps  7 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 375 steps  8 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 490 steps  8 reward -1 next state  4 agent position  (3, 8)
agent epsilon  0.6736168455752829 agent memory len 568 steps  8 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'U']
agent epsilon  0.6736168455752829 agent memory len 376 steps  9 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 491 steps  9 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.6736168455752829 agent memory len 569 steps  9 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 377 steps  10 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 492 steps  10 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.6736168455752829 agent memory len 570 steps  10 reward -1 next state  4 agent position  (7, 2)
 is_terminal [False, False, False]
actions ['R', 'S', 'U']
agent epsilon  0.6736168455752829 agent memory len 378 steps  11 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 493 steps  11 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.6736168455752829 agent memory len 571 steps  11 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.6736168455752829 agent memory len 379 steps  12 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 494 steps  12 reward -1 next state  5 agent position  (3, 7)
agent epsilon  0.6736168455752829 agent memory len 572 steps  12 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'U']
agent epsilon  0.6736168455752829 agent memory len 380 steps  13 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 495 steps  13 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.6736168455752829 agent memory len 573 steps  13 reward -1 next state  3 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 381 steps  14 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 496 steps  14 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.6736168455752829 agent memory len 574 steps  14 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 382 steps  15 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 497 steps  15 reward -1 next state  5 agent position  (1, 7)
agent epsilon  0.6736168455752829 agent memory len 575 steps  15 reward -1 next state  1 agent position  (3, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.6736168455752829 agent memory len 383 steps  16 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 498 steps  16 reward -1 next state  6 agent position  (1, 6)
agent epsilon  0.6736168455752829 agent memory len 576 steps  16 reward -1 next state  1 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6736168455752829 agent memory len 384 steps  17 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.6736168455752829 agent memory len 499 steps  17 reward -1 next state  6 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 577 steps  17 reward -1 next state  2 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'S', 'U']
agent epsilon  0.6736168455752829 agent memory len 385 steps  18 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.6736168455752829 agent memory len 500 steps  18 reward -1 next state  7 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 578 steps  18 reward -1 next state  2 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 386 steps  19 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6736168455752829 agent memory len 501 steps  19 reward 0 next state  8 agent position  (3, 6)
agent epsilon  0.6736168455752829 agent memory len 579 steps  19 reward -1 next state  3 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'U', 'L']
agent epsilon  0.6736168455752829 agent memory len 387 steps  20 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.6736168455752829 agent memory len 502 steps  20 reward -1 next state  7 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 580 steps  20 reward -2 next state  2 agent position  (4, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 388 steps  21 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6736168455752829 agent memory len 503 steps  21 reward -1 next state  8 agent position  (1, 6)
agent epsilon  0.6736168455752829 agent memory len 581 steps  21 reward -1 next state  1 agent position  (3, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'S']
agent epsilon  0.6736168455752829 agent memory len 389 steps  22 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 504 steps  22 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 582 steps  22 reward -1 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 390 steps  23 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 505 steps  23 reward -2 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 583 steps  23 reward -1 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.6736168455752829 agent memory len 391 steps  24 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 506 steps  24 reward -2 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 584 steps  24 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 392 steps  25 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 507 steps  25 reward -2 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 585 steps  25 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'R']
agent epsilon  0.6736168455752829 agent memory len 393 steps  26 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 508 steps  26 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 586 steps  26 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6736168455752829 agent memory len 394 steps  27 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 509 steps  27 reward -1 next state  9 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 587 steps  27 reward 0 next state  1 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 395 steps  28 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 510 steps  28 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 588 steps  28 reward 0 next state  0 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 396 steps  29 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6736168455752829 agent memory len 511 steps  29 reward -2 next state  8 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 589 steps  29 reward 0 next state  0 agent position  (3, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.6736168455752829 agent memory len 397 steps  30 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 512 steps  30 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 590 steps  30 reward 0 next state  0 agent position  (3, 4)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 398 steps  31 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 513 steps  31 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 591 steps  31 reward -1 next state  0 agent position  (2, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.6736168455752829 agent memory len 399 steps  32 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 514 steps  32 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 592 steps  32 reward -1 next state  0 agent position  (2, 3)
 is_terminal [False, False, False]
random action 0
actions ['D', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 400 steps  33 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 515 steps  33 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 593 steps  33 reward 0 next state  0 agent position  (3, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 401 steps  34 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 516 steps  34 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 594 steps  34 reward 0 next state  0 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 402 steps  35 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 517 steps  35 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 595 steps  35 reward 0 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 403 steps  36 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6736168455752829 agent memory len 518 steps  36 reward -1 next state  8 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 596 steps  36 reward 0 next state  1 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 404 steps  37 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.6736168455752829 agent memory len 519 steps  37 reward -1 next state  7 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 597 steps  37 reward 0 next state  2 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'U', 'L']
agent epsilon  0.6736168455752829 agent memory len 405 steps  38 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 520 steps  38 reward -1 next state  6 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 598 steps  38 reward -1 next state  1 agent position  (5, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.6736168455752829 agent memory len 406 steps  39 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.6736168455752829 agent memory len 521 steps  39 reward -1 next state  7 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 599 steps  39 reward 0 next state  1 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.6736168455752829 agent memory len 407 steps  40 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6736168455752829 agent memory len 522 steps  40 reward -1 next state  7 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 600 steps  40 reward 0 next state  1 agent position  (4, 3)
 is_terminal [False, False, False]
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 408 steps  41 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 523 steps  41 reward -1 next state  8 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 601 steps  41 reward 0 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 409 steps  42 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 524 steps  42 reward -1 next state  8 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 602 steps  42 reward 0 next state  1 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'L']
agent epsilon  0.6736168455752829 agent memory len 410 steps  43 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6736168455752829 agent memory len 525 steps  43 reward -1 next state  7 agent position  (1, 6)
agent epsilon  0.6736168455752829 agent memory len 603 steps  43 reward -1 next state  1 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 411 steps  44 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 526 steps  44 reward -1 next state  8 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 604 steps  44 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.6736168455752829 agent memory len 412 steps  45 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6736168455752829 agent memory len 527 steps  45 reward -1 next state  7 agent position  (1, 6)
agent epsilon  0.6736168455752829 agent memory len 605 steps  45 reward -1 next state  1 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 413 steps  46 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 528 steps  46 reward -1 next state  8 agent position  (2, 6)
agent epsilon  0.6736168455752829 agent memory len 606 steps  46 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 414 steps  47 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6736168455752829 agent memory len 529 steps  47 reward 0 next state  7 agent position  (3, 6)
agent epsilon  0.6736168455752829 agent memory len 607 steps  47 reward -1 next state  3 agent position  (3, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 415 steps  48 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 530 steps  48 reward 0 next state  8 agent position  (4, 6)
agent epsilon  0.6736168455752829 agent memory len 608 steps  48 reward -1 next state  4 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.6736168455752829 agent memory len 416 steps  49 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6736168455752829 agent memory len 531 steps  49 reward 0 next state  8 agent position  (3, 6)
agent epsilon  0.6736168455752829 agent memory len 609 steps  49 reward -1 next state  3 agent position  (4, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.6736168455752829 agent memory len 417 steps  50 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 532 steps  50 reward 0 next state  9 agent position  (3, 6)
agent epsilon  0.6736168455752829 agent memory len 610 steps  50 reward -1 next state  3 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'D']
agent epsilon  0.6736168455752829 agent memory len 418 steps  51 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 533 steps  51 reward 0 next state  9 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 611 steps  51 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  0.6736168455752829 agent memory len 419 steps  52 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 534 steps  52 reward 0 next state  8 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 612 steps  52 reward -1 next state  3 agent position  (6, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.6736168455752829 agent memory len 420 steps  53 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 535 steps  53 reward 0 next state  9 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 613 steps  53 reward 0 next state  3 agent position  (6, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 421 steps  54 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 536 steps  54 reward -1 next state  9 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 614 steps  54 reward 0 next state  2 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 422 steps  55 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 537 steps  55 reward 0 next state  9 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 615 steps  55 reward 0 next state  3 agent position  (6, 3)
 is_terminal [False, False, False]
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 423 steps  56 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 538 steps  56 reward -1 next state  9 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 616 steps  56 reward -1 next state  2 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 424 steps  57 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 539 steps  57 reward -1 next state  9 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 617 steps  57 reward 0 next state  1 agent position  (6, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 425 steps  58 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 540 steps  58 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 618 steps  58 reward 0 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'U', 'S']
agent epsilon  0.6736168455752829 agent memory len 426 steps  59 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 541 steps  59 reward -2 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 619 steps  59 reward 0 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.6736168455752829 agent memory len 427 steps  60 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 542 steps  60 reward -1 next state  9 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 620 steps  60 reward 0 next state  1 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.6736168455752829 agent memory len 428 steps  61 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 543 steps  61 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 621 steps  61 reward -1 next state  0 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.6736168455752829 agent memory len 429 steps  62 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6736168455752829 agent memory len 544 steps  62 reward -1 next state  9 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 622 steps  62 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'R', 'U']
agent epsilon  0.6736168455752829 agent memory len 430 steps  63 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 545 steps  63 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 623 steps  63 reward -1 next state  0 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.6736168455752829 agent memory len 431 steps  64 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 546 steps  64 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 624 steps  64 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.6736168455752829 agent memory len 432 steps  65 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6736168455752829 agent memory len 547 steps  65 reward -1 next state  8 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 625 steps  65 reward -1 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.6736168455752829 agent memory len 433 steps  66 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6736168455752829 agent memory len 548 steps  66 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 626 steps  66 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 434 steps  67 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 549 steps  67 reward -2 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 627 steps  67 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 435 steps  68 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 550 steps  68 reward -2 next state  9 agent position  (0, 6)
agent epsilon  0.6736168455752829 agent memory len 628 steps  68 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.6736168455752829 agent memory len 436 steps  69 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 551 steps  69 reward -1 next state  9 agent position  (0, 5)
agent epsilon  0.6736168455752829 agent memory len 629 steps  69 reward -1 next state  0 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'D']
agent epsilon  0.6736168455752829 agent memory len 437 steps  70 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 552 steps  70 reward -1 next state  9 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 630 steps  70 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 438 steps  71 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6736168455752829 agent memory len 553 steps  71 reward -1 next state  9 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 631 steps  71 reward -1 next state  1 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6736168455752829 agent memory len 439 steps  72 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6736168455752829 agent memory len 554 steps  72 reward -1 next state  9 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 632 steps  72 reward -1 next state  2 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.6736168455752829 agent memory len 440 steps  73 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6736168455752829 agent memory len 555 steps  73 reward -1 next state  8 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 633 steps  73 reward -1 next state  1 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6736168455752829 agent memory len 441 steps  74 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6736168455752829 agent memory len 556 steps  74 reward -1 next state  8 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 634 steps  74 reward -1 next state  2 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 442 steps  75 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6736168455752829 agent memory len 557 steps  75 reward -1 next state  8 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 635 steps  75 reward -1 next state  1 agent position  (6, 0)
max steps reached
total rewards -225
epsilon  0.6412518701055556
epsilon  0.6412518701055556
epsilon  0.6412518701055556
Episode number:  10
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 443 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6412518701055556 agent memory len 558 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 636 steps  1 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'R', 'U']
agent epsilon  0.6412518701055556 agent memory len 444 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6412518701055556 agent memory len 559 steps  2 reward -2 next state  0 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 637 steps  2 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 445 steps  3 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6412518701055556 agent memory len 560 steps  3 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 638 steps  3 reward -2 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 446 steps  4 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.6412518701055556 agent memory len 561 steps  4 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 639 steps  4 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'U']
agent epsilon  0.6412518701055556 agent memory len 447 steps  5 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.6412518701055556 agent memory len 562 steps  5 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 640 steps  5 reward -1 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 448 steps  6 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.6412518701055556 agent memory len 563 steps  6 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 641 steps  6 reward -2 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 449 steps  7 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.6412518701055556 agent memory len 564 steps  7 reward -1 next state  2 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 642 steps  7 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.6412518701055556 agent memory len 450 steps  8 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 565 steps  8 reward -1 next state  3 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 643 steps  8 reward -1 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
actions ['U', 'D', 'U']
agent epsilon  0.6412518701055556 agent memory len 451 steps  9 reward -2 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 566 steps  9 reward -1 next state  3 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 644 steps  9 reward -1 next state  1 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
actions ['U', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 452 steps  10 reward -2 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 567 steps  10 reward -1 next state  3 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 645 steps  10 reward -1 next state  0 agent position  (2, 0)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'S']
agent epsilon  0.6412518701055556 agent memory len 453 steps  11 reward -2 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 568 steps  11 reward -1 next state  3 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 646 steps  11 reward -1 next state  1 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.6412518701055556 agent memory len 454 steps  12 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6412518701055556 agent memory len 569 steps  12 reward -1 next state  4 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 647 steps  12 reward -2 next state  0 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.6412518701055556 agent memory len 455 steps  13 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 570 steps  13 reward -2 next state  5 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 648 steps  13 reward -1 next state  0 agent position  (1, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 456 steps  14 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 571 steps  14 reward -1 next state  5 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 649 steps  14 reward -1 next state  1 agent position  (1, 1)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.6412518701055556 agent memory len 457 steps  15 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 572 steps  15 reward -1 next state  5 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 650 steps  15 reward -1 next state  2 agent position  (0, 1)
 is_terminal [False, False, False]
random action 1
actions ['U', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 458 steps  16 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 573 steps  16 reward -1 next state  5 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 651 steps  16 reward -1 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 459 steps  17 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 574 steps  17 reward -1 next state  5 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 652 steps  17 reward -1 next state  0 agent position  (0, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  0.6412518701055556 agent memory len 460 steps  18 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6412518701055556 agent memory len 575 steps  18 reward -2 next state  5 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 653 steps  18 reward -1 next state  0 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'S']
agent epsilon  0.6412518701055556 agent memory len 461 steps  19 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 576 steps  19 reward -1 next state  5 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 654 steps  19 reward -1 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 462 steps  20 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 577 steps  20 reward -2 next state  5 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 655 steps  20 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 463 steps  21 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 578 steps  21 reward -1 next state  5 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 656 steps  21 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 464 steps  22 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.6412518701055556 agent memory len 579 steps  22 reward -1 next state  5 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 657 steps  22 reward -1 next state  0 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.6412518701055556 agent memory len 465 steps  23 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6412518701055556 agent memory len 580 steps  23 reward -2 next state  6 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 658 steps  23 reward -1 next state  0 agent position  (0, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 466 steps  24 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6412518701055556 agent memory len 581 steps  24 reward -1 next state  6 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 659 steps  24 reward -1 next state  1 agent position  (1, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 467 steps  25 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6412518701055556 agent memory len 582 steps  25 reward -1 next state  6 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 660 steps  25 reward -1 next state  0 agent position  (1, 4)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 468 steps  26 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.6412518701055556 agent memory len 583 steps  26 reward -1 next state  6 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 661 steps  26 reward -1 next state  1 agent position  (1, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 469 steps  27 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6412518701055556 agent memory len 584 steps  27 reward -1 next state  7 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 662 steps  27 reward -1 next state  1 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.6412518701055556 agent memory len 470 steps  28 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 585 steps  28 reward -1 next state  8 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 663 steps  28 reward -1 next state  0 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 471 steps  29 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 586 steps  29 reward -1 next state  8 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 664 steps  29 reward -1 next state  1 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 472 steps  30 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 587 steps  30 reward -1 next state  8 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 665 steps  30 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 473 steps  31 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 588 steps  31 reward -1 next state  9 agent position  (3, 9)
agent epsilon  0.6412518701055556 agent memory len 666 steps  31 reward -1 next state  3 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 474 steps  32 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 589 steps  32 reward -1 next state  8 agent position  (4, 9)
agent epsilon  0.6412518701055556 agent memory len 667 steps  32 reward 0 next state  4 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 475 steps  33 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 590 steps  33 reward -1 next state  8 agent position  (3, 9)
agent epsilon  0.6412518701055556 agent memory len 668 steps  33 reward -1 next state  3 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 476 steps  34 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 591 steps  34 reward -1 next state  8 agent position  (4, 9)
agent epsilon  0.6412518701055556 agent memory len 669 steps  34 reward -1 next state  4 agent position  (3, 8)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 477 steps  35 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 592 steps  35 reward -1 next state  9 agent position  (4, 8)
agent epsilon  0.6412518701055556 agent memory len 670 steps  35 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 478 steps  36 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 593 steps  36 reward -1 next state  9 agent position  (3, 8)
agent epsilon  0.6412518701055556 agent memory len 671 steps  36 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  0.6412518701055556 agent memory len 479 steps  37 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 594 steps  37 reward -1 next state  9 agent position  (3, 9)
agent epsilon  0.6412518701055556 agent memory len 672 steps  37 reward -1 next state  3 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 480 steps  38 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 595 steps  38 reward -1 next state  9 agent position  (3, 8)
agent epsilon  0.6412518701055556 agent memory len 673 steps  38 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 481 steps  39 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 596 steps  39 reward -1 next state  9 agent position  (3, 8)
agent epsilon  0.6412518701055556 agent memory len 674 steps  39 reward -1 next state  3 agent position  (2, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.6412518701055556 agent memory len 482 steps  40 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 597 steps  40 reward -1 next state  9 agent position  (4, 8)
agent epsilon  0.6412518701055556 agent memory len 675 steps  40 reward -1 next state  4 agent position  (1, 8)
 is_terminal [False, False, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 483 steps  41 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 598 steps  41 reward -1 next state  9 agent position  (4, 8)
agent epsilon  0.6412518701055556 agent memory len 676 steps  41 reward -1 next state  4 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 484 steps  42 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 599 steps  42 reward -1 next state  9 agent position  (4, 8)
agent epsilon  0.6412518701055556 agent memory len 677 steps  42 reward -1 next state  4 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.6412518701055556 agent memory len 485 steps  43 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 600 steps  43 reward -1 next state  9 agent position  (4, 7)
agent epsilon  0.6412518701055556 agent memory len 678 steps  43 reward -1 next state  4 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 486 steps  44 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6412518701055556 agent memory len 601 steps  44 reward -1 next state  8 agent position  (3, 7)
agent epsilon  0.6412518701055556 agent memory len 679 steps  44 reward -1 next state  3 agent position  (0, 7)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 487 steps  45 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 602 steps  45 reward -1 next state  9 agent position  (3, 7)
agent epsilon  0.6412518701055556 agent memory len 680 steps  45 reward -1 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 488 steps  46 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6412518701055556 agent memory len 603 steps  46 reward 0 next state  8 agent position  (3, 6)
agent epsilon  0.6412518701055556 agent memory len 681 steps  46 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 489 steps  47 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.6412518701055556 agent memory len 604 steps  47 reward -1 next state  7 agent position  (2, 6)
agent epsilon  0.6412518701055556 agent memory len 682 steps  47 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 490 steps  48 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6412518701055556 agent memory len 605 steps  48 reward -1 next state  8 agent position  (1, 6)
agent epsilon  0.6412518701055556 agent memory len 683 steps  48 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 491 steps  49 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 606 steps  49 reward -1 next state  9 agent position  (1, 5)
agent epsilon  0.6412518701055556 agent memory len 684 steps  49 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'D']
agent epsilon  0.6412518701055556 agent memory len 492 steps  50 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6412518701055556 agent memory len 607 steps  50 reward -1 next state  9 agent position  (1, 4)
agent epsilon  0.6412518701055556 agent memory len 685 steps  50 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.6412518701055556 agent memory len 493 steps  51 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6412518701055556 agent memory len 608 steps  51 reward -1 next state  8 agent position  (1, 3)
agent epsilon  0.6412518701055556 agent memory len 686 steps  51 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['U', 'L', 'L']
agent epsilon  0.6412518701055556 agent memory len 494 steps  52 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 609 steps  52 reward -1 next state  8 agent position  (1, 2)
agent epsilon  0.6412518701055556 agent memory len 687 steps  52 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 495 steps  53 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6412518701055556 agent memory len 610 steps  53 reward -1 next state  8 agent position  (2, 2)
agent epsilon  0.6412518701055556 agent memory len 688 steps  53 reward -1 next state  2 agent position  (1, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 496 steps  54 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 611 steps  54 reward -1 next state  8 agent position  (2, 2)
agent epsilon  0.6412518701055556 agent memory len 689 steps  54 reward -1 next state  2 agent position  (0, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  0.6412518701055556 agent memory len 497 steps  55 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 612 steps  55 reward -1 next state  8 agent position  (1, 2)
agent epsilon  0.6412518701055556 agent memory len 690 steps  55 reward -1 next state  1 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 498 steps  56 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.6412518701055556 agent memory len 613 steps  56 reward -1 next state  7 agent position  (1, 3)
agent epsilon  0.6412518701055556 agent memory len 691 steps  56 reward -1 next state  1 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'D']
agent epsilon  0.6412518701055556 agent memory len 499 steps  57 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.6412518701055556 agent memory len 614 steps  57 reward -1 next state  7 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 692 steps  57 reward -1 next state  0 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'R']
agent epsilon  0.6412518701055556 agent memory len 500 steps  58 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.6412518701055556 agent memory len 615 steps  58 reward -2 next state  7 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 693 steps  58 reward -1 next state  0 agent position  (2, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 501 steps  59 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.6412518701055556 agent memory len 616 steps  59 reward -2 next state  7 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 694 steps  59 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.6412518701055556 agent memory len 502 steps  60 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6412518701055556 agent memory len 617 steps  60 reward -2 next state  7 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 695 steps  60 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 503 steps  61 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6412518701055556 agent memory len 618 steps  61 reward -1 next state  8 agent position  (1, 3)
agent epsilon  0.6412518701055556 agent memory len 696 steps  61 reward -1 next state  1 agent position  (1, 6)
 is_terminal [False, False, False]
random action 1
actions ['U', 'L', 'D']
agent epsilon  0.6412518701055556 agent memory len 504 steps  62 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 619 steps  62 reward -1 next state  8 agent position  (1, 2)
agent epsilon  0.6412518701055556 agent memory len 697 steps  62 reward -1 next state  1 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.6412518701055556 agent memory len 505 steps  63 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 620 steps  63 reward -1 next state  9 agent position  (2, 2)
agent epsilon  0.6412518701055556 agent memory len 698 steps  63 reward -1 next state  2 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 506 steps  64 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 621 steps  64 reward -1 next state  9 agent position  (2, 3)
agent epsilon  0.6412518701055556 agent memory len 699 steps  64 reward 0 next state  2 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 507 steps  65 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 622 steps  65 reward -1 next state  9 agent position  (2, 2)
agent epsilon  0.6412518701055556 agent memory len 700 steps  65 reward -1 next state  2 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.6412518701055556 agent memory len 508 steps  66 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.6412518701055556 agent memory len 623 steps  66 reward -1 next state  9 agent position  (2, 1)
agent epsilon  0.6412518701055556 agent memory len 701 steps  66 reward -1 next state  2 agent position  (3, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.6412518701055556 agent memory len 509 steps  67 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 624 steps  67 reward -1 next state  9 agent position  (3, 1)
agent epsilon  0.6412518701055556 agent memory len 702 steps  67 reward -1 next state  3 agent position  (2, 7)
 is_terminal [False, False, False]
actions ['U', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 510 steps  68 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 625 steps  68 reward -1 next state  9 agent position  (4, 1)
agent epsilon  0.6412518701055556 agent memory len 703 steps  68 reward -1 next state  4 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.6412518701055556 agent memory len 511 steps  69 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 626 steps  69 reward -1 next state  9 agent position  (4, 0)
agent epsilon  0.6412518701055556 agent memory len 704 steps  69 reward -1 next state  4 agent position  (3, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  0.6412518701055556 agent memory len 512 steps  70 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 627 steps  70 reward -2 next state  9 agent position  (4, 0)
agent epsilon  0.6412518701055556 agent memory len 705 steps  70 reward -1 next state  4 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 513 steps  71 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 628 steps  71 reward -1 next state  8 agent position  (5, 0)
agent epsilon  0.6412518701055556 agent memory len 706 steps  71 reward -1 next state  5 agent position  (3, 8)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 514 steps  72 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.6412518701055556 agent memory len 629 steps  72 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.6412518701055556 agent memory len 707 steps  72 reward -1 next state  6 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'S']
agent epsilon  0.6412518701055556 agent memory len 515 steps  73 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 630 steps  73 reward -1 next state  9 agent position  (5, 0)
agent epsilon  0.6412518701055556 agent memory len 708 steps  73 reward -1 next state  5 agent position  (3, 7)
 is_terminal [False, False, False]
random action 1
actions ['U', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 516 steps  74 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 631 steps  74 reward -1 next state  9 agent position  (5, 0)
agent epsilon  0.6412518701055556 agent memory len 709 steps  74 reward -1 next state  5 agent position  (4, 7)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 517 steps  75 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.6412518701055556 agent memory len 632 steps  75 reward -1 next state  9 agent position  (6, 0)
agent epsilon  0.6412518701055556 agent memory len 710 steps  75 reward 0 next state  6 agent position  (4, 6)
max steps reached
total rewards -262
epsilon  0.6104653531155071
epsilon  0.6104653531155071
epsilon  0.6104653531155071
Episode number:  11
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 518 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.6104653531155071 agent memory len 633 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.6104653531155071 agent memory len 711 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 519 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6104653531155071 agent memory len 634 steps  2 reward -2 next state  0 agent position  (1, 9)
agent epsilon  0.6104653531155071 agent memory len 712 steps  2 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 520 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6104653531155071 agent memory len 635 steps  3 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 713 steps  3 reward -1 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 521 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6104653531155071 agent memory len 636 steps  4 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 714 steps  4 reward -1 next state  3 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.6104653531155071 agent memory len 522 steps  5 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6104653531155071 agent memory len 637 steps  5 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 715 steps  5 reward -2 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 523 steps  6 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6104653531155071 agent memory len 638 steps  6 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 716 steps  6 reward -1 next state  3 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 524 steps  7 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6104653531155071 agent memory len 639 steps  7 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 717 steps  7 reward -1 next state  4 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 525 steps  8 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6104653531155071 agent memory len 640 steps  8 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.6104653531155071 agent memory len 718 steps  8 reward -1 next state  5 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.6104653531155071 agent memory len 526 steps  9 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6104653531155071 agent memory len 641 steps  9 reward -1 next state  4 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 719 steps  9 reward -1 next state  6 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.6104653531155071 agent memory len 527 steps  10 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6104653531155071 agent memory len 642 steps  10 reward -1 next state  4 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 720 steps  10 reward -2 next state  7 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 528 steps  11 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6104653531155071 agent memory len 643 steps  11 reward -1 next state  5 agent position  (8, 9)
agent epsilon  0.6104653531155071 agent memory len 721 steps  11 reward -1 next state  8 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['R', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 529 steps  12 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 644 steps  12 reward -1 next state  6 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 722 steps  12 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  0.6104653531155071 agent memory len 530 steps  13 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 645 steps  13 reward -2 next state  6 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 723 steps  13 reward -2 next state  7 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
actions ['L', 'L', 'U']
agent epsilon  0.6104653531155071 agent memory len 531 steps  14 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6104653531155071 agent memory len 646 steps  14 reward -1 next state  5 agent position  (7, 8)
agent epsilon  0.6104653531155071 agent memory len 724 steps  14 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 532 steps  15 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 647 steps  15 reward -1 next state  6 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 725 steps  15 reward -1 next state  7 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 533 steps  16 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6104653531155071 agent memory len 648 steps  16 reward -1 next state  5 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 726 steps  16 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 534 steps  17 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6104653531155071 agent memory len 649 steps  17 reward -1 next state  5 agent position  (8, 9)
agent epsilon  0.6104653531155071 agent memory len 727 steps  17 reward -1 next state  8 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['R', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 535 steps  18 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.6104653531155071 agent memory len 650 steps  18 reward -1 next state  6 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 728 steps  18 reward -1 next state  7 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 536 steps  19 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 651 steps  19 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 729 steps  19 reward -1 next state  7 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.6104653531155071 agent memory len 537 steps  20 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 652 steps  20 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 730 steps  20 reward -1 next state  7 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 538 steps  21 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 653 steps  21 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.6104653531155071 agent memory len 731 steps  21 reward -1 next state  7 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 539 steps  22 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.6104653531155071 agent memory len 654 steps  22 reward -1 next state  7 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 732 steps  22 reward -1 next state  6 agent position  (3, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 540 steps  23 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 655 steps  23 reward -2 next state  8 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 733 steps  23 reward -1 next state  6 agent position  (2, 1)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'S']
agent epsilon  0.6104653531155071 agent memory len 541 steps  24 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 656 steps  24 reward -1 next state  9 agent position  (5, 9)
agent epsilon  0.6104653531155071 agent memory len 734 steps  24 reward -1 next state  5 agent position  (2, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 542 steps  25 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 657 steps  25 reward -1 next state  9 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 735 steps  25 reward -1 next state  4 agent position  (1, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 543 steps  26 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 658 steps  26 reward -1 next state  9 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 736 steps  26 reward -1 next state  4 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 544 steps  27 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 659 steps  27 reward -1 next state  9 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 737 steps  27 reward -1 next state  4 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.6104653531155071 agent memory len 545 steps  28 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 660 steps  28 reward -1 next state  9 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 738 steps  28 reward -1 next state  3 agent position  (0, 2)
 is_terminal [False, False, False]
actions ['R', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 546 steps  29 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 661 steps  29 reward -1 next state  9 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 739 steps  29 reward -2 next state  2 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
actions ['R', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 547 steps  30 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 662 steps  30 reward -1 next state  9 agent position  (1, 9)
agent epsilon  0.6104653531155071 agent memory len 740 steps  30 reward -2 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.6104653531155071 agent memory len 548 steps  31 reward -2 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 663 steps  31 reward -1 next state  9 agent position  (0, 9)
agent epsilon  0.6104653531155071 agent memory len 741 steps  31 reward -1 next state  0 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  0.6104653531155071 agent memory len 549 steps  32 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6104653531155071 agent memory len 664 steps  32 reward -1 next state  8 agent position  (0, 8)
agent epsilon  0.6104653531155071 agent memory len 742 steps  32 reward -1 next state  0 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 550 steps  33 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6104653531155071 agent memory len 665 steps  33 reward -2 next state  8 agent position  (0, 8)
agent epsilon  0.6104653531155071 agent memory len 743 steps  33 reward -2 next state  0 agent position  (0, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6104653531155071 agent memory len 551 steps  34 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 666 steps  34 reward -1 next state  8 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 744 steps  34 reward -2 next state  1 agent position  (0, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.6104653531155071 agent memory len 552 steps  35 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.6104653531155071 agent memory len 667 steps  35 reward -1 next state  8 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 745 steps  35 reward -2 next state  1 agent position  (0, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 553 steps  36 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 668 steps  36 reward -1 next state  8 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 746 steps  36 reward -1 next state  1 agent position  (0, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 554 steps  37 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6104653531155071 agent memory len 669 steps  37 reward -1 next state  8 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 747 steps  37 reward -1 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 555 steps  38 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 670 steps  38 reward -1 next state  8 agent position  (0, 7)
agent epsilon  0.6104653531155071 agent memory len 748 steps  38 reward -2 next state  0 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.6104653531155071 agent memory len 556 steps  39 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.6104653531155071 agent memory len 671 steps  39 reward -1 next state  8 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 749 steps  39 reward -1 next state  0 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
actions ['U', 'R', 'S']
agent epsilon  0.6104653531155071 agent memory len 557 steps  40 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 672 steps  40 reward -1 next state  8 agent position  (0, 7)
agent epsilon  0.6104653531155071 agent memory len 750 steps  40 reward -1 next state  0 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.6104653531155071 agent memory len 558 steps  41 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.6104653531155071 agent memory len 673 steps  41 reward -1 next state  8 agent position  (0, 8)
agent epsilon  0.6104653531155071 agent memory len 751 steps  41 reward -1 next state  0 agent position  (0, 3)
 is_terminal [False, False, False]
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 559 steps  42 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 674 steps  42 reward -1 next state  8 agent position  (0, 8)
agent epsilon  0.6104653531155071 agent memory len 752 steps  42 reward -2 next state  0 agent position  (0, 3)
 is_terminal [False, False, False]
random action 1
actions ['U', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 560 steps  43 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.6104653531155071 agent memory len 675 steps  43 reward -2 next state  8 agent position  (0, 8)
agent epsilon  0.6104653531155071 agent memory len 753 steps  43 reward -2 next state  0 agent position  (0, 3)
 is_terminal [False, False, False]
random action 2
actions ['U', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 561 steps  44 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 676 steps  44 reward -1 next state  8 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 754 steps  44 reward -1 next state  1 agent position  (1, 3)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 562 steps  45 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 677 steps  45 reward -1 next state  8 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 755 steps  45 reward -1 next state  2 agent position  (0, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'R', 'U']
agent epsilon  0.6104653531155071 agent memory len 563 steps  46 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 678 steps  46 reward -1 next state  8 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 756 steps  46 reward -2 next state  2 agent position  (0, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  0.6104653531155071 agent memory len 564 steps  47 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 679 steps  47 reward -2 next state  8 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 757 steps  47 reward -1 next state  2 agent position  (0, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.6104653531155071 agent memory len 565 steps  48 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 680 steps  48 reward -1 next state  9 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 758 steps  48 reward -1 next state  2 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 566 steps  49 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 681 steps  49 reward -1 next state  9 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 759 steps  49 reward -2 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.6104653531155071 agent memory len 567 steps  50 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 682 steps  50 reward -1 next state  9 agent position  (1, 9)
agent epsilon  0.6104653531155071 agent memory len 760 steps  50 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'U']
agent epsilon  0.6104653531155071 agent memory len 568 steps  51 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 683 steps  51 reward -1 next state  9 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 761 steps  51 reward -1 next state  1 agent position  (0, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 569 steps  52 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 684 steps  52 reward -1 next state  9 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 762 steps  52 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 570 steps  53 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.6104653531155071 agent memory len 685 steps  53 reward -1 next state  9 agent position  (2, 8)
agent epsilon  0.6104653531155071 agent memory len 763 steps  53 reward -1 next state  2 agent position  (0, 2)
 is_terminal [False, False, False]
random action 2
actions ['D', 'U', 'D']
agent epsilon  0.6104653531155071 agent memory len 571 steps  54 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.6104653531155071 agent memory len 686 steps  54 reward -1 next state  9 agent position  (1, 8)
agent epsilon  0.6104653531155071 agent memory len 764 steps  54 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.6104653531155071 agent memory len 572 steps  55 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 687 steps  55 reward -1 next state  9 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 765 steps  55 reward -1 next state  1 agent position  (2, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 573 steps  56 reward -2 next state  4 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 688 steps  56 reward -1 next state  9 agent position  (1, 7)
agent epsilon  0.6104653531155071 agent memory len 766 steps  56 reward -1 next state  1 agent position  (2, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  0.6104653531155071 agent memory len 574 steps  57 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 689 steps  57 reward -1 next state  8 agent position  (1, 6)
agent epsilon  0.6104653531155071 agent memory len 767 steps  57 reward -1 next state  1 agent position  (2, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 575 steps  58 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 690 steps  58 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 768 steps  58 reward -1 next state  0 agent position  (1, 4)
 is_terminal [False, False, False]
random action 0
actions ['S', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 576 steps  59 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.6104653531155071 agent memory len 691 steps  59 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 769 steps  59 reward -1 next state  0 agent position  (2, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 577 steps  60 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.6104653531155071 agent memory len 692 steps  60 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 770 steps  60 reward -1 next state  0 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 578 steps  61 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 693 steps  61 reward -1 next state  9 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 771 steps  61 reward 0 next state  0 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 579 steps  62 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.6104653531155071 agent memory len 694 steps  62 reward -1 next state  8 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 772 steps  62 reward 0 next state  0 agent position  (3, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 580 steps  63 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.6104653531155071 agent memory len 695 steps  63 reward -1 next state  8 agent position  (1, 6)
agent epsilon  0.6104653531155071 agent memory len 773 steps  63 reward -1 next state  1 agent position  (2, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 581 steps  64 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 696 steps  64 reward -1 next state  9 agent position  (1, 6)
agent epsilon  0.6104653531155071 agent memory len 774 steps  64 reward 0 next state  1 agent position  (3, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.6104653531155071 agent memory len 582 steps  65 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.6104653531155071 agent memory len 697 steps  65 reward -1 next state  9 agent position  (1, 5)
agent epsilon  0.6104653531155071 agent memory len 775 steps  65 reward 0 next state  1 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'D']
agent epsilon  0.6104653531155071 agent memory len 583 steps  66 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.6104653531155071 agent memory len 698 steps  66 reward -1 next state  8 agent position  (0, 5)
agent epsilon  0.6104653531155071 agent memory len 776 steps  66 reward 0 next state  0 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.6104653531155071 agent memory len 584 steps  67 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.6104653531155071 agent memory len 699 steps  67 reward -1 next state  8 agent position  (1, 5)
agent epsilon  0.6104653531155071 agent memory len 777 steps  67 reward 0 next state  1 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 585 steps  68 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.6104653531155071 agent memory len 700 steps  68 reward -1 next state  8 agent position  (2, 5)
agent epsilon  0.6104653531155071 agent memory len 778 steps  68 reward 0 next state  2 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'U']
agent epsilon  0.6104653531155071 agent memory len 586 steps  69 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.6104653531155071 agent memory len 701 steps  69 reward 0 next state  8 agent position  (3, 5)
agent epsilon  0.6104653531155071 agent memory len 779 steps  69 reward 0 next state  3 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'U']
agent epsilon  0.6104653531155071 agent memory len 587 steps  70 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.6104653531155071 agent memory len 702 steps  70 reward -1 next state  7 agent position  (2, 5)
agent epsilon  0.6104653531155071 agent memory len 780 steps  70 reward 0 next state  2 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 588 steps  71 reward 0 next state  5 agent position  (5, 6)
agent epsilon  0.6104653531155071 agent memory len 703 steps  71 reward 0 next state  6 agent position  (3, 5)
agent epsilon  0.6104653531155071 agent memory len 781 steps  71 reward 0 next state  3 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.6104653531155071 agent memory len 589 steps  72 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.6104653531155071 agent memory len 704 steps  72 reward -1 next state  6 agent position  (2, 5)
agent epsilon  0.6104653531155071 agent memory len 782 steps  72 reward -1 next state  2 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 590 steps  73 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.6104653531155071 agent memory len 705 steps  73 reward 0 next state  7 agent position  (3, 5)
agent epsilon  0.6104653531155071 agent memory len 783 steps  73 reward 0 next state  3 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 591 steps  74 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.6104653531155071 agent memory len 706 steps  74 reward 0 next state  7 agent position  (3, 5)
agent epsilon  0.6104653531155071 agent memory len 784 steps  74 reward 0 next state  3 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['D', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 592 steps  75 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.6104653531155071 agent memory len 707 steps  75 reward 10 next state  7 agent position  (4, 5)
agent epsilon  0.6104653531155071 agent memory len 785 steps  75 reward 0 next state  4 agent position  (3, 4)
max steps reached
total rewards -235
epsilon  0.5811803122766818
epsilon  0.5811803122766818
epsilon  0.5811803122766818
Episode number:  12
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 593 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 708 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5811803122766818 agent memory len 786 steps  1 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 594 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5811803122766818 agent memory len 709 steps  2 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.5811803122766818 agent memory len 787 steps  2 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 595 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 710 steps  3 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5811803122766818 agent memory len 788 steps  3 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 596 steps  4 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 711 steps  4 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5811803122766818 agent memory len 789 steps  4 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 597 steps  5 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 712 steps  5 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.5811803122766818 agent memory len 790 steps  5 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 598 steps  6 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 713 steps  6 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.5811803122766818 agent memory len 791 steps  6 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 599 steps  7 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 714 steps  7 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5811803122766818 agent memory len 792 steps  7 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  0.5811803122766818 agent memory len 600 steps  8 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 715 steps  8 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.5811803122766818 agent memory len 793 steps  8 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 601 steps  9 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 716 steps  9 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.5811803122766818 agent memory len 794 steps  9 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 602 steps  10 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5811803122766818 agent memory len 717 steps  10 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5811803122766818 agent memory len 795 steps  10 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 603 steps  11 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 718 steps  11 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.5811803122766818 agent memory len 796 steps  11 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 604 steps  12 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 719 steps  12 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.5811803122766818 agent memory len 797 steps  12 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 605 steps  13 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5811803122766818 agent memory len 720 steps  13 reward -1 next state  0 agent position  (1, 2)
agent epsilon  0.5811803122766818 agent memory len 798 steps  13 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.5811803122766818 agent memory len 606 steps  14 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.5811803122766818 agent memory len 721 steps  14 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.5811803122766818 agent memory len 799 steps  14 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 607 steps  15 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.5811803122766818 agent memory len 722 steps  15 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5811803122766818 agent memory len 800 steps  15 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5811803122766818 agent memory len 608 steps  16 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.5811803122766818 agent memory len 723 steps  16 reward -1 next state  1 agent position  (2, 2)
agent epsilon  0.5811803122766818 agent memory len 801 steps  16 reward -2 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 609 steps  17 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.5811803122766818 agent memory len 724 steps  17 reward -1 next state  1 agent position  (2, 1)
agent epsilon  0.5811803122766818 agent memory len 802 steps  17 reward -2 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.5811803122766818 agent memory len 610 steps  18 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.5811803122766818 agent memory len 725 steps  18 reward -1 next state  2 agent position  (3, 1)
agent epsilon  0.5811803122766818 agent memory len 803 steps  18 reward -1 next state  3 agent position  (8, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 611 steps  19 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.5811803122766818 agent memory len 726 steps  19 reward -1 next state  2 agent position  (3, 0)
agent epsilon  0.5811803122766818 agent memory len 804 steps  19 reward -1 next state  3 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 612 steps  20 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.5811803122766818 agent memory len 727 steps  20 reward -1 next state  2 agent position  (3, 0)
agent epsilon  0.5811803122766818 agent memory len 805 steps  20 reward -1 next state  3 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 613 steps  21 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.5811803122766818 agent memory len 728 steps  21 reward -1 next state  2 agent position  (4, 0)
agent epsilon  0.5811803122766818 agent memory len 806 steps  21 reward -1 next state  4 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 614 steps  22 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.5811803122766818 agent memory len 729 steps  22 reward -1 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 807 steps  22 reward -1 next state  5 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 615 steps  23 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.5811803122766818 agent memory len 730 steps  23 reward -2 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 808 steps  23 reward -2 next state  5 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['D', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 616 steps  24 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 731 steps  24 reward -2 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 809 steps  24 reward -1 next state  5 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 617 steps  25 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 732 steps  25 reward -2 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 810 steps  25 reward -1 next state  5 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 618 steps  26 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 733 steps  26 reward -2 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 811 steps  26 reward -1 next state  5 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  0.5811803122766818 agent memory len 619 steps  27 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.5811803122766818 agent memory len 734 steps  27 reward -1 next state  2 agent position  (4, 0)
agent epsilon  0.5811803122766818 agent memory len 812 steps  27 reward -2 next state  4 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 620 steps  28 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 735 steps  28 reward -1 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 813 steps  28 reward -1 next state  5 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 621 steps  29 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 736 steps  29 reward -2 next state  2 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 814 steps  29 reward -1 next state  5 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 622 steps  30 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.5811803122766818 agent memory len 737 steps  30 reward -1 next state  2 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 815 steps  30 reward -2 next state  6 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 623 steps  31 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 738 steps  31 reward -1 next state  2 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 816 steps  31 reward -1 next state  7 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.5811803122766818 agent memory len 624 steps  32 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5811803122766818 agent memory len 739 steps  32 reward -1 next state  3 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 817 steps  32 reward -1 next state  7 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 625 steps  33 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5811803122766818 agent memory len 740 steps  33 reward -2 next state  4 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 818 steps  33 reward -1 next state  7 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  0.5811803122766818 agent memory len 626 steps  34 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.5811803122766818 agent memory len 741 steps  34 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 819 steps  34 reward -1 next state  7 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 627 steps  35 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5811803122766818 agent memory len 742 steps  35 reward -1 next state  4 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 820 steps  35 reward -1 next state  7 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 628 steps  36 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5811803122766818 agent memory len 743 steps  36 reward -1 next state  5 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 821 steps  36 reward -1 next state  8 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 629 steps  37 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.5811803122766818 agent memory len 744 steps  37 reward -1 next state  6 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 822 steps  37 reward -1 next state  8 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 630 steps  38 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.5811803122766818 agent memory len 745 steps  38 reward -1 next state  6 agent position  (8, 1)
agent epsilon  0.5811803122766818 agent memory len 823 steps  38 reward -1 next state  8 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 631 steps  39 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.5811803122766818 agent memory len 746 steps  39 reward -1 next state  7 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 824 steps  39 reward -2 next state  8 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 632 steps  40 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.5811803122766818 agent memory len 747 steps  40 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 825 steps  40 reward -1 next state  8 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 633 steps  41 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.5811803122766818 agent memory len 748 steps  41 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 826 steps  41 reward -1 next state  8 agent position  (9, 4)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.5811803122766818 agent memory len 634 steps  42 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 749 steps  42 reward -2 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 827 steps  42 reward -1 next state  8 agent position  (8, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.5811803122766818 agent memory len 635 steps  43 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 750 steps  43 reward -1 next state  9 agent position  (8, 1)
agent epsilon  0.5811803122766818 agent memory len 828 steps  43 reward -1 next state  8 agent position  (8, 5)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 636 steps  44 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 751 steps  44 reward -1 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 829 steps  44 reward -1 next state  8 agent position  (8, 6)
 is_terminal [False, False, False]
actions ['R', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 637 steps  45 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 752 steps  45 reward -2 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 830 steps  45 reward -1 next state  8 agent position  (8, 5)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 638 steps  46 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 753 steps  46 reward -2 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 831 steps  46 reward -1 next state  8 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['R', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 639 steps  47 reward -2 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 754 steps  47 reward -2 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 832 steps  47 reward -1 next state  8 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.5811803122766818 agent memory len 640 steps  48 reward -1 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 755 steps  48 reward -2 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 833 steps  48 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 641 steps  49 reward -1 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 756 steps  49 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.5811803122766818 agent memory len 834 steps  49 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'U']
agent epsilon  0.5811803122766818 agent memory len 642 steps  50 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 757 steps  50 reward -1 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 835 steps  50 reward 0 next state  8 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'R']
agent epsilon  0.5811803122766818 agent memory len 643 steps  51 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 758 steps  51 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 836 steps  51 reward 0 next state  7 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 644 steps  52 reward -1 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 759 steps  52 reward -2 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 837 steps  52 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 645 steps  53 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 760 steps  53 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 838 steps  53 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.5811803122766818 agent memory len 646 steps  54 reward -1 next state  9 agent position  (9, 9)
agent epsilon  0.5811803122766818 agent memory len 761 steps  54 reward -1 next state  9 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 839 steps  54 reward -1 next state  6 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 647 steps  55 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 762 steps  55 reward -2 next state  9 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 840 steps  55 reward -1 next state  6 agent position  (6, 7)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5811803122766818 agent memory len 648 steps  56 reward -2 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 763 steps  56 reward -1 next state  9 agent position  (6, 1)
agent epsilon  0.5811803122766818 agent memory len 841 steps  56 reward -1 next state  6 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 649 steps  57 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.5811803122766818 agent memory len 764 steps  57 reward -1 next state  8 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 842 steps  57 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5811803122766818 agent memory len 650 steps  58 reward -1 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 765 steps  58 reward -1 next state  9 agent position  (7, 2)
agent epsilon  0.5811803122766818 agent memory len 843 steps  58 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 651 steps  59 reward -2 next state  8 agent position  (8, 9)
agent epsilon  0.5811803122766818 agent memory len 766 steps  59 reward -1 next state  9 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 844 steps  59 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 652 steps  60 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 767 steps  60 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 845 steps  60 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.5811803122766818 agent memory len 653 steps  61 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 768 steps  61 reward -1 next state  9 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 846 steps  61 reward -1 next state  6 agent position  (7, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.5811803122766818 agent memory len 654 steps  62 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 769 steps  62 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 847 steps  62 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 655 steps  63 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 770 steps  63 reward -2 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 848 steps  63 reward 0 next state  7 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 656 steps  64 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 771 steps  64 reward -1 next state  9 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 849 steps  64 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 657 steps  65 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 772 steps  65 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 850 steps  65 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5811803122766818 agent memory len 658 steps  66 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 773 steps  66 reward -1 next state  9 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 851 steps  66 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, False]
actions ['R', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 659 steps  67 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 774 steps  67 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 852 steps  67 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.5811803122766818 agent memory len 660 steps  68 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.5811803122766818 agent memory len 775 steps  68 reward -2 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 853 steps  68 reward 0 next state  7 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 661 steps  69 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.5811803122766818 agent memory len 776 steps  69 reward -2 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 854 steps  69 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 662 steps  70 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.5811803122766818 agent memory len 777 steps  70 reward -1 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 855 steps  70 reward -1 next state  7 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 663 steps  71 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.5811803122766818 agent memory len 778 steps  71 reward -2 next state  9 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 856 steps  71 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.5811803122766818 agent memory len 664 steps  72 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.5811803122766818 agent memory len 779 steps  72 reward -1 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 857 steps  72 reward 0 next state  8 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 665 steps  73 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.5811803122766818 agent memory len 780 steps  73 reward -1 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 858 steps  73 reward 0 next state  8 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 666 steps  74 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.5811803122766818 agent memory len 781 steps  74 reward -1 next state  9 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 859 steps  74 reward 0 next state  8 agent position  (6, 5)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.5811803122766818 agent memory len 667 steps  75 reward -2 next state  4 agent position  (4, 9)
agent epsilon  0.5811803122766818 agent memory len 782 steps  75 reward -1 next state  9 agent position  (8, 1)
agent epsilon  0.5811803122766818 agent memory len 860 steps  75 reward -1 next state  8 agent position  (6, 6)
max steps reached
total rewards -269
epsilon  0.5533235197330861
epsilon  0.5533235197330861
epsilon  0.5533235197330861
Episode number:  13
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'R', 'U']
agent epsilon  0.5533235197330861 agent memory len 668 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5533235197330861 agent memory len 783 steps  1 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.5533235197330861 agent memory len 861 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 669 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 784 steps  2 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5533235197330861 agent memory len 862 steps  2 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 670 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 785 steps  3 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5533235197330861 agent memory len 863 steps  3 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 671 steps  4 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 786 steps  4 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 864 steps  4 reward -1 next state  0 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 672 steps  5 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 787 steps  5 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.5533235197330861 agent memory len 865 steps  5 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
actions ['S', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 673 steps  6 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 788 steps  6 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 866 steps  6 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.5533235197330861 agent memory len 674 steps  7 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 789 steps  7 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5533235197330861 agent memory len 867 steps  7 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 675 steps  8 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 790 steps  8 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 868 steps  8 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 676 steps  9 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 791 steps  9 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 869 steps  9 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 677 steps  10 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 792 steps  10 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 870 steps  10 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.5533235197330861 agent memory len 678 steps  11 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 793 steps  11 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.5533235197330861 agent memory len 871 steps  11 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  0.5533235197330861 agent memory len 679 steps  12 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 794 steps  12 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 872 steps  12 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 680 steps  13 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 795 steps  13 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 873 steps  13 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 681 steps  14 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 796 steps  14 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 874 steps  14 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 682 steps  15 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 797 steps  15 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 875 steps  15 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 683 steps  16 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 798 steps  16 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.5533235197330861 agent memory len 876 steps  16 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.5533235197330861 agent memory len 684 steps  17 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.5533235197330861 agent memory len 799 steps  17 reward -1 next state  3 agent position  (0, 7)
agent epsilon  0.5533235197330861 agent memory len 877 steps  17 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.5533235197330861 agent memory len 685 steps  18 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.5533235197330861 agent memory len 800 steps  18 reward -1 next state  3 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 878 steps  18 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 686 steps  19 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.5533235197330861 agent memory len 801 steps  19 reward -1 next state  3 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 879 steps  19 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 687 steps  20 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5533235197330861 agent memory len 802 steps  20 reward -1 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 880 steps  20 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5533235197330861 agent memory len 688 steps  21 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 803 steps  21 reward -1 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 881 steps  21 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  0.5533235197330861 agent memory len 689 steps  22 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 804 steps  22 reward -2 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 882 steps  22 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.5533235197330861 agent memory len 690 steps  23 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 805 steps  23 reward -2 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 883 steps  23 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 691 steps  24 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 806 steps  24 reward -1 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 884 steps  24 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 692 steps  25 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 807 steps  25 reward -1 next state  4 agent position  (0, 8)
agent epsilon  0.5533235197330861 agent memory len 885 steps  25 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.5533235197330861 agent memory len 693 steps  26 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 808 steps  26 reward -1 next state  4 agent position  (0, 7)
agent epsilon  0.5533235197330861 agent memory len 886 steps  26 reward -2 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 694 steps  27 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 809 steps  27 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.5533235197330861 agent memory len 887 steps  27 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 695 steps  28 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5533235197330861 agent memory len 810 steps  28 reward -1 next state  4 agent position  (0, 6)
agent epsilon  0.5533235197330861 agent memory len 888 steps  28 reward -1 next state  0 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'U']
agent epsilon  0.5533235197330861 agent memory len 696 steps  29 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.5533235197330861 agent memory len 811 steps  29 reward -1 next state  4 agent position  (1, 6)
agent epsilon  0.5533235197330861 agent memory len 889 steps  29 reward -1 next state  1 agent position  (7, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.5533235197330861 agent memory len 697 steps  30 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 812 steps  30 reward -1 next state  4 agent position  (2, 6)
agent epsilon  0.5533235197330861 agent memory len 890 steps  30 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'L', 'L']
agent epsilon  0.5533235197330861 agent memory len 698 steps  31 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 813 steps  31 reward -1 next state  4 agent position  (2, 5)
agent epsilon  0.5533235197330861 agent memory len 891 steps  31 reward -1 next state  2 agent position  (7, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 814 steps  32 reward -1 next state  4 agent position  (2, 5)
agent epsilon  0.5533235197330861 agent memory len 892 steps  32 reward -1 next state  2 agent position  (7, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 815 steps  33 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.5533235197330861 agent memory len 893 steps  33 reward -1 next state  2 agent position  (6, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 698 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 816 steps  34 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 894 steps  34 reward -2 next state  3 agent position  (6, 0)
 is_terminal [True, False, False]
random action 1
actions ['S', 'U', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 817 steps  35 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.5533235197330861 agent memory len 895 steps  35 reward -1 next state  2 agent position  (5, 0)
 is_terminal [True, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 818 steps  36 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 896 steps  36 reward -1 next state  3 agent position  (4, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 819 steps  37 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 897 steps  37 reward -1 next state  3 agent position  (3, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 820 steps  38 reward -1 next state  4 agent position  (3, 3)
agent epsilon  0.5533235197330861 agent memory len 898 steps  38 reward -1 next state  3 agent position  (4, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 821 steps  39 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 899 steps  39 reward -1 next state  3 agent position  (4, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 822 steps  40 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 900 steps  40 reward -1 next state  3 agent position  (5, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 823 steps  41 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 901 steps  41 reward -1 next state  3 agent position  (6, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 824 steps  42 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 902 steps  42 reward -1 next state  3 agent position  (7, 0)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 825 steps  43 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 903 steps  43 reward -1 next state  3 agent position  (6, 0)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 826 steps  44 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 904 steps  44 reward -1 next state  3 agent position  (6, 0)
 is_terminal [True, False, False]
random action 1
actions ['S', 'U', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 827 steps  45 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.5533235197330861 agent memory len 905 steps  45 reward -1 next state  2 agent position  (5, 0)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 828 steps  46 reward -1 next state  4 agent position  (2, 3)
agent epsilon  0.5533235197330861 agent memory len 906 steps  46 reward -1 next state  2 agent position  (5, 1)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 829 steps  47 reward -1 next state  4 agent position  (2, 3)
agent epsilon  0.5533235197330861 agent memory len 907 steps  47 reward -1 next state  2 agent position  (6, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 830 steps  48 reward -1 next state  4 agent position  (3, 3)
agent epsilon  0.5533235197330861 agent memory len 908 steps  48 reward -1 next state  3 agent position  (6, 1)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 831 steps  49 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.5533235197330861 agent memory len 909 steps  49 reward -1 next state  3 agent position  (5, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 832 steps  50 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.5533235197330861 agent memory len 910 steps  50 reward -1 next state  3 agent position  (5, 1)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 833 steps  51 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.5533235197330861 agent memory len 911 steps  51 reward -1 next state  3 agent position  (6, 1)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 834 steps  52 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.5533235197330861 agent memory len 912 steps  52 reward -1 next state  3 agent position  (6, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 835 steps  53 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.5533235197330861 agent memory len 913 steps  53 reward -1 next state  3 agent position  (7, 2)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 836 steps  54 reward 0 next state  4 agent position  (4, 6)
agent epsilon  0.5533235197330861 agent memory len 914 steps  54 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, False, False]
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  55 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 915 steps  55 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 916 steps  56 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 917 steps  57 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5533235197330861 agent memory len 698 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 918 steps  58 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 919 steps  59 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 920 steps  60 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 921 steps  61 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 922 steps  62 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 923 steps  63 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 924 steps  64 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 698 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 925 steps  65 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 926 steps  66 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 927 steps  67 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 928 steps  68 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 929 steps  69 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 930 steps  70 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 931 steps  71 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 698 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 932 steps  72 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 933 steps  73 reward -1 next state  4 agent position  (3, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 698 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 934 steps  74 reward -1 next state  4 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 698 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.5533235197330861 agent memory len 837 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5533235197330861 agent memory len 935 steps  75 reward -1 next state  4 agent position  (2, 8)
max steps reached
total rewards -126
epsilon  0.5268253189934059
epsilon  0.5268253189934059
epsilon  0.5268253189934059
Episode number:  14
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.5268253189934059 agent memory len 699 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5268253189934059 agent memory len 838 steps  1 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 936 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 700 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.5268253189934059 agent memory len 839 steps  2 reward -1 next state  1 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 937 steps  2 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.5268253189934059 agent memory len 701 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5268253189934059 agent memory len 840 steps  3 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 938 steps  3 reward -1 next state  0 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'U']
agent epsilon  0.5268253189934059 agent memory len 702 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5268253189934059 agent memory len 841 steps  4 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 939 steps  4 reward -1 next state  0 agent position  (6, 0)
 is_terminal [False, False, False]
actions ['R', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 703 steps  5 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.5268253189934059 agent memory len 842 steps  5 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 940 steps  5 reward -1 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 704 steps  6 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5268253189934059 agent memory len 843 steps  6 reward -1 next state  2 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 941 steps  6 reward -1 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 705 steps  7 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.5268253189934059 agent memory len 844 steps  7 reward -1 next state  3 agent position  (2, 9)
agent epsilon  0.5268253189934059 agent memory len 942 steps  7 reward -1 next state  2 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.5268253189934059 agent memory len 706 steps  8 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.5268253189934059 agent memory len 845 steps  8 reward -1 next state  3 agent position  (2, 8)
agent epsilon  0.5268253189934059 agent memory len 943 steps  8 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.5268253189934059 agent memory len 707 steps  9 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5268253189934059 agent memory len 846 steps  9 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.5268253189934059 agent memory len 944 steps  9 reward -1 next state  2 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'R', 'U']
agent epsilon  0.5268253189934059 agent memory len 708 steps  10 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.5268253189934059 agent memory len 847 steps  10 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.5268253189934059 agent memory len 945 steps  10 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5268253189934059 agent memory len 709 steps  11 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.5268253189934059 agent memory len 848 steps  11 reward -1 next state  5 agent position  (2, 7)
agent epsilon  0.5268253189934059 agent memory len 946 steps  11 reward -1 next state  2 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 710 steps  12 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.5268253189934059 agent memory len 849 steps  12 reward -1 next state  5 agent position  (3, 7)
agent epsilon  0.5268253189934059 agent memory len 947 steps  12 reward -1 next state  3 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 711 steps  13 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5268253189934059 agent memory len 850 steps  13 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.5268253189934059 agent memory len 948 steps  13 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 712 steps  14 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.5268253189934059 agent memory len 851 steps  14 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.5268253189934059 agent memory len 949 steps  14 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.5268253189934059 agent memory len 713 steps  15 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5268253189934059 agent memory len 852 steps  15 reward 0 next state  4 agent position  (4, 6)
agent epsilon  0.5268253189934059 agent memory len 950 steps  15 reward -1 next state  4 agent position  (1, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.5268253189934059 agent memory len 714 steps  16 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.5268253189934059 agent memory len 853 steps  16 reward 0 next state  5 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 951 steps  16 reward -1 next state  5 agent position  (1, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.5268253189934059 agent memory len 715 steps  17 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.5268253189934059 agent memory len 854 steps  17 reward 0 next state  6 agent position  (4, 6)
agent epsilon  0.5268253189934059 agent memory len 952 steps  17 reward -1 next state  4 agent position  (0, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 716 steps  18 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5268253189934059 agent memory len 855 steps  18 reward 0 next state  7 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 953 steps  18 reward -1 next state  5 agent position  (1, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.5268253189934059 agent memory len 717 steps  19 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 856 steps  19 reward 0 next state  8 agent position  (5, 5)
agent epsilon  0.5268253189934059 agent memory len 954 steps  19 reward -1 next state  5 agent position  (1, 4)
 is_terminal [False, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'U', 'L']
agent epsilon  0.5268253189934059 agent memory len 718 steps  20 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  20 reward 10 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 955 steps  20 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 719 steps  21 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  21 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 956 steps  21 reward -1 next state  4 agent position  (1, 4)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 720 steps  22 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  22 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 957 steps  22 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 721 steps  23 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  23 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 958 steps  23 reward -1 next state  4 agent position  (1, 4)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 722 steps  24 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  24 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 959 steps  24 reward -1 next state  4 agent position  (2, 4)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 723 steps  25 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  25 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 960 steps  25 reward -1 next state  4 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 724 steps  26 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  26 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 961 steps  26 reward -1 next state  4 agent position  (2, 4)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 725 steps  27 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  27 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 962 steps  27 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 726 steps  28 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  28 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 963 steps  28 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 727 steps  29 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  29 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 964 steps  29 reward -1 next state  4 agent position  (2, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 728 steps  30 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  30 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 965 steps  30 reward -1 next state  4 agent position  (3, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 729 steps  31 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  31 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 966 steps  31 reward 0 next state  4 agent position  (3, 5)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 730 steps  32 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  32 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 967 steps  32 reward -1 next state  4 agent position  (3, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 731 steps  33 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  33 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 968 steps  33 reward -1 next state  4 agent position  (3, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 732 steps  34 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  34 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 969 steps  34 reward 0 next state  4 agent position  (3, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 733 steps  35 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  35 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 970 steps  35 reward 0 next state  4 agent position  (3, 4)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 734 steps  36 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  36 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 971 steps  36 reward 0 next state  4 agent position  (3, 3)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 735 steps  37 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  37 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 972 steps  37 reward 0 next state  4 agent position  (3, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 736 steps  38 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  38 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 973 steps  38 reward 0 next state  4 agent position  (3, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 737 steps  39 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  39 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 974 steps  39 reward -1 next state  4 agent position  (2, 3)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 738 steps  40 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  40 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 975 steps  40 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 739 steps  41 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  41 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 976 steps  41 reward -1 next state  4 agent position  (1, 4)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 740 steps  42 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  42 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 977 steps  42 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 741 steps  43 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  43 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 978 steps  43 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 742 steps  44 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  44 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 979 steps  44 reward -1 next state  4 agent position  (0, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 743 steps  45 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  45 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 980 steps  45 reward -1 next state  4 agent position  (0, 4)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 744 steps  46 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  46 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 981 steps  46 reward -1 next state  4 agent position  (0, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 745 steps  47 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  47 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 982 steps  47 reward -2 next state  4 agent position  (0, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 746 steps  48 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  48 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 983 steps  48 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 747 steps  49 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  49 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 984 steps  49 reward -1 next state  4 agent position  (1, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 748 steps  50 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  50 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 985 steps  50 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 749 steps  51 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  51 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 986 steps  51 reward -1 next state  4 agent position  (1, 2)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 750 steps  52 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  52 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 987 steps  52 reward -1 next state  4 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 751 steps  53 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  53 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 988 steps  53 reward -1 next state  4 agent position  (1, 4)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 752 steps  54 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  54 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 989 steps  54 reward -1 next state  4 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 753 steps  55 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  55 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 990 steps  55 reward -1 next state  4 agent position  (2, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 754 steps  56 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  56 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 991 steps  56 reward -1 next state  4 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 755 steps  57 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  57 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 992 steps  57 reward -1 next state  4 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 756 steps  58 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  58 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 993 steps  58 reward 0 next state  4 agent position  (3, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 757 steps  59 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  59 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 994 steps  59 reward 0 next state  4 agent position  (3, 4)
 is_terminal [False, True, False]
random action 0
random action 2
landmark captured 0
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 758 steps  60 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  60 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  60 reward 10 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 759 steps  61 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  61 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  61 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 760 steps  62 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  62 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  62 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 761 steps  63 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.5268253189934059 agent memory len 857 steps  63 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  63 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 762 steps  64 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  64 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  64 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 763 steps  65 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  65 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  65 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 764 steps  66 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  66 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  66 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 765 steps  67 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  67 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  67 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 766 steps  68 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  68 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  68 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 767 steps  69 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  69 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  69 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 768 steps  70 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  70 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  70 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 769 steps  71 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  71 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  71 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 770 steps  72 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  72 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  72 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 771 steps  73 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  73 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  73 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 772 steps  74 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5268253189934059 agent memory len 857 steps  74 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  74 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 773 steps  75 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5268253189934059 agent memory len 857 steps  75 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 995 steps  75 reward 0 next state  4 agent position  (4, 4)
max steps reached
total rewards -130
epsilon  0.5016194507534953
epsilon  0.5016194507534953
epsilon  0.5016194507534953
Episode number:  15
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 774 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 858 steps  1 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.5016194507534953 agent memory len 996 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.5016194507534953 agent memory len 775 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 859 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.5016194507534953 agent memory len 997 steps  2 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 776 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5016194507534953 agent memory len 860 steps  3 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.5016194507534953 agent memory len 998 steps  3 reward -1 next state  1 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 777 steps  4 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5016194507534953 agent memory len 861 steps  4 reward -1 next state  2 agent position  (0, 7)
agent epsilon  0.5016194507534953 agent memory len 999 steps  4 reward -1 next state  0 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 778 steps  5 reward -2 next state  0 agent position  (0, 2)
agent epsilon  0.5016194507534953 agent memory len 862 steps  5 reward -1 next state  2 agent position  (0, 6)
agent epsilon  0.5016194507534953 agent memory len 1000 steps  5 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  0.5016194507534953 agent memory len 779 steps  6 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5016194507534953 agent memory len 863 steps  6 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.5016194507534953 agent memory len 1001 steps  6 reward -1 next state  0 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 780 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 864 steps  7 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.5016194507534953 agent memory len 1002 steps  7 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 781 steps  8 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5016194507534953 agent memory len 865 steps  8 reward -1 next state  2 agent position  (1, 6)
agent epsilon  0.5016194507534953 agent memory len 1003 steps  8 reward -1 next state  1 agent position  (8, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.5016194507534953 agent memory len 782 steps  9 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5016194507534953 agent memory len 866 steps  9 reward -1 next state  2 agent position  (1, 6)
agent epsilon  0.5016194507534953 agent memory len 1004 steps  9 reward -1 next state  1 agent position  (7, 5)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 783 steps  10 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5016194507534953 agent memory len 867 steps  10 reward -1 next state  2 agent position  (1, 5)
agent epsilon  0.5016194507534953 agent memory len 1005 steps  10 reward -1 next state  1 agent position  (7, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.5016194507534953 agent memory len 784 steps  11 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.5016194507534953 agent memory len 868 steps  11 reward -1 next state  2 agent position  (1, 6)
agent epsilon  0.5016194507534953 agent memory len 1006 steps  11 reward -1 next state  1 agent position  (6, 6)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'L']
agent epsilon  0.5016194507534953 agent memory len 785 steps  12 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.5016194507534953 agent memory len 869 steps  12 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.5016194507534953 agent memory len 1007 steps  12 reward 0 next state  2 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 786 steps  13 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.5016194507534953 agent memory len 870 steps  13 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.5016194507534953 agent memory len 1008 steps  13 reward -1 next state  3 agent position  (6, 6)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'L']
agent epsilon  0.5016194507534953 agent memory len 787 steps  14 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.5016194507534953 agent memory len 871 steps  14 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.5016194507534953 agent memory len 1009 steps  14 reward 0 next state  4 agent position  (6, 5)
 is_terminal [False, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['D', 'L', 'L']
agent epsilon  0.5016194507534953 agent memory len 788 steps  15 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  15 reward 10 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1010 steps  15 reward 0 next state  4 agent position  (6, 4)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 789 steps  16 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1011 steps  16 reward 0 next state  4 agent position  (6, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.5016194507534953 agent memory len 790 steps  17 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1012 steps  17 reward -1 next state  4 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 791 steps  18 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1013 steps  18 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 792 steps  19 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1014 steps  19 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 793 steps  20 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1015 steps  20 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 794 steps  21 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  21 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1016 steps  21 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 795 steps  22 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  22 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1017 steps  22 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.5016194507534953 agent memory len 796 steps  23 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  23 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1018 steps  23 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 797 steps  24 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1019 steps  24 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 798 steps  25 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  25 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1020 steps  25 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 799 steps  26 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  26 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1021 steps  26 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.5016194507534953 agent memory len 800 steps  27 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  27 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1022 steps  27 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 801 steps  28 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  28 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1023 steps  28 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 802 steps  29 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  29 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1024 steps  29 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.5016194507534953 agent memory len 803 steps  30 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  30 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1025 steps  30 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 804 steps  31 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  31 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1026 steps  31 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 805 steps  32 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  32 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1027 steps  32 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 806 steps  33 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  33 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1028 steps  33 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 807 steps  34 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  34 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1029 steps  34 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 808 steps  35 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  35 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1030 steps  35 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 809 steps  36 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  36 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1031 steps  36 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 810 steps  37 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1032 steps  37 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 811 steps  38 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1033 steps  38 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 812 steps  39 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1034 steps  39 reward -1 next state  4 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 813 steps  40 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  40 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1035 steps  40 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 814 steps  41 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  41 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1036 steps  41 reward -1 next state  4 agent position  (7, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 815 steps  42 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  42 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1037 steps  42 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 816 steps  43 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1038 steps  43 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5016194507534953 agent memory len 817 steps  44 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1039 steps  44 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 818 steps  45 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1040 steps  45 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.5016194507534953 agent memory len 819 steps  46 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1041 steps  46 reward -1 next state  4 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 820 steps  47 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1042 steps  47 reward -1 next state  4 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.5016194507534953 agent memory len 821 steps  48 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  48 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1043 steps  48 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 822 steps  49 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  49 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1044 steps  49 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 823 steps  50 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  50 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1045 steps  50 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.5016194507534953 agent memory len 824 steps  51 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  51 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1046 steps  51 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 825 steps  52 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1047 steps  52 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 826 steps  53 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1048 steps  53 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 827 steps  54 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1049 steps  54 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 828 steps  55 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1050 steps  55 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 829 steps  56 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1051 steps  56 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5016194507534953 agent memory len 830 steps  57 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1052 steps  57 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5016194507534953 agent memory len 831 steps  58 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1053 steps  58 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 832 steps  59 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  59 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1054 steps  59 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 833 steps  60 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  60 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1055 steps  60 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 834 steps  61 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5016194507534953 agent memory len 872 steps  61 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1056 steps  61 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 835 steps  62 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1057 steps  62 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 836 steps  63 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1058 steps  63 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 837 steps  64 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  64 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1059 steps  64 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 838 steps  65 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  65 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1060 steps  65 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 839 steps  66 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  66 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1061 steps  66 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 840 steps  67 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  67 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1062 steps  67 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 841 steps  68 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  68 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1063 steps  68 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.5016194507534953 agent memory len 842 steps  69 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  69 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1064 steps  69 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 843 steps  70 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5016194507534953 agent memory len 872 steps  70 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1065 steps  70 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 844 steps  71 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1066 steps  71 reward -1 next state  4 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 845 steps  72 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1067 steps  72 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 846 steps  73 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1068 steps  73 reward -1 next state  4 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 847 steps  74 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1069 steps  74 reward -1 next state  4 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 848 steps  75 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5016194507534953 agent memory len 872 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.5016194507534953 agent memory len 1070 steps  75 reward -1 next state  4 agent position  (8, 7)
max steps reached
total rewards -171
epsilon  0.47764288721360454
epsilon  0.47764288721360454
epsilon  0.47764288721360454
Episode number:  16
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.47764288721360454 agent memory len 849 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 873 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.47764288721360454 agent memory len 1071 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.47764288721360454 agent memory len 850 steps  2 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 874 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.47764288721360454 agent memory len 1072 steps  2 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'R']
agent epsilon  0.47764288721360454 agent memory len 851 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.47764288721360454 agent memory len 875 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.47764288721360454 agent memory len 1073 steps  3 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 852 steps  4 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.47764288721360454 agent memory len 876 steps  4 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.47764288721360454 agent memory len 1074 steps  4 reward -1 next state  2 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.47764288721360454 agent memory len 853 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 877 steps  5 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.47764288721360454 agent memory len 1075 steps  5 reward -1 next state  1 agent position  (8, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.47764288721360454 agent memory len 854 steps  6 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.47764288721360454 agent memory len 878 steps  6 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.47764288721360454 agent memory len 1076 steps  6 reward -1 next state  1 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 855 steps  7 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 879 steps  7 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.47764288721360454 agent memory len 1077 steps  7 reward -1 next state  1 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 856 steps  8 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.47764288721360454 agent memory len 880 steps  8 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1078 steps  8 reward -1 next state  1 agent position  (7, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.47764288721360454 agent memory len 857 steps  9 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 881 steps  9 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1079 steps  9 reward -1 next state  0 agent position  (7, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 858 steps  10 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.47764288721360454 agent memory len 882 steps  10 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1080 steps  10 reward -1 next state  1 agent position  (8, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.47764288721360454 agent memory len 859 steps  11 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.47764288721360454 agent memory len 883 steps  11 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1081 steps  11 reward -1 next state  1 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'L']
agent epsilon  0.47764288721360454 agent memory len 860 steps  12 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.47764288721360454 agent memory len 884 steps  12 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.47764288721360454 agent memory len 1082 steps  12 reward -1 next state  1 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.47764288721360454 agent memory len 861 steps  13 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.47764288721360454 agent memory len 885 steps  13 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.47764288721360454 agent memory len 1083 steps  13 reward -1 next state  0 agent position  (8, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.47764288721360454 agent memory len 862 steps  14 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.47764288721360454 agent memory len 886 steps  14 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1084 steps  14 reward -1 next state  0 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
actions ['L', 'R', 'L']
agent epsilon  0.47764288721360454 agent memory len 863 steps  15 reward -2 next state  6 agent position  (6, 0)
agent epsilon  0.47764288721360454 agent memory len 887 steps  15 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1085 steps  15 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.47764288721360454 agent memory len 864 steps  16 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.47764288721360454 agent memory len 888 steps  16 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1086 steps  16 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'R', 'R']
agent epsilon  0.47764288721360454 agent memory len 865 steps  17 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.47764288721360454 agent memory len 889 steps  17 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1087 steps  17 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'L']
agent epsilon  0.47764288721360454 agent memory len 866 steps  18 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.47764288721360454 agent memory len 890 steps  18 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1088 steps  18 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'R']
agent epsilon  0.47764288721360454 agent memory len 867 steps  19 reward -1 next state  8 agent position  (8, 1)
agent epsilon  0.47764288721360454 agent memory len 891 steps  19 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1089 steps  19 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['U', 'R', 'L']
agent epsilon  0.47764288721360454 agent memory len 868 steps  20 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.47764288721360454 agent memory len 892 steps  20 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1090 steps  20 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 869 steps  21 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.47764288721360454 agent memory len 893 steps  21 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1091 steps  21 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 870 steps  22 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.47764288721360454 agent memory len 894 steps  22 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1092 steps  22 reward 0 next state  0 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
actions ['S', 'R', 'L']
agent epsilon  0.47764288721360454 agent memory len 871 steps  23 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.47764288721360454 agent memory len 895 steps  23 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1093 steps  23 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['D', 'R', 'R']
agent epsilon  0.47764288721360454 agent memory len 872 steps  24 reward -1 next state  8 agent position  (8, 1)
agent epsilon  0.47764288721360454 agent memory len 896 steps  24 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1094 steps  24 reward 0 next state  0 agent position  (6, 5)
 is_terminal [False, False, False]
actions ['D', 'L', 'L']
agent epsilon  0.47764288721360454 agent memory len 873 steps  25 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 897 steps  25 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1095 steps  25 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 874 steps  26 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 898 steps  26 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1096 steps  26 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 875 steps  27 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 899 steps  27 reward -1 next state  2 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1097 steps  27 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.47764288721360454 agent memory len 876 steps  28 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.47764288721360454 agent memory len 900 steps  28 reward -1 next state  2 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1098 steps  28 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 877 steps  29 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.47764288721360454 agent memory len 901 steps  29 reward -1 next state  2 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1099 steps  29 reward 0 next state  0 agent position  (6, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  0.47764288721360454 agent memory len 878 steps  30 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 902 steps  30 reward -1 next state  2 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1100 steps  30 reward -1 next state  0 agent position  (7, 5)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'L']
agent epsilon  0.47764288721360454 agent memory len 879 steps  31 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 903 steps  31 reward -2 next state  2 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1101 steps  31 reward -1 next state  0 agent position  (7, 4)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'L']
agent epsilon  0.47764288721360454 agent memory len 880 steps  32 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 904 steps  32 reward -1 next state  2 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1102 steps  32 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 881 steps  33 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 905 steps  33 reward -1 next state  2 agent position  (0, 7)
agent epsilon  0.47764288721360454 agent memory len 1103 steps  33 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 882 steps  34 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 906 steps  34 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.47764288721360454 agent memory len 1104 steps  34 reward -1 next state  0 agent position  (7, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'L']
agent epsilon  0.47764288721360454 agent memory len 883 steps  35 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 907 steps  35 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1105 steps  35 reward -1 next state  0 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.47764288721360454 agent memory len 884 steps  36 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 908 steps  36 reward -2 next state  1 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1106 steps  36 reward -1 next state  0 agent position  (7, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 885 steps  37 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 909 steps  37 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1107 steps  37 reward -1 next state  0 agent position  (7, 5)
 is_terminal [False, False, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 886 steps  38 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 910 steps  38 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1108 steps  38 reward 0 next state  0 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 887 steps  39 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 911 steps  39 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1109 steps  39 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'L']
agent epsilon  0.47764288721360454 agent memory len 888 steps  40 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 912 steps  40 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1110 steps  40 reward 0 next state  0 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.47764288721360454 agent memory len 889 steps  41 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 913 steps  41 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1111 steps  41 reward -1 next state  0 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 890 steps  42 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 914 steps  42 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1112 steps  42 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.47764288721360454 agent memory len 891 steps  43 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 915 steps  43 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1113 steps  43 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.47764288721360454 agent memory len 892 steps  44 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 916 steps  44 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.47764288721360454 agent memory len 1114 steps  44 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 893 steps  45 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 917 steps  45 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1115 steps  45 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 894 steps  46 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 918 steps  46 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1116 steps  46 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.47764288721360454 agent memory len 895 steps  47 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 919 steps  47 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1117 steps  47 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 896 steps  48 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 920 steps  48 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1118 steps  48 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 897 steps  49 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 921 steps  49 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1119 steps  49 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.47764288721360454 agent memory len 898 steps  50 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 922 steps  50 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1120 steps  50 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
actions ['L', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 899 steps  51 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 923 steps  51 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 1121 steps  51 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.47764288721360454 agent memory len 900 steps  52 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 924 steps  52 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1122 steps  52 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 901 steps  53 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 925 steps  53 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.47764288721360454 agent memory len 1123 steps  53 reward -2 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.47764288721360454 agent memory len 902 steps  54 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 926 steps  54 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1124 steps  54 reward -2 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 903 steps  55 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.47764288721360454 agent memory len 927 steps  55 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1125 steps  55 reward -1 next state  1 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 904 steps  56 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 928 steps  56 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1126 steps  56 reward -1 next state  1 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.47764288721360454 agent memory len 905 steps  57 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 929 steps  57 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1127 steps  57 reward -1 next state  1 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 906 steps  58 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 930 steps  58 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1128 steps  58 reward -1 next state  1 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 907 steps  59 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 931 steps  59 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1129 steps  59 reward 0 next state  1 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 908 steps  60 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 932 steps  60 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  60 reward 10 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 909 steps  61 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 933 steps  61 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  61 reward 0 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.47764288721360454 agent memory len 910 steps  62 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 934 steps  62 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  62 reward 0 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 911 steps  63 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.47764288721360454 agent memory len 935 steps  63 reward -1 next state  2 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  63 reward 0 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 912 steps  64 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 936 steps  64 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  64 reward 0 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 913 steps  65 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 937 steps  65 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  65 reward 0 next state  1 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.47764288721360454 agent memory len 914 steps  66 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 938 steps  66 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  66 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 915 steps  67 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.47764288721360454 agent memory len 939 steps  67 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  67 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 916 steps  68 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 940 steps  68 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  68 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 917 steps  69 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 941 steps  69 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  69 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.47764288721360454 agent memory len 918 steps  70 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.47764288721360454 agent memory len 942 steps  70 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  70 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 919 steps  71 reward -1 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 943 steps  71 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  71 reward 0 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.47764288721360454 agent memory len 920 steps  72 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 944 steps  72 reward 0 next state  0 agent position  (3, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  72 reward 0 next state  3 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['D', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 921 steps  73 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 945 steps  73 reward 0 next state  0 agent position  (3, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  73 reward 0 next state  3 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 922 steps  74 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 946 steps  74 reward 0 next state  0 agent position  (3, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  74 reward 0 next state  3 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.47764288721360454 agent memory len 923 steps  75 reward -2 next state  9 agent position  (9, 0)
agent epsilon  0.47764288721360454 agent memory len 947 steps  75 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.47764288721360454 agent memory len 1130 steps  75 reward 0 next state  2 agent position  (5, 4)
max steps reached
total rewards -212
epsilon  0.45483567447604933
epsilon  0.45483567447604933
epsilon  0.45483567447604933
Episode number:  17
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.45483567447604933 agent memory len 924 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.45483567447604933 agent memory len 948 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.45483567447604933 agent memory len 1131 steps  1 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.45483567447604933 agent memory len 925 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.45483567447604933 agent memory len 949 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.45483567447604933 agent memory len 1132 steps  2 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.45483567447604933 agent memory len 926 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.45483567447604933 agent memory len 950 steps  3 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.45483567447604933 agent memory len 1133 steps  3 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.45483567447604933 agent memory len 927 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.45483567447604933 agent memory len 951 steps  4 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.45483567447604933 agent memory len 1134 steps  4 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'D']
agent epsilon  0.45483567447604933 agent memory len 928 steps  5 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.45483567447604933 agent memory len 952 steps  5 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.45483567447604933 agent memory len 1135 steps  5 reward -2 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.45483567447604933 agent memory len 929 steps  6 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.45483567447604933 agent memory len 953 steps  6 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.45483567447604933 agent memory len 1136 steps  6 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.45483567447604933 agent memory len 930 steps  7 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.45483567447604933 agent memory len 954 steps  7 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 1137 steps  7 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 931 steps  8 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 955 steps  8 reward -1 next state  4 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 1138 steps  8 reward -1 next state  2 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.45483567447604933 agent memory len 932 steps  9 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.45483567447604933 agent memory len 956 steps  9 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.45483567447604933 agent memory len 1139 steps  9 reward -1 next state  3 agent position  (9, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 933 steps  10 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 957 steps  10 reward 0 next state  5 agent position  (3, 6)
agent epsilon  0.45483567447604933 agent memory len 1140 steps  10 reward -1 next state  3 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 934 steps  11 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 958 steps  11 reward 0 next state  5 agent position  (3, 6)
agent epsilon  0.45483567447604933 agent memory len 1141 steps  11 reward -1 next state  3 agent position  (9, 5)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.45483567447604933 agent memory len 935 steps  12 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 959 steps  12 reward 0 next state  6 agent position  (4, 6)
agent epsilon  0.45483567447604933 agent memory len 1142 steps  12 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['L', 'L', 'D']
agent epsilon  0.45483567447604933 agent memory len 936 steps  13 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  13 reward 10 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1143 steps  13 reward -2 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 937 steps  14 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  14 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1144 steps  14 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 938 steps  15 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  15 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1145 steps  15 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 939 steps  16 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  16 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1146 steps  16 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 940 steps  17 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  17 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1147 steps  17 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 941 steps  18 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  18 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1148 steps  18 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 942 steps  19 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  19 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1149 steps  19 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 943 steps  20 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  20 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1150 steps  20 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 944 steps  21 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  21 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1151 steps  21 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 945 steps  22 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  22 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1152 steps  22 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 946 steps  23 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  23 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1153 steps  23 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 947 steps  24 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  24 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1154 steps  24 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 948 steps  25 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  25 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1155 steps  25 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 949 steps  26 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1156 steps  26 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 950 steps  27 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  27 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1157 steps  27 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 951 steps  28 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  28 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1158 steps  28 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 952 steps  29 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  29 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1159 steps  29 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 953 steps  30 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  30 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1160 steps  30 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 954 steps  31 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  31 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1161 steps  31 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 955 steps  32 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  32 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1162 steps  32 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 956 steps  33 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  33 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1163 steps  33 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 957 steps  34 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  34 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1164 steps  34 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 958 steps  35 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  35 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1165 steps  35 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 959 steps  36 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  36 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1166 steps  36 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 960 steps  37 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  37 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1167 steps  37 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 961 steps  38 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  38 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1168 steps  38 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 962 steps  39 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  39 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1169 steps  39 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 963 steps  40 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  40 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1170 steps  40 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 964 steps  41 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  41 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1171 steps  41 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 965 steps  42 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  42 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1172 steps  42 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 966 steps  43 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  43 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1173 steps  43 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 967 steps  44 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  44 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1174 steps  44 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 968 steps  45 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  45 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1175 steps  45 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 969 steps  46 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  46 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1176 steps  46 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 970 steps  47 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  47 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1177 steps  47 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 971 steps  48 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  48 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1178 steps  48 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 972 steps  49 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  49 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1179 steps  49 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 973 steps  50 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  50 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1180 steps  50 reward -2 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 974 steps  51 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.45483567447604933 agent memory len 960 steps  51 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1181 steps  51 reward -1 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 975 steps  52 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  52 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1182 steps  52 reward -2 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 976 steps  53 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1183 steps  53 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 977 steps  54 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1184 steps  54 reward -1 next state  4 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 978 steps  55 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  55 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1185 steps  55 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 979 steps  56 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1186 steps  56 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 980 steps  57 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  57 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1187 steps  57 reward -1 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 981 steps  58 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1188 steps  58 reward -2 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 982 steps  59 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.45483567447604933 agent memory len 960 steps  59 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1189 steps  59 reward -1 next state  4 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 983 steps  60 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1190 steps  60 reward -1 next state  4 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'U']
agent epsilon  0.45483567447604933 agent memory len 984 steps  61 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1191 steps  61 reward -1 next state  4 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.45483567447604933 agent memory len 984 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1192 steps  62 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 984 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1193 steps  63 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 984 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1194 steps  64 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 984 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1195 steps  65 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 984 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1196 steps  66 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 984 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1197 steps  67 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 984 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1198 steps  68 reward -1 next state  4 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 984 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1199 steps  69 reward -1 next state  4 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 984 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1200 steps  70 reward -1 next state  4 agent position  (9, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 984 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1201 steps  71 reward -1 next state  4 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 984 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1202 steps  72 reward -1 next state  4 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 984 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1203 steps  73 reward -1 next state  4 agent position  (9, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.45483567447604933 agent memory len 984 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1204 steps  74 reward -1 next state  4 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.45483567447604933 agent memory len 984 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 960 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 1205 steps  75 reward -2 next state  4 agent position  (9, 8)
max steps reached
total rewards -128
epsilon  0.4331407826292394
epsilon  0.4331407826292394
epsilon  0.4331407826292394
Episode number:  18
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 985 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.4331407826292394 agent memory len 961 steps  1 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1206 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 986 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.4331407826292394 agent memory len 962 steps  2 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1207 steps  2 reward -1 next state  0 agent position  (9, 2)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'R']
agent epsilon  0.4331407826292394 agent memory len 987 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4331407826292394 agent memory len 963 steps  3 reward -2 next state  2 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1208 steps  3 reward -1 next state  0 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.4331407826292394 agent memory len 988 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.4331407826292394 agent memory len 964 steps  4 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1209 steps  4 reward -1 next state  0 agent position  (9, 4)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'R']
agent epsilon  0.4331407826292394 agent memory len 989 steps  5 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4331407826292394 agent memory len 965 steps  5 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.4331407826292394 agent memory len 1210 steps  5 reward -1 next state  1 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 990 steps  6 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4331407826292394 agent memory len 966 steps  6 reward -1 next state  2 agent position  (1, 6)
agent epsilon  0.4331407826292394 agent memory len 1211 steps  6 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 991 steps  7 reward -2 next state  0 agent position  (0, 2)
agent epsilon  0.4331407826292394 agent memory len 967 steps  7 reward -1 next state  2 agent position  (1, 5)
agent epsilon  0.4331407826292394 agent memory len 1212 steps  7 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 992 steps  8 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.4331407826292394 agent memory len 968 steps  8 reward -1 next state  3 agent position  (1, 4)
agent epsilon  0.4331407826292394 agent memory len 1213 steps  8 reward -2 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 993 steps  9 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.4331407826292394 agent memory len 969 steps  9 reward -1 next state  4 agent position  (1, 3)
agent epsilon  0.4331407826292394 agent memory len 1214 steps  9 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 994 steps  10 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.4331407826292394 agent memory len 970 steps  10 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.4331407826292394 agent memory len 1215 steps  10 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.4331407826292394 agent memory len 995 steps  11 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.4331407826292394 agent memory len 971 steps  11 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.4331407826292394 agent memory len 1216 steps  11 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.4331407826292394 agent memory len 996 steps  12 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 972 steps  12 reward -1 next state  6 agent position  (1, 3)
agent epsilon  0.4331407826292394 agent memory len 1217 steps  12 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 997 steps  13 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 973 steps  13 reward -1 next state  6 agent position  (1, 2)
agent epsilon  0.4331407826292394 agent memory len 1218 steps  13 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 998 steps  14 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 974 steps  14 reward -1 next state  7 agent position  (1, 1)
agent epsilon  0.4331407826292394 agent memory len 1219 steps  14 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 999 steps  15 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 975 steps  15 reward -1 next state  7 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1220 steps  15 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
actions ['S', 'R', 'R']
agent epsilon  0.4331407826292394 agent memory len 1000 steps  16 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 976 steps  16 reward -1 next state  7 agent position  (1, 1)
agent epsilon  0.4331407826292394 agent memory len 1221 steps  16 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1001 steps  17 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 977 steps  17 reward -1 next state  7 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1222 steps  17 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1002 steps  18 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 978 steps  18 reward -2 next state  6 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1223 steps  18 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1003 steps  19 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 979 steps  19 reward -2 next state  6 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1224 steps  19 reward -1 next state  1 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1004 steps  20 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 980 steps  20 reward -2 next state  6 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1225 steps  20 reward -1 next state  1 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1005 steps  21 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 981 steps  21 reward -1 next state  6 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1226 steps  21 reward -1 next state  2 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  0.4331407826292394 agent memory len 1006 steps  22 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 982 steps  22 reward -1 next state  6 agent position  (2, 1)
agent epsilon  0.4331407826292394 agent memory len 1227 steps  22 reward -1 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1007 steps  23 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 983 steps  23 reward -1 next state  6 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1228 steps  23 reward -2 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.4331407826292394 agent memory len 1008 steps  24 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 984 steps  24 reward -1 next state  6 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1229 steps  24 reward -1 next state  2 agent position  (9, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1009 steps  25 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 985 steps  25 reward -2 next state  6 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1230 steps  25 reward -1 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1010 steps  26 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 986 steps  26 reward -2 next state  7 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1231 steps  26 reward -2 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1011 steps  27 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 987 steps  27 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1232 steps  27 reward -1 next state  2 agent position  (9, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1012 steps  28 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 988 steps  28 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1233 steps  28 reward -1 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'R']
agent epsilon  0.4331407826292394 agent memory len 1013 steps  29 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 989 steps  29 reward -1 next state  8 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1234 steps  29 reward -1 next state  3 agent position  (9, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1014 steps  30 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 990 steps  30 reward -2 next state  8 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1235 steps  30 reward -1 next state  3 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1015 steps  31 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 991 steps  31 reward -1 next state  8 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1236 steps  31 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1016 steps  32 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 992 steps  32 reward -1 next state  8 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1237 steps  32 reward -1 next state  5 agent position  (9, 4)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1017 steps  33 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 993 steps  33 reward -2 next state  8 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1238 steps  33 reward -1 next state  5 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1018 steps  34 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 994 steps  34 reward -2 next state  8 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1239 steps  34 reward -1 next state  5 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1019 steps  35 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.4331407826292394 agent memory len 995 steps  35 reward -2 next state  9 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1240 steps  35 reward -1 next state  5 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.4331407826292394 agent memory len 1020 steps  36 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.4331407826292394 agent memory len 996 steps  36 reward -1 next state  9 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1241 steps  36 reward -1 next state  5 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1021 steps  37 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 997 steps  37 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1242 steps  37 reward -1 next state  6 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'R', 'L']
agent epsilon  0.4331407826292394 agent memory len 1022 steps  38 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 998 steps  38 reward -1 next state  8 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 1243 steps  38 reward -1 next state  6 agent position  (9, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.4331407826292394 agent memory len 1023 steps  39 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.4331407826292394 agent memory len 999 steps  39 reward -1 next state  9 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 1244 steps  39 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1024 steps  40 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.4331407826292394 agent memory len 1000 steps  40 reward -1 next state  9 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1245 steps  40 reward -1 next state  6 agent position  (8, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1025 steps  41 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.4331407826292394 agent memory len 1001 steps  41 reward -2 next state  9 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1246 steps  41 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1026 steps  42 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1002 steps  42 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1247 steps  42 reward -1 next state  6 agent position  (8, 7)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1027 steps  43 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1003 steps  43 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1248 steps  43 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1028 steps  44 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1004 steps  44 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1249 steps  44 reward -1 next state  6 agent position  (8, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1029 steps  45 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1005 steps  45 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1250 steps  45 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'R', 'R']
agent epsilon  0.4331407826292394 agent memory len 1030 steps  46 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.4331407826292394 agent memory len 1006 steps  46 reward -1 next state  7 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 1251 steps  46 reward -1 next state  6 agent position  (8, 7)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1031 steps  47 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1007 steps  47 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1252 steps  47 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.4331407826292394 agent memory len 1032 steps  48 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.4331407826292394 agent memory len 1008 steps  48 reward -2 next state  9 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1253 steps  48 reward -1 next state  6 agent position  (7, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1033 steps  49 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1009 steps  49 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1254 steps  49 reward -1 next state  6 agent position  (7, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'S', 'R']
agent epsilon  0.4331407826292394 agent memory len 1034 steps  50 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1010 steps  50 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1255 steps  50 reward -1 next state  6 agent position  (7, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1035 steps  51 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1011 steps  51 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1256 steps  51 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1036 steps  52 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1012 steps  52 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1257 steps  52 reward -1 next state  6 agent position  (9, 6)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1037 steps  53 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1013 steps  53 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1258 steps  53 reward -1 next state  6 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.4331407826292394 agent memory len 1038 steps  54 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.4331407826292394 agent memory len 1014 steps  54 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1259 steps  54 reward -1 next state  6 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1039 steps  55 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1015 steps  55 reward -2 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1260 steps  55 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1040 steps  56 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  56 reward -1 next state  8 agent position  (7, 0)
agent epsilon  0.4331407826292394 agent memory len 1261 steps  56 reward -1 next state  7 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'L']
agent epsilon  0.4331407826292394 agent memory len 1041 steps  57 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1017 steps  57 reward -1 next state  8 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1262 steps  57 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.4331407826292394 agent memory len 1042 steps  58 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1018 steps  58 reward -1 next state  8 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 1263 steps  58 reward -1 next state  6 agent position  (8, 5)
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'R']
agent epsilon  0.4331407826292394 agent memory len 1043 steps  59 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1019 steps  59 reward -1 next state  8 agent position  (5, 1)
agent epsilon  0.4331407826292394 agent memory len 1264 steps  59 reward -1 next state  5 agent position  (8, 6)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1044 steps  60 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1020 steps  60 reward -1 next state  8 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 1265 steps  60 reward -1 next state  6 agent position  (8, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.4331407826292394 agent memory len 1045 steps  61 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.4331407826292394 agent memory len 1021 steps  61 reward -1 next state  8 agent position  (5, 1)
agent epsilon  0.4331407826292394 agent memory len 1266 steps  61 reward -1 next state  5 agent position  (7, 5)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1046 steps  62 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1022 steps  62 reward -1 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1267 steps  62 reward -1 next state  5 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1047 steps  63 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1023 steps  63 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1268 steps  63 reward -1 next state  5 agent position  (7, 5)
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1048 steps  64 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1024 steps  64 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1269 steps  64 reward -1 next state  5 agent position  (7, 6)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1049 steps  65 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1025 steps  65 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1270 steps  65 reward -1 next state  5 agent position  (7, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1050 steps  66 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1026 steps  66 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1271 steps  66 reward -1 next state  5 agent position  (8, 5)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1051 steps  67 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1027 steps  67 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1272 steps  67 reward -1 next state  5 agent position  (8, 6)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1052 steps  68 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1028 steps  68 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1273 steps  68 reward -1 next state  5 agent position  (8, 7)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1053 steps  69 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1029 steps  69 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1274 steps  69 reward -1 next state  5 agent position  (8, 6)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1054 steps  70 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1030 steps  70 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1275 steps  70 reward -1 next state  5 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1055 steps  71 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1031 steps  71 reward -2 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1276 steps  71 reward -1 next state  5 agent position  (8, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1056 steps  72 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1032 steps  72 reward -1 next state  7 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1277 steps  72 reward -1 next state  6 agent position  (8, 5)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'R']
agent epsilon  0.4331407826292394 agent memory len 1057 steps  73 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.4331407826292394 agent memory len 1033 steps  73 reward -2 next state  7 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1278 steps  73 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1058 steps  74 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 1034 steps  74 reward -2 next state  6 agent position  (6, 0)
agent epsilon  0.4331407826292394 agent memory len 1279 steps  74 reward -1 next state  6 agent position  (8, 6)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'R']
agent epsilon  0.4331407826292394 agent memory len 1059 steps  75 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.4331407826292394 agent memory len 1035 steps  75 reward -1 next state  6 agent position  (7, 0)
agent epsilon  0.4331407826292394 agent memory len 1280 steps  75 reward -1 next state  7 agent position  (8, 7)
max steps reached
total rewards -266
epsilon  0.4125039631431931
epsilon  0.4125039631431931
epsilon  0.4125039631431931
Episode number:  19
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.4125039631431931 agent memory len 1060 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.4125039631431931 agent memory len 1036 steps  1 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.4125039631431931 agent memory len 1281 steps  1 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1061 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.4125039631431931 agent memory len 1037 steps  2 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.4125039631431931 agent memory len 1282 steps  2 reward -2 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.4125039631431931 agent memory len 1062 steps  3 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.4125039631431931 agent memory len 1038 steps  3 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.4125039631431931 agent memory len 1283 steps  3 reward -1 next state  2 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'U']
agent epsilon  0.4125039631431931 agent memory len 1063 steps  4 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4125039631431931 agent memory len 1039 steps  4 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.4125039631431931 agent memory len 1284 steps  4 reward -1 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'U']
agent epsilon  0.4125039631431931 agent memory len 1064 steps  5 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.4125039631431931 agent memory len 1040 steps  5 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.4125039631431931 agent memory len 1285 steps  5 reward -1 next state  3 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.4125039631431931 agent memory len 1065 steps  6 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4125039631431931 agent memory len 1041 steps  6 reward -1 next state  2 agent position  (4, 7)
agent epsilon  0.4125039631431931 agent memory len 1286 steps  6 reward -1 next state  4 agent position  (6, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1066 steps  7 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.4125039631431931 agent memory len 1042 steps  7 reward -1 next state  2 agent position  (5, 7)
agent epsilon  0.4125039631431931 agent memory len 1287 steps  7 reward -1 next state  5 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1067 steps  8 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.4125039631431931 agent memory len 1043 steps  8 reward -1 next state  2 agent position  (5, 7)
agent epsilon  0.4125039631431931 agent memory len 1288 steps  8 reward -1 next state  5 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'U']
agent epsilon  0.4125039631431931 agent memory len 1068 steps  9 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.4125039631431931 agent memory len 1044 steps  9 reward 0 next state  2 agent position  (5, 6)
agent epsilon  0.4125039631431931 agent memory len 1289 steps  9 reward -1 next state  5 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1069 steps  10 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.4125039631431931 agent memory len 1045 steps  10 reward 0 next state  2 agent position  (5, 6)
agent epsilon  0.4125039631431931 agent memory len 1290 steps  10 reward -1 next state  5 agent position  (5, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.4125039631431931 agent memory len 1070 steps  11 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.4125039631431931 agent memory len 1046 steps  11 reward 0 next state  2 agent position  (5, 5)
agent epsilon  0.4125039631431931 agent memory len 1291 steps  11 reward -1 next state  5 agent position  (6, 1)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 1
actions ['D', 'L', 'U']
agent epsilon  0.4125039631431931 agent memory len 1071 steps  12 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  12 reward 10 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1292 steps  12 reward -1 next state  5 agent position  (5, 1)
 is_terminal [False, True, False]
actions ['D', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1072 steps  13 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  13 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1293 steps  13 reward -1 next state  5 agent position  (4, 1)
 is_terminal [False, True, False]
actions ['R', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1073 steps  14 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  14 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1294 steps  14 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1074 steps  15 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  15 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1295 steps  15 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1075 steps  16 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  16 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1296 steps  16 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1076 steps  17 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  17 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1297 steps  17 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1077 steps  18 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  18 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1298 steps  18 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1078 steps  19 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  19 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1299 steps  19 reward -1 next state  5 agent position  (4, 2)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1079 steps  20 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  20 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1300 steps  20 reward -1 next state  5 agent position  (4, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1080 steps  21 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  21 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1301 steps  21 reward -1 next state  5 agent position  (4, 2)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1081 steps  22 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  22 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1302 steps  22 reward -1 next state  5 agent position  (4, 2)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1082 steps  23 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  23 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1303 steps  23 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1083 steps  24 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  24 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1304 steps  24 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1084 steps  25 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  25 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1305 steps  25 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1085 steps  26 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  26 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1306 steps  26 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1086 steps  27 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  27 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1307 steps  27 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1087 steps  28 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  28 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1308 steps  28 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1088 steps  29 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  29 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1309 steps  29 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1089 steps  30 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  30 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1310 steps  30 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1090 steps  31 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  31 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1311 steps  31 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1091 steps  32 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  32 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1312 steps  32 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1092 steps  33 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  33 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1313 steps  33 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1093 steps  34 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  34 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1314 steps  34 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1094 steps  35 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  35 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1315 steps  35 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1095 steps  36 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  36 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1316 steps  36 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1096 steps  37 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  37 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1317 steps  37 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1097 steps  38 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  38 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1318 steps  38 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1098 steps  39 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  39 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1319 steps  39 reward -1 next state  5 agent position  (3, 0)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1099 steps  40 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  40 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1320 steps  40 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1100 steps  41 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  41 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1321 steps  41 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1101 steps  42 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  42 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1322 steps  42 reward -1 next state  5 agent position  (3, 1)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1102 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  43 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1323 steps  43 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1103 steps  44 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  44 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1324 steps  44 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1104 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  45 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1325 steps  45 reward -1 next state  5 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1105 steps  46 reward -1 next state  7 agent position  (7, 5)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  46 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1326 steps  46 reward 0 next state  5 agent position  (3, 3)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1106 steps  47 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  47 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1327 steps  47 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1107 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  48 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1328 steps  48 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1108 steps  49 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  49 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1329 steps  49 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1109 steps  50 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  50 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1330 steps  50 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1110 steps  51 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  51 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1331 steps  51 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1111 steps  52 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  52 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1332 steps  52 reward 0 next state  5 agent position  (4, 3)
 is_terminal [False, True, False]
random action 2
landmark captured 0
agent reached landmark-------------------------------- 2
actions ['L', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1112 steps  53 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  53 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  53 reward 10 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1113 steps  54 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  54 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  54 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1114 steps  55 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  55 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  55 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1115 steps  56 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  56 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  56 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1116 steps  57 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  57 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  57 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1117 steps  58 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  58 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  58 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1118 steps  59 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  59 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  59 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1119 steps  60 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  60 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  60 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1120 steps  61 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  61 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  61 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1121 steps  62 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  62 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  62 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1122 steps  63 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  63 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  63 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1123 steps  64 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  64 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  64 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1124 steps  65 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  65 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  65 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1125 steps  66 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  66 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  66 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1126 steps  67 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  67 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  67 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1127 steps  68 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  68 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  68 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1128 steps  69 reward -1 next state  8 agent position  (8, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  69 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  69 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1129 steps  70 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  70 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  70 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1130 steps  71 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  71 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  71 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1131 steps  72 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  72 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  72 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1132 steps  73 reward -1 next state  7 agent position  (7, 3)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  73 reward 0 next state  3 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  73 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1133 steps  74 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  74 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  74 reward 0 next state  5 agent position  (4, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1134 steps  75 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  75 reward 0 next state  2 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1333 steps  75 reward 0 next state  5 agent position  (4, 4)
max steps reached
total rewards -105
epsilon  0.3928736132199562
epsilon  0.3928736132199562
epsilon  0.3928736132199562
Episode number:  20
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'U']
agent epsilon  0.3928736132199562 agent memory len 1135 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.3928736132199562 agent memory len 1048 steps  1 reward -2 next state  1 agent position  (0, 9)
agent epsilon  0.3928736132199562 agent memory len 1334 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'R']
agent epsilon  0.3928736132199562 agent memory len 1136 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.3928736132199562 agent memory len 1049 steps  2 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.3928736132199562 agent memory len 1335 steps  2 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'R']
agent epsilon  0.3928736132199562 agent memory len 1137 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 1050 steps  3 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.3928736132199562 agent memory len 1336 steps  3 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.3928736132199562 agent memory len 1138 steps  4 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1051 steps  4 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.3928736132199562 agent memory len 1337 steps  4 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['R', 'D', 'U']
agent epsilon  0.3928736132199562 agent memory len 1139 steps  5 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1052 steps  5 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.3928736132199562 agent memory len 1338 steps  5 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.3928736132199562 agent memory len 1140 steps  6 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.3928736132199562 agent memory len 1053 steps  6 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.3928736132199562 agent memory len 1339 steps  6 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['S', 'L', 'U']
agent epsilon  0.3928736132199562 agent memory len 1141 steps  7 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.3928736132199562 agent memory len 1054 steps  7 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.3928736132199562 agent memory len 1340 steps  7 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.3928736132199562 agent memory len 1142 steps  8 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.3928736132199562 agent memory len 1055 steps  8 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.3928736132199562 agent memory len 1341 steps  8 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.3928736132199562 agent memory len 1143 steps  9 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.3928736132199562 agent memory len 1056 steps  9 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.3928736132199562 agent memory len 1342 steps  9 reward -1 next state  4 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.3928736132199562 agent memory len 1144 steps  10 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.3928736132199562 agent memory len 1057 steps  10 reward 0 next state  2 agent position  (4, 6)
agent epsilon  0.3928736132199562 agent memory len 1343 steps  10 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.3928736132199562 agent memory len 1145 steps  11 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.3928736132199562 agent memory len 1058 steps  11 reward 0 next state  2 agent position  (3, 6)
agent epsilon  0.3928736132199562 agent memory len 1344 steps  11 reward -1 next state  3 agent position  (6, 2)
 is_terminal [False, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.3928736132199562 agent memory len 1146 steps  12 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.3928736132199562 agent memory len 1059 steps  12 reward 0 next state  2 agent position  (4, 6)
agent epsilon  0.3928736132199562 agent memory len 1345 steps  12 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['U', 'L', 'R']
agent epsilon  0.3928736132199562 agent memory len 1147 steps  13 reward -2 next state  0 agent position  (0, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  13 reward 10 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1346 steps  13 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1148 steps  14 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  14 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1347 steps  14 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1149 steps  15 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  15 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1348 steps  15 reward 0 next state  4 agent position  (5, 3)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1150 steps  16 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  16 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1349 steps  16 reward 0 next state  4 agent position  (5, 3)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1151 steps  17 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  17 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1350 steps  17 reward -1 next state  4 agent position  (5, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1152 steps  18 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  18 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1351 steps  18 reward -1 next state  4 agent position  (5, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1153 steps  19 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  19 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1352 steps  19 reward -1 next state  4 agent position  (4, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1154 steps  20 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  20 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1353 steps  20 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1155 steps  21 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  21 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1354 steps  21 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1156 steps  22 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  22 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1355 steps  22 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1157 steps  23 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  23 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1356 steps  23 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1158 steps  24 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  24 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1357 steps  24 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1159 steps  25 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  25 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1358 steps  25 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1160 steps  26 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1359 steps  26 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1161 steps  27 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  27 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1360 steps  27 reward -1 next state  4 agent position  (4, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1162 steps  28 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  28 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1361 steps  28 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1163 steps  29 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  29 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1362 steps  29 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1164 steps  30 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  30 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1363 steps  30 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1165 steps  31 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  31 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1364 steps  31 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1166 steps  32 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  32 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1365 steps  32 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1167 steps  33 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  33 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1366 steps  33 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1168 steps  34 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  34 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1367 steps  34 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1169 steps  35 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  35 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1368 steps  35 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1170 steps  36 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  36 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1369 steps  36 reward -1 next state  4 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1171 steps  37 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  37 reward 0 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1370 steps  37 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1172 steps  38 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  38 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1371 steps  38 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1173 steps  39 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  39 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1372 steps  39 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1174 steps  40 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1373 steps  40 reward -1 next state  4 agent position  (2, 1)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1175 steps  41 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1374 steps  41 reward -1 next state  4 agent position  (2, 2)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1176 steps  42 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  42 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1375 steps  42 reward -1 next state  4 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1177 steps  43 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  43 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1376 steps  43 reward -1 next state  4 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1178 steps  44 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  44 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1377 steps  44 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1179 steps  45 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  45 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1378 steps  45 reward -1 next state  4 agent position  (2, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1180 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  46 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1379 steps  46 reward -1 next state  4 agent position  (1, 6)
 is_terminal [False, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1181 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  47 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1380 steps  47 reward -1 next state  4 agent position  (1, 7)
 is_terminal [False, True, False]
random action 0
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['L', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  48 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1381 steps  48 reward -1 next state  4 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1382 steps  49 reward -1 next state  4 agent position  (0, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1383 steps  50 reward -1 next state  4 agent position  (0, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1384 steps  51 reward -1 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1385 steps  52 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1386 steps  53 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1387 steps  54 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1388 steps  55 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1389 steps  56 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1390 steps  57 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1391 steps  58 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1392 steps  59 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1393 steps  60 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1394 steps  61 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1395 steps  62 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1396 steps  63 reward -2 next state  4 agent position  (0, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1397 steps  64 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1398 steps  65 reward -1 next state  4 agent position  (1, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1399 steps  66 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1400 steps  67 reward -2 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1401 steps  68 reward -2 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1402 steps  69 reward -2 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1403 steps  70 reward -1 next state  4 agent position  (1, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1404 steps  71 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1405 steps  72 reward -2 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1406 steps  73 reward -1 next state  4 agent position  (1, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1407 steps  74 reward -1 next state  4 agent position  (1, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1182 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3928736132199562 agent memory len 1060 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1408 steps  75 reward -1 next state  4 agent position  (1, 8)
max steps reached
total rewards -118
epsilon  0.3742006467597279
epsilon  0.3742006467597279
epsilon  0.3742006467597279
Episode number:  21
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.3742006467597279 agent memory len 1183 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.3742006467597279 agent memory len 1061 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.3742006467597279 agent memory len 1409 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'U']
agent epsilon  0.3742006467597279 agent memory len 1184 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.3742006467597279 agent memory len 1062 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.3742006467597279 agent memory len 1410 steps  2 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'D', 'U']
agent epsilon  0.3742006467597279 agent memory len 1185 steps  3 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.3742006467597279 agent memory len 1063 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.3742006467597279 agent memory len 1411 steps  3 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.3742006467597279 agent memory len 1186 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3742006467597279 agent memory len 1064 steps  4 reward -1 next state  0 agent position  (3, 8)
agent epsilon  0.3742006467597279 agent memory len 1412 steps  4 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'S']
agent epsilon  0.3742006467597279 agent memory len 1187 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.3742006467597279 agent memory len 1065 steps  5 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.3742006467597279 agent memory len 1413 steps  5 reward -1 next state  2 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.3742006467597279 agent memory len 1188 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3742006467597279 agent memory len 1066 steps  6 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.3742006467597279 agent memory len 1414 steps  6 reward -1 next state  1 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.3742006467597279 agent memory len 1189 steps  7 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.3742006467597279 agent memory len 1067 steps  7 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.3742006467597279 agent memory len 1415 steps  7 reward -1 next state  1 agent position  (5, 2)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'U']
agent epsilon  0.3742006467597279 agent memory len 1190 steps  8 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3742006467597279 agent memory len 1068 steps  8 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.3742006467597279 agent memory len 1416 steps  8 reward -1 next state  1 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'U']
agent epsilon  0.3742006467597279 agent memory len 1191 steps  9 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.3742006467597279 agent memory len 1069 steps  9 reward -1 next state  1 agent position  (2, 8)
agent epsilon  0.3742006467597279 agent memory len 1417 steps  9 reward -1 next state  2 agent position  (3, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.3742006467597279 agent memory len 1192 steps  10 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3742006467597279 agent memory len 1070 steps  10 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.3742006467597279 agent memory len 1418 steps  10 reward -1 next state  1 agent position  (3, 2)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'U']
agent epsilon  0.3742006467597279 agent memory len 1193 steps  11 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.3742006467597279 agent memory len 1071 steps  11 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.3742006467597279 agent memory len 1419 steps  11 reward -1 next state  1 agent position  (2, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.3742006467597279 agent memory len 1194 steps  12 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.3742006467597279 agent memory len 1072 steps  12 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.3742006467597279 agent memory len 1420 steps  12 reward -1 next state  2 agent position  (2, 2)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'U']
agent epsilon  0.3742006467597279 agent memory len 1195 steps  13 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.3742006467597279 agent memory len 1073 steps  13 reward -1 next state  3 agent position  (1, 7)
agent epsilon  0.3742006467597279 agent memory len 1421 steps  13 reward -1 next state  1 agent position  (1, 2)
 is_terminal [False, False, False]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  14 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1074 steps  14 reward -1 next state  4 agent position  (1, 7)
agent epsilon  0.3742006467597279 agent memory len 1422 steps  14 reward -1 next state  1 agent position  (0, 2)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1075 steps  15 reward -1 next state  4 agent position  (1, 8)
agent epsilon  0.3742006467597279 agent memory len 1423 steps  15 reward -1 next state  1 agent position  (1, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1076 steps  16 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.3742006467597279 agent memory len 1424 steps  16 reward -1 next state  2 agent position  (2, 2)
 is_terminal [True, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1077 steps  17 reward -1 next state  4 agent position  (3, 8)
agent epsilon  0.3742006467597279 agent memory len 1425 steps  17 reward -1 next state  3 agent position  (3, 2)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1078 steps  18 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.3742006467597279 agent memory len 1426 steps  18 reward -1 next state  4 agent position  (2, 2)
 is_terminal [True, False, False]
actions ['S', 'L', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1079 steps  19 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.3742006467597279 agent memory len 1427 steps  19 reward -1 next state  4 agent position  (3, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1080 steps  20 reward 0 next state  4 agent position  (4, 6)
agent epsilon  0.3742006467597279 agent memory len 1428 steps  20 reward -1 next state  4 agent position  (3, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1081 steps  21 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.3742006467597279 agent memory len 1429 steps  21 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1082 steps  22 reward 0 next state  4 agent position  (4, 6)
agent epsilon  0.3742006467597279 agent memory len 1430 steps  22 reward -1 next state  4 agent position  (3, 3)
 is_terminal [True, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  23 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1431 steps  23 reward -1 next state  4 agent position  (2, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1432 steps  24 reward -1 next state  4 agent position  (3, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1433 steps  25 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1434 steps  26 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1435 steps  27 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1436 steps  28 reward -1 next state  4 agent position  (2, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1437 steps  29 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1438 steps  30 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1439 steps  31 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1440 steps  32 reward -1 next state  4 agent position  (2, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1441 steps  33 reward -1 next state  4 agent position  (2, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1442 steps  34 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1443 steps  35 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1444 steps  36 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1445 steps  37 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1446 steps  38 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1447 steps  39 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1448 steps  40 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1449 steps  41 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1450 steps  42 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1451 steps  43 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1452 steps  44 reward -1 next state  4 agent position  (2, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1453 steps  45 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1454 steps  46 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1455 steps  47 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1456 steps  48 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1457 steps  49 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1458 steps  50 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1459 steps  51 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1196 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.3742006467597279 agent memory len 1083 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1460 steps  52 reward 10 next state  4 agent position  (5, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards -43
epsilon  0.35643837162004377
epsilon  0.35643837162004377
epsilon  0.35643837162004377
Episode number:  22
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1197 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1084 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.35643837162004377 agent memory len 1461 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1198 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.35643837162004377 agent memory len 1085 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.35643837162004377 agent memory len 1462 steps  2 reward -1 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1199 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.35643837162004377 agent memory len 1086 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.35643837162004377 agent memory len 1463 steps  3 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1200 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.35643837162004377 agent memory len 1087 steps  4 reward -1 next state  2 agent position  (3, 9)
agent epsilon  0.35643837162004377 agent memory len 1464 steps  4 reward -1 next state  3 agent position  (9, 3)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.35643837162004377 agent memory len 1201 steps  5 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.35643837162004377 agent memory len 1088 steps  5 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.35643837162004377 agent memory len 1465 steps  5 reward -1 next state  3 agent position  (9, 4)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.35643837162004377 agent memory len 1202 steps  6 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.35643837162004377 agent memory len 1089 steps  6 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.35643837162004377 agent memory len 1466 steps  6 reward -1 next state  3 agent position  (9, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.35643837162004377 agent memory len 1203 steps  7 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.35643837162004377 agent memory len 1090 steps  7 reward -1 next state  5 agent position  (3, 7)
agent epsilon  0.35643837162004377 agent memory len 1467 steps  7 reward -1 next state  3 agent position  (9, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.35643837162004377 agent memory len 1204 steps  8 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.35643837162004377 agent memory len 1091 steps  8 reward 0 next state  5 agent position  (3, 6)
agent epsilon  0.35643837162004377 agent memory len 1468 steps  8 reward -1 next state  3 agent position  (9, 3)
 is_terminal [False, False, False]
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'R', 'L']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  9 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1092 steps  9 reward -1 next state  5 agent position  (3, 7)
agent epsilon  0.35643837162004377 agent memory len 1469 steps  9 reward -1 next state  3 agent position  (9, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  10 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1093 steps  10 reward -1 next state  5 agent position  (4, 7)
agent epsilon  0.35643837162004377 agent memory len 1470 steps  10 reward -2 next state  4 agent position  (9, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1094 steps  11 reward -1 next state  5 agent position  (4, 6)
agent epsilon  0.35643837162004377 agent memory len 1471 steps  11 reward -2 next state  4 agent position  (9, 2)
 is_terminal [True, False, False]
actions ['S', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1095 steps  12 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.35643837162004377 agent memory len 1472 steps  12 reward -1 next state  5 agent position  (9, 3)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1096 steps  13 reward 0 next state  5 agent position  (5, 5)
agent epsilon  0.35643837162004377 agent memory len 1473 steps  13 reward -1 next state  5 agent position  (8, 3)
 is_terminal [True, False, False]
landmark captured 2
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  14 reward 10 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1474 steps  14 reward -1 next state  5 agent position  (8, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  15 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1475 steps  15 reward -1 next state  5 agent position  (8, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  16 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1476 steps  16 reward -1 next state  5 agent position  (7, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  17 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1477 steps  17 reward -1 next state  5 agent position  (7, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  18 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1478 steps  18 reward -1 next state  5 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  19 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1479 steps  19 reward -1 next state  5 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  20 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1480 steps  20 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  21 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1481 steps  21 reward -1 next state  5 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  22 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1482 steps  22 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  23 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1483 steps  23 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  24 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1484 steps  24 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  25 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1485 steps  25 reward -1 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  26 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1486 steps  26 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  27 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1487 steps  27 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  28 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1488 steps  28 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  29 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1489 steps  29 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  30 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1490 steps  30 reward -1 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  31 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1491 steps  31 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  32 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1492 steps  32 reward -1 next state  5 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  33 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1493 steps  33 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  34 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1494 steps  34 reward -1 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  35 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1495 steps  35 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  36 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1496 steps  36 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  37 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1497 steps  37 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  38 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1498 steps  38 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  39 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1499 steps  39 reward -2 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  40 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1500 steps  40 reward -1 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  41 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1501 steps  41 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  42 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1502 steps  42 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  43 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1503 steps  43 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  44 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1504 steps  44 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  45 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1505 steps  45 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  46 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1506 steps  46 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  47 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1507 steps  47 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  48 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1508 steps  48 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  49 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1509 steps  49 reward -1 next state  5 agent position  (8, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  50 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1510 steps  50 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  51 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1511 steps  51 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  52 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1512 steps  52 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  53 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1513 steps  53 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  54 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1514 steps  54 reward -1 next state  5 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  55 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1515 steps  55 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  56 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1516 steps  56 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  57 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1517 steps  57 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  58 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1518 steps  58 reward -1 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  59 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1519 steps  59 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  60 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1520 steps  60 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  61 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1521 steps  61 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  62 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1522 steps  62 reward -2 next state  5 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  63 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1523 steps  63 reward -1 next state  5 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  64 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1524 steps  64 reward -1 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  65 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1525 steps  65 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  66 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1526 steps  66 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  67 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1527 steps  67 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  68 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1528 steps  68 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  69 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1529 steps  69 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  70 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1530 steps  70 reward -1 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  71 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1531 steps  71 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  72 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1532 steps  72 reward -2 next state  5 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  73 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1533 steps  73 reward -1 next state  5 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  74 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1534 steps  74 reward -2 next state  5 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1205 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1097 steps  75 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.35643837162004377 agent memory len 1535 steps  75 reward -1 next state  5 agent position  (4, 9)
max steps reached
total rewards -109
epsilon  0.3395423728610988
epsilon  0.3395423728610988
epsilon  0.3395423728610988
Episode number:  23
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1206 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3395423728610988 agent memory len 1098 steps  1 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.3395423728610988 agent memory len 1536 steps  1 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'L']
agent epsilon  0.3395423728610988 agent memory len 1207 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1099 steps  2 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.3395423728610988 agent memory len 1537 steps  2 reward -2 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1208 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3395423728610988 agent memory len 1100 steps  3 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.3395423728610988 agent memory len 1538 steps  3 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1209 steps  4 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1101 steps  4 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.3395423728610988 agent memory len 1539 steps  4 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1210 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3395423728610988 agent memory len 1102 steps  5 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.3395423728610988 agent memory len 1540 steps  5 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1211 steps  6 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1103 steps  6 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.3395423728610988 agent memory len 1541 steps  6 reward -1 next state  0 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1212 steps  7 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1104 steps  7 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.3395423728610988 agent memory len 1542 steps  7 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1213 steps  8 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1105 steps  8 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.3395423728610988 agent memory len 1543 steps  8 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['S', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1214 steps  9 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1106 steps  9 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.3395423728610988 agent memory len 1544 steps  9 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1215 steps  10 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1107 steps  10 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.3395423728610988 agent memory len 1545 steps  10 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 1
actions ['S', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1216 steps  11 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1108 steps  11 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.3395423728610988 agent memory len 1546 steps  11 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'L']
agent epsilon  0.3395423728610988 agent memory len 1217 steps  12 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1109 steps  12 reward -1 next state  1 agent position  (0, 6)
agent epsilon  0.3395423728610988 agent memory len 1547 steps  12 reward -1 next state  0 agent position  (9, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1218 steps  13 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1110 steps  13 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.3395423728610988 agent memory len 1548 steps  13 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.3395423728610988 agent memory len 1219 steps  14 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1111 steps  14 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.3395423728610988 agent memory len 1549 steps  14 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.3395423728610988 agent memory len 1220 steps  15 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1112 steps  15 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.3395423728610988 agent memory len 1550 steps  15 reward -1 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1221 steps  16 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1113 steps  16 reward 0 next state  1 agent position  (3, 5)
agent epsilon  0.3395423728610988 agent memory len 1551 steps  16 reward -1 next state  3 agent position  (8, 1)
 is_terminal [False, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1222 steps  17 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1114 steps  17 reward 0 next state  1 agent position  (3, 4)
agent epsilon  0.3395423728610988 agent memory len 1552 steps  17 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3395423728610988 agent memory len 1223 steps  18 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1115 steps  18 reward 0 next state  1 agent position  (3, 4)
agent epsilon  0.3395423728610988 agent memory len 1553 steps  18 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'D']
agent epsilon  0.3395423728610988 agent memory len 1224 steps  19 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1116 steps  19 reward 0 next state  1 agent position  (3, 3)
agent epsilon  0.3395423728610988 agent memory len 1554 steps  19 reward -2 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1225 steps  20 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1117 steps  20 reward 0 next state  1 agent position  (3, 4)
agent epsilon  0.3395423728610988 agent memory len 1555 steps  20 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1226 steps  21 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1118 steps  21 reward 0 next state  1 agent position  (3, 3)
agent epsilon  0.3395423728610988 agent memory len 1556 steps  21 reward -1 next state  3 agent position  (9, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.3395423728610988 agent memory len 1227 steps  22 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1119 steps  22 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.3395423728610988 agent memory len 1557 steps  22 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'L']
agent epsilon  0.3395423728610988 agent memory len 1228 steps  23 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1120 steps  23 reward 0 next state  1 agent position  (3, 3)
agent epsilon  0.3395423728610988 agent memory len 1558 steps  23 reward -1 next state  3 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1229 steps  24 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1121 steps  24 reward 0 next state  1 agent position  (4, 3)
agent epsilon  0.3395423728610988 agent memory len 1559 steps  24 reward -1 next state  4 agent position  (9, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1230 steps  25 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1122 steps  25 reward 0 next state  1 agent position  (5, 3)
agent epsilon  0.3395423728610988 agent memory len 1560 steps  25 reward -1 next state  5 agent position  (9, 3)
 is_terminal [False, False, False]
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.3395423728610988 agent memory len 1231 steps  26 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.3395423728610988 agent memory len 1123 steps  26 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1561 steps  26 reward -2 next state  5 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.3395423728610988 agent memory len 1232 steps  27 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1124 steps  27 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1562 steps  27 reward -1 next state  5 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.3395423728610988 agent memory len 1233 steps  28 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1125 steps  28 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1563 steps  28 reward -1 next state  5 agent position  (8, 2)
 is_terminal [False, False, False]
random action 2
actions ['L', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1234 steps  29 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1126 steps  29 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1564 steps  29 reward -1 next state  5 agent position  (8, 2)
 is_terminal [False, False, False]
actions ['L', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1235 steps  30 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1127 steps  30 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1565 steps  30 reward -1 next state  5 agent position  (8, 3)
 is_terminal [False, False, False]
random action 2
actions ['L', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1236 steps  31 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1128 steps  31 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1566 steps  31 reward -1 next state  5 agent position  (8, 2)
 is_terminal [False, False, False]
actions ['L', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1237 steps  32 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1129 steps  32 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1567 steps  32 reward -1 next state  5 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['L', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1238 steps  33 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1130 steps  33 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1568 steps  33 reward -1 next state  5 agent position  (8, 2)
 is_terminal [False, False, False]
random action 1
actions ['L', 'S', 'R']
agent epsilon  0.3395423728610988 agent memory len 1239 steps  34 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1131 steps  34 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1569 steps  34 reward -1 next state  5 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1240 steps  35 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1132 steps  35 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1570 steps  35 reward -1 next state  5 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'L']
agent epsilon  0.3395423728610988 agent memory len 1241 steps  36 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1133 steps  36 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1571 steps  36 reward -1 next state  6 agent position  (8, 2)
 is_terminal [False, False, False]
actions ['L', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1242 steps  37 reward -2 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1134 steps  37 reward -1 next state  0 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1572 steps  37 reward -1 next state  6 agent position  (8, 3)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1243 steps  38 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1135 steps  38 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1573 steps  38 reward -1 next state  6 agent position  (8, 2)
 is_terminal [False, False, False]
random action 1
actions ['L', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1244 steps  39 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1136 steps  39 reward -1 next state  0 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1574 steps  39 reward -1 next state  7 agent position  (8, 3)
 is_terminal [False, False, False]
random action 2
actions ['L', 'R', 'U']
agent epsilon  0.3395423728610988 agent memory len 1245 steps  40 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1137 steps  40 reward -1 next state  0 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1575 steps  40 reward -1 next state  7 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1246 steps  41 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1138 steps  41 reward -1 next state  0 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1576 steps  41 reward -1 next state  7 agent position  (7, 3)
 is_terminal [False, False, False]
random action 1
actions ['L', 'U', 'R']
agent epsilon  0.3395423728610988 agent memory len 1247 steps  42 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1139 steps  42 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1577 steps  42 reward -1 next state  6 agent position  (7, 4)
 is_terminal [False, False, False]
actions ['L', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1248 steps  43 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1140 steps  43 reward -1 next state  0 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1578 steps  43 reward -1 next state  6 agent position  (7, 3)
 is_terminal [False, False, False]
actions ['L', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1249 steps  44 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1141 steps  44 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1579 steps  44 reward -1 next state  6 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.3395423728610988 agent memory len 1250 steps  45 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.3395423728610988 agent memory len 1142 steps  45 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1580 steps  45 reward -1 next state  6 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1251 steps  46 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1143 steps  46 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1581 steps  46 reward -1 next state  6 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1252 steps  47 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1144 steps  47 reward 0 next state  1 agent position  (6, 3)
agent epsilon  0.3395423728610988 agent memory len 1582 steps  47 reward -1 next state  6 agent position  (7, 3)
 is_terminal [False, False, False]
random action 2
actions ['U', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1253 steps  48 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1145 steps  48 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1583 steps  48 reward -1 next state  6 agent position  (7, 4)
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'L']
agent epsilon  0.3395423728610988 agent memory len 1254 steps  49 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1146 steps  49 reward -1 next state  1 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1584 steps  49 reward -1 next state  5 agent position  (7, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.3395423728610988 agent memory len 1255 steps  50 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1147 steps  50 reward -1 next state  1 agent position  (4, 2)
agent epsilon  0.3395423728610988 agent memory len 1585 steps  50 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, False, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3395423728610988 agent memory len 1256 steps  51 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1148 steps  51 reward -1 next state  1 agent position  (4, 2)
agent epsilon  0.3395423728610988 agent memory len 1586 steps  51 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1257 steps  52 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1149 steps  52 reward -1 next state  1 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1587 steps  52 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1258 steps  53 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1150 steps  53 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1588 steps  53 reward 0 next state  5 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['S', 'R', 'L']
agent epsilon  0.3395423728610988 agent memory len 1259 steps  54 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1151 steps  54 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1589 steps  54 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1260 steps  55 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1152 steps  55 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1590 steps  55 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
actions ['U', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1261 steps  56 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1153 steps  56 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1591 steps  56 reward 0 next state  5 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1262 steps  57 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1154 steps  57 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1592 steps  57 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1263 steps  58 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1155 steps  58 reward -1 next state  0 agent position  (5, 0)
agent epsilon  0.3395423728610988 agent memory len 1593 steps  58 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.3395423728610988 agent memory len 1264 steps  59 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1156 steps  59 reward -1 next state  0 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1594 steps  59 reward 0 next state  4 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['R', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1265 steps  60 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1157 steps  60 reward -1 next state  1 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1595 steps  60 reward 0 next state  4 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.3395423728610988 agent memory len 1266 steps  61 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1158 steps  61 reward -1 next state  1 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1596 steps  61 reward 0 next state  4 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.3395423728610988 agent memory len 1267 steps  62 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.3395423728610988 agent memory len 1159 steps  62 reward -1 next state  2 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1597 steps  62 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'R']
agent epsilon  0.3395423728610988 agent memory len 1268 steps  63 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.3395423728610988 agent memory len 1160 steps  63 reward -1 next state  2 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1598 steps  63 reward 0 next state  5 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'S', 'U']
agent epsilon  0.3395423728610988 agent memory len 1269 steps  64 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1161 steps  64 reward -1 next state  2 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  64 reward 10 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['L', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1270 steps  65 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1162 steps  65 reward -1 next state  1 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  65 reward 0 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1271 steps  66 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.3395423728610988 agent memory len 1163 steps  66 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  66 reward 0 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1272 steps  67 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1164 steps  67 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  67 reward 0 next state  6 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1273 steps  68 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3395423728610988 agent memory len 1165 steps  68 reward -1 next state  1 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  68 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1274 steps  69 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1166 steps  69 reward -1 next state  1 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  69 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1275 steps  70 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1167 steps  70 reward -1 next state  1 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  70 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1276 steps  71 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1168 steps  71 reward -1 next state  1 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  71 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1277 steps  72 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1169 steps  72 reward -1 next state  1 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  72 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1278 steps  73 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1170 steps  73 reward -1 next state  1 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  73 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1279 steps  74 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3395423728610988 agent memory len 1171 steps  74 reward -1 next state  1 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  74 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['U', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1280 steps  75 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.3395423728610988 agent memory len 1172 steps  75 reward -1 next state  1 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1599 steps  75 reward 0 next state  7 agent position  (5, 4)
max steps reached
total rewards -198
epsilon  0.32347040168526264
epsilon  0.32347040168526264
epsilon  0.32347040168526264
Episode number:  24
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1281 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.32347040168526264 agent memory len 1173 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.32347040168526264 agent memory len 1600 steps  1 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.32347040168526264 agent memory len 1282 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.32347040168526264 agent memory len 1174 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.32347040168526264 agent memory len 1601 steps  2 reward -1 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1283 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.32347040168526264 agent memory len 1175 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1602 steps  3 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1284 steps  4 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1176 steps  4 reward -1 next state  1 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1603 steps  4 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.32347040168526264 agent memory len 1285 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.32347040168526264 agent memory len 1177 steps  5 reward -1 next state  0 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1604 steps  5 reward -2 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1286 steps  6 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1178 steps  6 reward -1 next state  1 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1605 steps  6 reward -1 next state  2 agent position  (9, 4)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1287 steps  7 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1179 steps  7 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.32347040168526264 agent memory len 1606 steps  7 reward -1 next state  2 agent position  (9, 5)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.32347040168526264 agent memory len 1288 steps  8 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.32347040168526264 agent memory len 1180 steps  8 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.32347040168526264 agent memory len 1607 steps  8 reward -2 next state  2 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1289 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.32347040168526264 agent memory len 1181 steps  9 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.32347040168526264 agent memory len 1608 steps  9 reward -1 next state  2 agent position  (9, 6)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1290 steps  10 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.32347040168526264 agent memory len 1182 steps  10 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1609 steps  10 reward -1 next state  2 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.32347040168526264 agent memory len 1291 steps  11 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.32347040168526264 agent memory len 1183 steps  11 reward -1 next state  3 agent position  (2, 3)
agent epsilon  0.32347040168526264 agent memory len 1610 steps  11 reward -1 next state  2 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.32347040168526264 agent memory len 1292 steps  12 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.32347040168526264 agent memory len 1184 steps  12 reward -1 next state  4 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1611 steps  12 reward -1 next state  2 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['R', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1293 steps  13 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.32347040168526264 agent memory len 1185 steps  13 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1612 steps  13 reward -1 next state  2 agent position  (9, 8)
 is_terminal [False, False, False]
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  14 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1186 steps  14 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1613 steps  14 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1187 steps  15 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1614 steps  15 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1188 steps  16 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1615 steps  16 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1189 steps  17 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1616 steps  17 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1190 steps  18 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1617 steps  18 reward -1 next state  1 agent position  (9, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1191 steps  19 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1618 steps  19 reward -1 next state  2 agent position  (9, 7)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1192 steps  20 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1619 steps  20 reward -1 next state  2 agent position  (8, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1193 steps  21 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1620 steps  21 reward -1 next state  1 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1194 steps  22 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1621 steps  22 reward -1 next state  1 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1195 steps  23 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1622 steps  23 reward -1 next state  1 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1196 steps  24 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1623 steps  24 reward -1 next state  1 agent position  (9, 7)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1197 steps  25 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1624 steps  25 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1198 steps  26 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1625 steps  26 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1199 steps  27 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1626 steps  27 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1200 steps  28 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1627 steps  28 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1201 steps  29 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1628 steps  29 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1202 steps  30 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1629 steps  30 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1203 steps  31 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1630 steps  31 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1204 steps  32 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1631 steps  32 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1205 steps  33 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1632 steps  33 reward -2 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1206 steps  34 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1633 steps  34 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1207 steps  35 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1634 steps  35 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1208 steps  36 reward -1 next state  5 agent position  (1, 2)
agent epsilon  0.32347040168526264 agent memory len 1635 steps  36 reward -1 next state  1 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1209 steps  37 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1636 steps  37 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1210 steps  38 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1637 steps  38 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1211 steps  39 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1638 steps  39 reward -2 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1212 steps  40 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1639 steps  40 reward -1 next state  2 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1213 steps  41 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1640 steps  41 reward -1 next state  2 agent position  (9, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1214 steps  42 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1641 steps  42 reward -1 next state  2 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1215 steps  43 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1642 steps  43 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1216 steps  44 reward -1 next state  5 agent position  (2, 0)
agent epsilon  0.32347040168526264 agent memory len 1643 steps  44 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1217 steps  45 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1644 steps  45 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1218 steps  46 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1645 steps  46 reward -2 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1219 steps  47 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1646 steps  47 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1220 steps  48 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1647 steps  48 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1221 steps  49 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1648 steps  49 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1222 steps  50 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1649 steps  50 reward -2 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1223 steps  51 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1650 steps  51 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1224 steps  52 reward -1 next state  5 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1651 steps  52 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1225 steps  53 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1652 steps  53 reward -1 next state  2 agent position  (8, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1226 steps  54 reward -1 next state  5 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1653 steps  54 reward -1 next state  2 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1227 steps  55 reward -1 next state  5 agent position  (3, 1)
agent epsilon  0.32347040168526264 agent memory len 1654 steps  55 reward -1 next state  3 agent position  (9, 9)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1228 steps  56 reward -1 next state  5 agent position  (3, 1)
agent epsilon  0.32347040168526264 agent memory len 1655 steps  56 reward -1 next state  3 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1229 steps  57 reward -1 next state  5 agent position  (4, 1)
agent epsilon  0.32347040168526264 agent memory len 1656 steps  57 reward -1 next state  4 agent position  (9, 9)
 is_terminal [True, False, False]
actions ['S', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1230 steps  58 reward -1 next state  5 agent position  (4, 1)
agent epsilon  0.32347040168526264 agent memory len 1657 steps  58 reward -1 next state  4 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1231 steps  59 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.32347040168526264 agent memory len 1658 steps  59 reward -1 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1232 steps  60 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.32347040168526264 agent memory len 1659 steps  60 reward -1 next state  5 agent position  (9, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1233 steps  61 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1660 steps  61 reward -1 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1234 steps  62 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1661 steps  62 reward -1 next state  5 agent position  (9, 7)
 is_terminal [True, False, False]
actions ['S', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1235 steps  63 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1662 steps  63 reward -1 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1236 steps  64 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1663 steps  64 reward -1 next state  5 agent position  (9, 9)
 is_terminal [True, False, False]
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1237 steps  65 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1664 steps  65 reward -1 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1238 steps  66 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.32347040168526264 agent memory len 1665 steps  66 reward -1 next state  5 agent position  (9, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1239 steps  67 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1666 steps  67 reward -1 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1240 steps  68 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1667 steps  68 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, False]
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1241 steps  69 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1668 steps  69 reward -1 next state  5 agent position  (8, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1242 steps  70 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.32347040168526264 agent memory len 1669 steps  70 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1243 steps  71 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.32347040168526264 agent memory len 1670 steps  71 reward -1 next state  5 agent position  (8, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1244 steps  72 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.32347040168526264 agent memory len 1671 steps  72 reward -1 next state  5 agent position  (8, 6)
 is_terminal [True, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1245 steps  73 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.32347040168526264 agent memory len 1672 steps  73 reward -1 next state  5 agent position  (8, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'L']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1246 steps  74 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1673 steps  74 reward -1 next state  5 agent position  (8, 6)
 is_terminal [True, False, False]
actions ['S', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1294 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.32347040168526264 agent memory len 1247 steps  75 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.32347040168526264 agent memory len 1674 steps  75 reward -1 next state  5 agent position  (8, 7)
max steps reached
total rewards -155
epsilon  0.30818226979308
epsilon  0.30818226979308
epsilon  0.30818226979308
Episode number:  25
 is_terminal [False, False, False]
actions ['R', 'D', 'U']
agent epsilon  0.30818226979308 agent memory len 1295 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.30818226979308 agent memory len 1248 steps  1 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.30818226979308 agent memory len 1675 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.30818226979308 agent memory len 1296 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1249 steps  2 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.30818226979308 agent memory len 1676 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'U']
agent epsilon  0.30818226979308 agent memory len 1297 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.30818226979308 agent memory len 1250 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.30818226979308 agent memory len 1677 steps  3 reward -1 next state  3 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.30818226979308 agent memory len 1298 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1251 steps  4 reward -1 next state  1 agent position  (3, 8)
agent epsilon  0.30818226979308 agent memory len 1678 steps  4 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1299 steps  5 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1252 steps  5 reward -1 next state  2 agent position  (3, 7)
agent epsilon  0.30818226979308 agent memory len 1679 steps  5 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1300 steps  6 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1253 steps  6 reward -1 next state  2 agent position  (3, 8)
agent epsilon  0.30818226979308 agent memory len 1680 steps  6 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1301 steps  7 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.30818226979308 agent memory len 1254 steps  7 reward -1 next state  1 agent position  (3, 7)
agent epsilon  0.30818226979308 agent memory len 1681 steps  7 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'D', 'R']
agent epsilon  0.30818226979308 agent memory len 1302 steps  8 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.30818226979308 agent memory len 1255 steps  8 reward -1 next state  1 agent position  (4, 7)
agent epsilon  0.30818226979308 agent memory len 1682 steps  8 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'R']
agent epsilon  0.30818226979308 agent memory len 1303 steps  9 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.30818226979308 agent memory len 1256 steps  9 reward 0 next state  1 agent position  (4, 6)
agent epsilon  0.30818226979308 agent memory len 1683 steps  9 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'L']
agent epsilon  0.30818226979308 agent memory len 1304 steps  10 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1257 steps  10 reward 10 next state  2 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1684 steps  10 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1305 steps  11 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.30818226979308 agent memory len 1257 steps  11 reward 0 next state  2 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1685 steps  11 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1306 steps  12 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.30818226979308 agent memory len 1257 steps  12 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1686 steps  12 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1307 steps  13 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.30818226979308 agent memory len 1257 steps  13 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1687 steps  13 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, True, False]
random action 2
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  14 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1688 steps  14 reward 0 next state  4 agent position  (6, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1689 steps  15 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1690 steps  16 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1691 steps  17 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.30818226979308 agent memory len 1308 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1692 steps  18 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1693 steps  19 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1694 steps  20 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1695 steps  21 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1308 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1696 steps  22 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1697 steps  23 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1698 steps  24 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1699 steps  25 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.30818226979308 agent memory len 1308 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1700 steps  26 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1701 steps  27 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1702 steps  28 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1703 steps  29 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1704 steps  30 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1705 steps  31 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1706 steps  32 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1707 steps  33 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1708 steps  34 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1709 steps  35 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.30818226979308 agent memory len 1308 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1710 steps  36 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1711 steps  37 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1712 steps  38 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1713 steps  39 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1714 steps  40 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1715 steps  41 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1716 steps  42 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1308 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1717 steps  43 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1718 steps  44 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1719 steps  45 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1720 steps  46 reward -1 next state  4 agent position  (5, 1)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1721 steps  47 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1722 steps  48 reward -1 next state  4 agent position  (5, 1)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1723 steps  49 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1724 steps  50 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1725 steps  51 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1726 steps  52 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1727 steps  53 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1728 steps  54 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1729 steps  55 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1730 steps  56 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1731 steps  57 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1308 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1732 steps  58 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1733 steps  59 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1734 steps  60 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1308 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1735 steps  61 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1736 steps  62 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1308 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1737 steps  63 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1738 steps  64 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1739 steps  65 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1740 steps  66 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1741 steps  67 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1308 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1742 steps  68 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1743 steps  69 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1744 steps  70 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1745 steps  71 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1746 steps  72 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1747 steps  73 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1308 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1748 steps  74 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1308 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1257 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1749 steps  75 reward -1 next state  4 agent position  (6, 2)
max steps reached
total rewards -41
epsilon  0.29363974889158817
epsilon  0.29363974889158817
epsilon  0.29363974889158817
Episode number:  26
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.29363974889158817 agent memory len 1309 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.29363974889158817 agent memory len 1258 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.29363974889158817 agent memory len 1750 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1310 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.29363974889158817 agent memory len 1259 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.29363974889158817 agent memory len 1751 steps  2 reward -2 next state  2 agent position  (9, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.29363974889158817 agent memory len 1311 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.29363974889158817 agent memory len 1260 steps  3 reward -1 next state  0 agent position  (3, 9)
agent epsilon  0.29363974889158817 agent memory len 1752 steps  3 reward -1 next state  3 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'U']
agent epsilon  0.29363974889158817 agent memory len 1312 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.29363974889158817 agent memory len 1261 steps  4 reward -2 next state  1 agent position  (3, 9)
agent epsilon  0.29363974889158817 agent memory len 1753 steps  4 reward -1 next state  3 agent position  (7, 1)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.29363974889158817 agent memory len 1313 steps  5 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.29363974889158817 agent memory len 1262 steps  5 reward -1 next state  1 agent position  (4, 9)
agent epsilon  0.29363974889158817 agent memory len 1754 steps  5 reward -1 next state  4 agent position  (7, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'L']
agent epsilon  0.29363974889158817 agent memory len 1314 steps  6 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.29363974889158817 agent memory len 1263 steps  6 reward -1 next state  2 agent position  (4, 8)
agent epsilon  0.29363974889158817 agent memory len 1755 steps  6 reward -1 next state  4 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.29363974889158817 agent memory len 1315 steps  7 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.29363974889158817 agent memory len 1264 steps  7 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.29363974889158817 agent memory len 1756 steps  7 reward -1 next state  4 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.29363974889158817 agent memory len 1316 steps  8 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.29363974889158817 agent memory len 1265 steps  8 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.29363974889158817 agent memory len 1757 steps  8 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['U', 'L', 'R']
agent epsilon  0.29363974889158817 agent memory len 1317 steps  9 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  9 reward 10 next state  3 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1758 steps  9 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.29363974889158817 agent memory len 1318 steps  10 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  10 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1759 steps  10 reward 0 next state  4 agent position  (5, 3)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1319 steps  11 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  11 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1760 steps  11 reward 0 next state  4 agent position  (5, 3)
 is_terminal [False, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1320 steps  12 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  12 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1761 steps  12 reward 10 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1321 steps  13 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1761 steps  13 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
landmark captured 0
No distances were found
No distances were found
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1322 steps  14 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.29363974889158817 agent memory len 1266 steps  14 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.29363974889158817 agent memory len 1761 steps  14 reward -1 next state  4 agent position  (5, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards 1
epsilon  0.27980647510367246
epsilon  0.27980647510367246
epsilon  0.27980647510367246
Episode number:  27
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.27980647510367246 agent memory len 1323 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.27980647510367246 agent memory len 1267 steps  1 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.27980647510367246 agent memory len 1762 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.27980647510367246 agent memory len 1324 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.27980647510367246 agent memory len 1268 steps  2 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.27980647510367246 agent memory len 1763 steps  2 reward -1 next state  0 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'U']
agent epsilon  0.27980647510367246 agent memory len 1325 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.27980647510367246 agent memory len 1269 steps  3 reward -1 next state  1 agent position  (0, 7)
agent epsilon  0.27980647510367246 agent memory len 1764 steps  3 reward -1 next state  0 agent position  (6, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.27980647510367246 agent memory len 1326 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.27980647510367246 agent memory len 1270 steps  4 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.27980647510367246 agent memory len 1765 steps  4 reward -2 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1327 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.27980647510367246 agent memory len 1271 steps  5 reward -1 next state  2 agent position  (1, 7)
agent epsilon  0.27980647510367246 agent memory len 1766 steps  5 reward -1 next state  1 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'R']
agent epsilon  0.27980647510367246 agent memory len 1328 steps  6 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.27980647510367246 agent memory len 1272 steps  6 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.27980647510367246 agent memory len 1767 steps  6 reward -1 next state  2 agent position  (6, 2)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.27980647510367246 agent memory len 1329 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.27980647510367246 agent memory len 1273 steps  7 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.27980647510367246 agent memory len 1768 steps  7 reward -1 next state  2 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.27980647510367246 agent memory len 1330 steps  8 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.27980647510367246 agent memory len 1274 steps  8 reward -1 next state  3 agent position  (2, 5)
agent epsilon  0.27980647510367246 agent memory len 1769 steps  8 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.27980647510367246 agent memory len 1331 steps  9 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.27980647510367246 agent memory len 1275 steps  9 reward 0 next state  2 agent position  (3, 5)
agent epsilon  0.27980647510367246 agent memory len 1770 steps  9 reward -1 next state  3 agent position  (7, 2)
 is_terminal [False, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'D', 'U']
agent epsilon  0.27980647510367246 agent memory len 1332 steps  10 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  10 reward 10 next state  3 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1771 steps  10 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, True, False]
landmark captured 2
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  11 reward 10 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1772 steps  11 reward -1 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  12 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1773 steps  12 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  13 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1774 steps  13 reward -1 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  14 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1775 steps  14 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  15 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1776 steps  15 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, True, False]
landmark captured 0
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'U']
agent epsilon  0.27980647510367246 agent memory len 1333 steps  16 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1276 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1777 steps  16 reward 10 next state  4 agent position  (4, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards 1
epsilon  0.2666478580394326
epsilon  0.2666478580394326
epsilon  0.2666478580394326
Episode number:  28
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'U']
agent epsilon  0.2666478580394326 agent memory len 1334 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.2666478580394326 agent memory len 1277 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.2666478580394326 agent memory len 1778 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.2666478580394326 agent memory len 1335 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.2666478580394326 agent memory len 1278 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.2666478580394326 agent memory len 1779 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'U']
agent epsilon  0.2666478580394326 agent memory len 1336 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.2666478580394326 agent memory len 1279 steps  3 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.2666478580394326 agent memory len 1780 steps  3 reward -1 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'U']
agent epsilon  0.2666478580394326 agent memory len 1337 steps  4 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.2666478580394326 agent memory len 1280 steps  4 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.2666478580394326 agent memory len 1781 steps  4 reward -1 next state  2 agent position  (5, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'U']
agent epsilon  0.2666478580394326 agent memory len 1338 steps  5 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.2666478580394326 agent memory len 1281 steps  5 reward -2 next state  1 agent position  (2, 9)
agent epsilon  0.2666478580394326 agent memory len 1782 steps  5 reward -1 next state  2 agent position  (4, 0)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.2666478580394326 agent memory len 1339 steps  6 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.2666478580394326 agent memory len 1282 steps  6 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.2666478580394326 agent memory len 1783 steps  6 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.2666478580394326 agent memory len 1340 steps  7 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.2666478580394326 agent memory len 1283 steps  7 reward -1 next state  3 agent position  (2, 7)
agent epsilon  0.2666478580394326 agent memory len 1784 steps  7 reward -1 next state  2 agent position  (4, 2)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.2666478580394326 agent memory len 1341 steps  8 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.2666478580394326 agent memory len 1284 steps  8 reward -1 next state  3 agent position  (2, 8)
agent epsilon  0.2666478580394326 agent memory len 1785 steps  8 reward -1 next state  2 agent position  (5, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.2666478580394326 agent memory len 1342 steps  9 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.2666478580394326 agent memory len 1285 steps  9 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.2666478580394326 agent memory len 1786 steps  9 reward -1 next state  2 agent position  (4, 2)
 is_terminal [False, False, False]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'R', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  10 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1286 steps  10 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.2666478580394326 agent memory len 1787 steps  10 reward -1 next state  2 agent position  (5, 2)
 is_terminal [True, False, False]
actions ['S', 'L', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1287 steps  11 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.2666478580394326 agent memory len 1788 steps  11 reward -1 next state  2 agent position  (6, 2)
 is_terminal [True, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1288 steps  12 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.2666478580394326 agent memory len 1789 steps  12 reward -1 next state  3 agent position  (5, 2)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1289 steps  13 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.2666478580394326 agent memory len 1790 steps  13 reward -1 next state  3 agent position  (4, 2)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1290 steps  14 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.2666478580394326 agent memory len 1791 steps  14 reward 0 next state  3 agent position  (4, 3)
 is_terminal [True, False, False]
actions ['S', 'L', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1291 steps  15 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.2666478580394326 agent memory len 1792 steps  15 reward 0 next state  3 agent position  (5, 3)
 is_terminal [True, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  16 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1793 steps  16 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1794 steps  17 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1795 steps  18 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1796 steps  19 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1797 steps  20 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1798 steps  21 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1799 steps  22 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1800 steps  23 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1801 steps  24 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1802 steps  25 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1803 steps  26 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1804 steps  27 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1805 steps  28 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1806 steps  29 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1807 steps  30 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1808 steps  31 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1809 steps  32 reward -1 next state  4 agent position  (8, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1810 steps  33 reward -1 next state  4 agent position  (7, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1811 steps  34 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1812 steps  35 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1813 steps  36 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1814 steps  37 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1815 steps  38 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1816 steps  39 reward -1 next state  4 agent position  (6, 1)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1817 steps  40 reward -1 next state  4 agent position  (5, 1)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1818 steps  41 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1819 steps  42 reward -1 next state  4 agent position  (6, 2)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1820 steps  43 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1821 steps  44 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1822 steps  45 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1823 steps  46 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1824 steps  47 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1825 steps  48 reward 0 next state  4 agent position  (5, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1826 steps  49 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1827 steps  50 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1828 steps  51 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1829 steps  52 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1830 steps  53 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1831 steps  54 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1832 steps  55 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1833 steps  56 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1834 steps  57 reward -1 next state  4 agent position  (8, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1835 steps  58 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1836 steps  59 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1837 steps  60 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1838 steps  61 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1839 steps  62 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1840 steps  63 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1841 steps  64 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1842 steps  65 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1843 steps  66 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1844 steps  67 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1845 steps  68 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1846 steps  69 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1847 steps  70 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1848 steps  71 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1849 steps  72 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1850 steps  73 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1851 steps  74 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1343 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2666478580394326 agent memory len 1292 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1852 steps  75 reward -1 next state  4 agent position  (7, 3)
max steps reached
total rewards -52
epsilon  0.2541309943021904
epsilon  0.2541309943021904
epsilon  0.2541309943021904
Episode number:  29
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.2541309943021904 agent memory len 1344 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.2541309943021904 agent memory len 1293 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.2541309943021904 agent memory len 1853 steps  1 reward -2 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.2541309943021904 agent memory len 1345 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.2541309943021904 agent memory len 1294 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.2541309943021904 agent memory len 1854 steps  2 reward -1 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.2541309943021904 agent memory len 1346 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.2541309943021904 agent memory len 1295 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.2541309943021904 agent memory len 1855 steps  3 reward -2 next state  3 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1347 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.2541309943021904 agent memory len 1296 steps  4 reward -1 next state  2 agent position  (3, 9)
agent epsilon  0.2541309943021904 agent memory len 1856 steps  4 reward -1 next state  3 agent position  (7, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.2541309943021904 agent memory len 1348 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.2541309943021904 agent memory len 1297 steps  5 reward -1 next state  2 agent position  (4, 9)
agent epsilon  0.2541309943021904 agent memory len 1857 steps  5 reward -1 next state  4 agent position  (6, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.2541309943021904 agent memory len 1349 steps  6 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1298 steps  6 reward -1 next state  2 agent position  (4, 8)
agent epsilon  0.2541309943021904 agent memory len 1858 steps  6 reward -1 next state  4 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.2541309943021904 agent memory len 1350 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1299 steps  7 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.2541309943021904 agent memory len 1859 steps  7 reward -1 next state  4 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.2541309943021904 agent memory len 1351 steps  8 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.2541309943021904 agent memory len 1300 steps  8 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.2541309943021904 agent memory len 1860 steps  8 reward -1 next state  4 agent position  (5, 1)
 is_terminal [False, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['D', 'L', 'S']
agent epsilon  0.2541309943021904 agent memory len 1352 steps  9 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  9 reward 10 next state  3 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1861 steps  9 reward -1 next state  4 agent position  (5, 1)
 is_terminal [False, True, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  10 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  10 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1862 steps  10 reward -1 next state  4 agent position  (5, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1863 steps  11 reward -1 next state  4 agent position  (4, 2)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1864 steps  12 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1865 steps  13 reward -1 next state  4 agent position  (3, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1866 steps  14 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1867 steps  15 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1868 steps  16 reward -1 next state  4 agent position  (2, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1869 steps  17 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1870 steps  18 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1871 steps  19 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1872 steps  20 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1873 steps  21 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1874 steps  22 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1875 steps  23 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1876 steps  24 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1877 steps  25 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1878 steps  26 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1879 steps  27 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1880 steps  28 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1881 steps  29 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1882 steps  30 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1883 steps  31 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1884 steps  32 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1885 steps  33 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1886 steps  34 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1887 steps  35 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1888 steps  36 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1889 steps  37 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1890 steps  38 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1891 steps  39 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1892 steps  40 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1893 steps  41 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1894 steps  42 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1895 steps  43 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1896 steps  44 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1897 steps  45 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1898 steps  46 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1899 steps  47 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1900 steps  48 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1901 steps  49 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1902 steps  50 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1903 steps  51 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1904 steps  52 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1905 steps  53 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1906 steps  54 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1907 steps  55 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1908 steps  56 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1909 steps  57 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1910 steps  58 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1911 steps  59 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1912 steps  60 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1913 steps  61 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1914 steps  62 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1915 steps  63 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1916 steps  64 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1917 steps  65 reward -1 next state  4 agent position  (3, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1918 steps  66 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1919 steps  67 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1920 steps  68 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1921 steps  69 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1922 steps  70 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1923 steps  71 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1924 steps  72 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1925 steps  73 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1926 steps  74 reward -1 next state  4 agent position  (3, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1353 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1301 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1927 steps  75 reward -1 next state  4 agent position  (3, 8)
max steps reached
total rewards -45
epsilon  0.24222458521285967
epsilon  0.24222458521285967
epsilon  0.24222458521285967
Episode number:  30
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.24222458521285967 agent memory len 1354 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1302 steps  1 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.24222458521285967 agent memory len 1928 steps  1 reward -1 next state  0 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.24222458521285967 agent memory len 1355 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.24222458521285967 agent memory len 1303 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.24222458521285967 agent memory len 1929 steps  2 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.24222458521285967 agent memory len 1356 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.24222458521285967 agent memory len 1304 steps  3 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.24222458521285967 agent memory len 1930 steps  3 reward -1 next state  1 agent position  (6, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.24222458521285967 agent memory len 1357 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.24222458521285967 agent memory len 1305 steps  4 reward -1 next state  1 agent position  (0, 8)
agent epsilon  0.24222458521285967 agent memory len 1931 steps  4 reward -1 next state  0 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'R']
agent epsilon  0.24222458521285967 agent memory len 1358 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.24222458521285967 agent memory len 1306 steps  5 reward -1 next state  2 agent position  (1, 8)
agent epsilon  0.24222458521285967 agent memory len 1932 steps  5 reward -1 next state  1 agent position  (6, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1359 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.24222458521285967 agent memory len 1307 steps  6 reward -1 next state  3 agent position  (1, 9)
agent epsilon  0.24222458521285967 agent memory len 1933 steps  6 reward -1 next state  1 agent position  (6, 2)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.24222458521285967 agent memory len 1360 steps  7 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1308 steps  7 reward -1 next state  4 agent position  (2, 9)
agent epsilon  0.24222458521285967 agent memory len 1934 steps  7 reward 0 next state  2 agent position  (6, 3)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'L', 'R']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1309 steps  8 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.24222458521285967 agent memory len 1935 steps  8 reward 0 next state  2 agent position  (6, 4)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1310 steps  9 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.24222458521285967 agent memory len 1936 steps  9 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1311 steps  10 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.24222458521285967 agent memory len 1937 steps  10 reward 0 next state  2 agent position  (6, 4)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1312 steps  11 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.24222458521285967 agent memory len 1938 steps  11 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1313 steps  12 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.24222458521285967 agent memory len 1939 steps  12 reward 0 next state  2 agent position  (6, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1314 steps  13 reward -1 next state  4 agent position  (2, 9)
agent epsilon  0.24222458521285967 agent memory len 1940 steps  13 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'L']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1315 steps  14 reward -2 next state  4 agent position  (2, 9)
agent epsilon  0.24222458521285967 agent memory len 1941 steps  14 reward 0 next state  2 agent position  (6, 4)
 is_terminal [True, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1316 steps  15 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.24222458521285967 agent memory len 1942 steps  15 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1317 steps  16 reward -1 next state  4 agent position  (2, 9)
agent epsilon  0.24222458521285967 agent memory len 1943 steps  16 reward 0 next state  2 agent position  (6, 5)
 is_terminal [True, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1318 steps  17 reward -1 next state  4 agent position  (2, 8)
agent epsilon  0.24222458521285967 agent memory len 1944 steps  17 reward 0 next state  2 agent position  (6, 4)
 is_terminal [True, False, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'L', 'U']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1319 steps  18 reward -1 next state  4 agent position  (2, 7)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  18 reward 10 next state  2 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1320 steps  19 reward -1 next state  4 agent position  (3, 7)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  19 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1321 steps  20 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  20 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1322 steps  21 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  21 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1323 steps  22 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  22 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1324 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  23 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1325 steps  24 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  24 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1326 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  25 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1327 steps  26 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  26 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1328 steps  27 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  27 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1329 steps  28 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  28 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1330 steps  29 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  29 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1331 steps  30 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  30 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1332 steps  31 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  31 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1333 steps  32 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  32 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1334 steps  33 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  33 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1335 steps  34 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  34 reward 0 next state  7 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1336 steps  35 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  35 reward 0 next state  7 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1337 steps  36 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  36 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1338 steps  37 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  37 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1339 steps  38 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  38 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1340 steps  39 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  39 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1341 steps  40 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  40 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1342 steps  41 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  41 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1343 steps  42 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  42 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1344 steps  43 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  43 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1345 steps  44 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  44 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1346 steps  45 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  45 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1347 steps  46 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  46 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1348 steps  47 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  47 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1349 steps  48 reward -2 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  48 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1350 steps  49 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  49 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1351 steps  50 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  50 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1352 steps  51 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  51 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1353 steps  52 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  52 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1354 steps  53 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  53 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1355 steps  54 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  54 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1356 steps  55 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  55 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1357 steps  56 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  56 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1358 steps  57 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  57 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1359 steps  58 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  58 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1360 steps  59 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  59 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1361 steps  60 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  60 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1362 steps  61 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  61 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1363 steps  62 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  62 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1364 steps  63 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  63 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1365 steps  64 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  64 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1366 steps  65 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  65 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1367 steps  66 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  66 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1368 steps  67 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  67 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1369 steps  68 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  68 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1370 steps  69 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  69 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1371 steps  70 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  70 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1372 steps  71 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  71 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1373 steps  72 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  72 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1374 steps  73 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  73 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1375 steps  74 reward -1 next state  4 agent position  (9, 1)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  74 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1361 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1376 steps  75 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.24222458521285967 agent memory len 1945 steps  75 reward 0 next state  9 agent position  (5, 4)
max steps reached
total rewards -60
epsilon  0.23089885854694553
epsilon  0.23089885854694553
epsilon  0.23089885854694553
Episode number:  31
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.23089885854694553 agent memory len 1362 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.23089885854694553 agent memory len 1377 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.23089885854694553 agent memory len 1946 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'U']
agent epsilon  0.23089885854694553 agent memory len 1363 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.23089885854694553 agent memory len 1378 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.23089885854694553 agent memory len 1947 steps  2 reward -1 next state  1 agent position  (7, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.23089885854694553 agent memory len 1364 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.23089885854694553 agent memory len 1379 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.23089885854694553 agent memory len 1948 steps  3 reward -1 next state  2 agent position  (6, 0)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.23089885854694553 agent memory len 1365 steps  4 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.23089885854694553 agent memory len 1380 steps  4 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.23089885854694553 agent memory len 1949 steps  4 reward -1 next state  2 agent position  (5, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.23089885854694553 agent memory len 1366 steps  5 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.23089885854694553 agent memory len 1381 steps  5 reward -1 next state  1 agent position  (2, 6)
agent epsilon  0.23089885854694553 agent memory len 1950 steps  5 reward -1 next state  2 agent position  (4, 0)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.23089885854694553 agent memory len 1367 steps  6 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.23089885854694553 agent memory len 1382 steps  6 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.23089885854694553 agent memory len 1951 steps  6 reward -1 next state  2 agent position  (3, 0)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.23089885854694553 agent memory len 1368 steps  7 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.23089885854694553 agent memory len 1383 steps  7 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.23089885854694553 agent memory len 1952 steps  7 reward -1 next state  2 agent position  (3, 1)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.23089885854694553 agent memory len 1369 steps  8 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.23089885854694553 agent memory len 1384 steps  8 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.23089885854694553 agent memory len 1953 steps  8 reward -1 next state  3 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.23089885854694553 agent memory len 1370 steps  9 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.23089885854694553 agent memory len 1385 steps  9 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.23089885854694553 agent memory len 1954 steps  9 reward 0 next state  3 agent position  (3, 3)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'R']
agent epsilon  0.23089885854694553 agent memory len 1371 steps  10 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.23089885854694553 agent memory len 1386 steps  10 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.23089885854694553 agent memory len 1955 steps  10 reward 0 next state  3 agent position  (3, 4)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'D', 'R']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  11 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1387 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1956 steps  11 reward 0 next state  4 agent position  (3, 5)
 is_terminal [True, False, False]
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['S', 'L', 'D']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1388 steps  12 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  12 reward 10 next state  4 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1389 steps  13 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  13 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1390 steps  14 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  14 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1391 steps  15 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  15 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1392 steps  16 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  16 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1393 steps  17 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  17 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1394 steps  18 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  18 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1395 steps  19 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  19 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1396 steps  20 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  20 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1397 steps  21 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  21 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1398 steps  22 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  22 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1399 steps  23 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  23 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1400 steps  24 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  24 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1401 steps  25 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  25 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1402 steps  26 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  26 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1403 steps  27 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  27 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1404 steps  28 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  28 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1405 steps  29 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  29 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1406 steps  30 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  30 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1407 steps  31 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  31 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1408 steps  32 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  32 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1409 steps  33 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  33 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1410 steps  34 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  34 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1411 steps  35 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  35 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1412 steps  36 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  36 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1413 steps  37 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  37 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1414 steps  38 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  38 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1415 steps  39 reward 0 next state  4 agent position  (6, 3)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  39 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1416 steps  40 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  40 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1417 steps  41 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  41 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1418 steps  42 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  42 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1419 steps  43 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  43 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1420 steps  44 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  44 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1421 steps  45 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  45 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1422 steps  46 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  46 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1423 steps  47 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  47 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1424 steps  48 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  48 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1425 steps  49 reward 0 next state  4 agent position  (6, 3)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  49 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1426 steps  50 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  50 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1427 steps  51 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  51 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1428 steps  52 reward -1 next state  4 agent position  (6, 0)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  52 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1429 steps  53 reward -1 next state  4 agent position  (7, 0)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  53 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1430 steps  54 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  54 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1431 steps  55 reward -1 next state  4 agent position  (8, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  55 reward 0 next state  8 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1432 steps  56 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  56 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1433 steps  57 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  57 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1434 steps  58 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  58 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1435 steps  59 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  59 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1436 steps  60 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  60 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1437 steps  61 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  61 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1438 steps  62 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  62 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1439 steps  63 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  63 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1440 steps  64 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  64 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1441 steps  65 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  65 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1442 steps  66 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  66 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1443 steps  67 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  67 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1444 steps  68 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  68 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1445 steps  69 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  69 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1446 steps  70 reward -1 next state  4 agent position  (6, 0)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  70 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1447 steps  71 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  71 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1448 steps  72 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  72 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1449 steps  73 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  73 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1450 steps  74 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  74 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.23089885854694553 agent memory len 1372 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1451 steps  75 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.23089885854694553 agent memory len 1957 steps  75 reward 0 next state  6 agent position  (4, 5)
max steps reached
total rewards -62
epsilon  0.22012549408847562
epsilon  0.22012549408847562
epsilon  0.22012549408847562
Episode number:  32
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.22012549408847562 agent memory len 1373 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.22012549408847562 agent memory len 1452 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.22012549408847562 agent memory len 1958 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.22012549408847562 agent memory len 1374 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1453 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.22012549408847562 agent memory len 1959 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.22012549408847562 agent memory len 1375 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.22012549408847562 agent memory len 1454 steps  3 reward -1 next state  0 agent position  (3, 9)
agent epsilon  0.22012549408847562 agent memory len 1960 steps  3 reward -1 next state  3 agent position  (6, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.22012549408847562 agent memory len 1376 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1455 steps  4 reward -1 next state  0 agent position  (4, 9)
agent epsilon  0.22012549408847562 agent memory len 1961 steps  4 reward -1 next state  4 agent position  (6, 1)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.22012549408847562 agent memory len 1377 steps  5 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.22012549408847562 agent memory len 1456 steps  5 reward -1 next state  1 agent position  (5, 9)
agent epsilon  0.22012549408847562 agent memory len 1962 steps  5 reward -1 next state  5 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.22012549408847562 agent memory len 1378 steps  6 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.22012549408847562 agent memory len 1457 steps  6 reward -1 next state  1 agent position  (5, 8)
agent epsilon  0.22012549408847562 agent memory len 1963 steps  6 reward -1 next state  5 agent position  (5, 2)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.22012549408847562 agent memory len 1379 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.22012549408847562 agent memory len 1458 steps  7 reward -1 next state  1 agent position  (5, 7)
agent epsilon  0.22012549408847562 agent memory len 1964 steps  7 reward 0 next state  5 agent position  (5, 3)
 is_terminal [False, False, False]
random action 1
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'U', 'R']
agent epsilon  0.22012549408847562 agent memory len 1380 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.22012549408847562 agent memory len 1459 steps  8 reward -1 next state  1 agent position  (4, 7)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  8 reward 10 next state  4 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['D', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1381 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.22012549408847562 agent memory len 1460 steps  9 reward -1 next state  1 agent position  (5, 7)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  9 reward 0 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1382 steps  10 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.22012549408847562 agent memory len 1461 steps  10 reward -1 next state  0 agent position  (6, 7)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  10 reward 0 next state  6 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1383 steps  11 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.22012549408847562 agent memory len 1462 steps  11 reward -1 next state  1 agent position  (7, 7)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  11 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1384 steps  12 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.22012549408847562 agent memory len 1463 steps  12 reward -1 next state  1 agent position  (7, 6)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  12 reward 0 next state  7 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1385 steps  13 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.22012549408847562 agent memory len 1464 steps  13 reward -1 next state  2 agent position  (8, 6)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  13 reward 0 next state  8 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1386 steps  14 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.22012549408847562 agent memory len 1465 steps  14 reward -1 next state  1 agent position  (8, 5)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  14 reward 0 next state  8 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 1387 steps  15 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.22012549408847562 agent memory len 1466 steps  15 reward -1 next state  2 agent position  (8, 5)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  15 reward 0 next state  8 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 1388 steps  16 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1467 steps  16 reward -1 next state  3 agent position  (8, 5)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  16 reward 0 next state  8 agent position  (5, 4)
 is_terminal [False, False, True]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  17 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1468 steps  17 reward -1 next state  4 agent position  (8, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  17 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1469 steps  18 reward -1 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  18 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1470 steps  19 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  19 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1471 steps  20 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  20 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1472 steps  21 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  21 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1473 steps  22 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  22 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1474 steps  23 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  23 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1475 steps  24 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  24 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1476 steps  25 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  25 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1477 steps  26 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  26 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1478 steps  27 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  27 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1479 steps  28 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  28 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1480 steps  29 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  29 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1481 steps  30 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  30 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1482 steps  31 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  31 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1483 steps  32 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  32 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1484 steps  33 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  33 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1485 steps  34 reward -1 next state  4 agent position  (8, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  34 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1486 steps  35 reward -1 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  35 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1487 steps  36 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  36 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1488 steps  37 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  37 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1489 steps  38 reward -2 next state  4 agent position  (9, 4)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  38 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1490 steps  39 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  39 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1491 steps  40 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  40 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1492 steps  41 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  41 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1493 steps  42 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  42 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1494 steps  43 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  43 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1495 steps  44 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  44 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1496 steps  45 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  45 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1497 steps  46 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  46 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1498 steps  47 reward -2 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  47 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1499 steps  48 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  48 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1500 steps  49 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  49 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1501 steps  50 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  50 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1502 steps  51 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  51 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1503 steps  52 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  52 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1504 steps  53 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  53 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1505 steps  54 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  54 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1506 steps  55 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  55 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1507 steps  56 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  56 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1508 steps  57 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  57 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1509 steps  58 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  58 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1510 steps  59 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  59 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1511 steps  60 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  60 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1512 steps  61 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  61 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1513 steps  62 reward -1 next state  4 agent position  (8, 2)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  62 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1514 steps  63 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  63 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1515 steps  64 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  64 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1516 steps  65 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  65 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1517 steps  66 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  66 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1518 steps  67 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  67 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1519 steps  68 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  68 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1520 steps  69 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  69 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1521 steps  70 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  70 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1522 steps  71 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  71 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1523 steps  72 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  72 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1524 steps  73 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  73 reward 0 next state  9 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1525 steps  74 reward -1 next state  4 agent position  (8, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  74 reward 0 next state  8 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 1389 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1526 steps  75 reward -1 next state  4 agent position  (9, 3)
agent epsilon  0.22012549408847562 agent memory len 1965 steps  75 reward 0 next state  9 agent position  (5, 4)
max steps reached
total rewards -95
epsilon  0.20987755281470882
epsilon  0.20987755281470882
epsilon  0.20987755281470882
Episode number:  33
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.20987755281470882 agent memory len 1390 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.20987755281470882 agent memory len 1527 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.20987755281470882 agent memory len 1966 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.20987755281470882 agent memory len 1391 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.20987755281470882 agent memory len 1528 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.20987755281470882 agent memory len 1967 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.20987755281470882 agent memory len 1392 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.20987755281470882 agent memory len 1529 steps  3 reward -1 next state  1 agent position  (2, 8)
agent epsilon  0.20987755281470882 agent memory len 1968 steps  3 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'R']
agent epsilon  0.20987755281470882 agent memory len 1393 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.20987755281470882 agent memory len 1530 steps  4 reward -1 next state  2 agent position  (3, 8)
agent epsilon  0.20987755281470882 agent memory len 1969 steps  4 reward -1 next state  3 agent position  (7, 2)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.20987755281470882 agent memory len 1394 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.20987755281470882 agent memory len 1531 steps  5 reward -1 next state  2 agent position  (3, 7)
agent epsilon  0.20987755281470882 agent memory len 1970 steps  5 reward -1 next state  3 agent position  (6, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.20987755281470882 agent memory len 1395 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.20987755281470882 agent memory len 1532 steps  6 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.20987755281470882 agent memory len 1971 steps  6 reward 0 next state  3 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 1396 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.20987755281470882 agent memory len 1533 steps  7 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.20987755281470882 agent memory len 1972 steps  7 reward -1 next state  4 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'D']
agent epsilon  0.20987755281470882 agent memory len 1397 steps  8 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.20987755281470882 agent memory len 1534 steps  8 reward 10 next state  3 agent position  (4, 5)
agent epsilon  0.20987755281470882 agent memory len 1973 steps  8 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.20987755281470882 agent memory len 1398 steps  9 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.20987755281470882 agent memory len 1534 steps  9 reward 0 next state  3 agent position  (4, 5)
agent epsilon  0.20987755281470882 agent memory len 1974 steps  9 reward -1 next state  4 agent position  (8, 4)
 is_terminal [False, True, False]
random action 0
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'U']
agent epsilon  0.20987755281470882 agent memory len 1399 steps  10 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.20987755281470882 agent memory len 1534 steps  10 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20987755281470882 agent memory len 1975 steps  10 reward -1 next state  4 agent position  (7, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.20987755281470882 agent memory len 1399 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20987755281470882 agent memory len 1534 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20987755281470882 agent memory len 1976 steps  11 reward 0 next state  4 agent position  (6, 4)
 is_terminal [True, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'U']
agent epsilon  0.20987755281470882 agent memory len 1399 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20987755281470882 agent memory len 1534 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20987755281470882 agent memory len 1977 steps  12 reward 10 next state  4 agent position  (5, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards 11
epsilon  0.20012940953454655
epsilon  0.20012940953454655
epsilon  0.20012940953454655
Episode number:  34
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.20012940953454655 agent memory len 1400 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.20012940953454655 agent memory len 1535 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.20012940953454655 agent memory len 1978 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.20012940953454655 agent memory len 1401 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.20012940953454655 agent memory len 1536 steps  2 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.20012940953454655 agent memory len 1979 steps  2 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.20012940953454655 agent memory len 1402 steps  3 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.20012940953454655 agent memory len 1537 steps  3 reward -1 next state  2 agent position  (3, 9)
agent epsilon  0.20012940953454655 agent memory len 1980 steps  3 reward -1 next state  3 agent position  (9, 2)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.20012940953454655 agent memory len 1403 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.20012940953454655 agent memory len 1538 steps  4 reward -1 next state  2 agent position  (4, 9)
agent epsilon  0.20012940953454655 agent memory len 1981 steps  4 reward -1 next state  4 agent position  (9, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.20012940953454655 agent memory len 1404 steps  5 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.20012940953454655 agent memory len 1539 steps  5 reward -1 next state  3 agent position  (4, 8)
agent epsilon  0.20012940953454655 agent memory len 1982 steps  5 reward -1 next state  4 agent position  (9, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.20012940953454655 agent memory len 1405 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.20012940953454655 agent memory len 1540 steps  6 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.20012940953454655 agent memory len 1983 steps  6 reward -1 next state  4 agent position  (9, 5)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.20012940953454655 agent memory len 1406 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.20012940953454655 agent memory len 1541 steps  7 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.20012940953454655 agent memory len 1984 steps  7 reward -1 next state  4 agent position  (9, 6)
 is_terminal [False, False, False]
landmark captured 0
landmark captured 1
agent reached landmark-------------------------------- 0
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  8 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1985 steps  8 reward -1 next state  4 agent position  (9, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  9 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1986 steps  9 reward -1 next state  4 agent position  (9, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  10 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1987 steps  10 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1988 steps  11 reward -1 next state  4 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1989 steps  12 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1990 steps  13 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1991 steps  14 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1992 steps  15 reward -1 next state  4 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1993 steps  16 reward -2 next state  4 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1994 steps  17 reward -1 next state  4 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1995 steps  18 reward -1 next state  4 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1996 steps  19 reward -1 next state  4 agent position  (6, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1997 steps  20 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1998 steps  21 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 1999 steps  22 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2000 steps  23 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2001 steps  24 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2002 steps  25 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2003 steps  26 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2004 steps  27 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2005 steps  28 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2006 steps  29 reward -1 next state  4 agent position  (7, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2007 steps  30 reward -1 next state  4 agent position  (7, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2008 steps  31 reward -1 next state  4 agent position  (6, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2009 steps  32 reward -1 next state  4 agent position  (5, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2010 steps  33 reward -1 next state  4 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2011 steps  34 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2012 steps  35 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2013 steps  36 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2014 steps  37 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2015 steps  38 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2016 steps  39 reward -1 next state  4 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2017 steps  40 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2018 steps  41 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2019 steps  42 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2020 steps  43 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2021 steps  44 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2022 steps  45 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2023 steps  46 reward -1 next state  4 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2024 steps  47 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2025 steps  48 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2026 steps  49 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2027 steps  50 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2028 steps  51 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2029 steps  52 reward -1 next state  4 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2030 steps  53 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2031 steps  54 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2032 steps  55 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2033 steps  56 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2034 steps  57 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2035 steps  58 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2036 steps  59 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2037 steps  60 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2038 steps  61 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2039 steps  62 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2040 steps  63 reward -1 next state  4 agent position  (5, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2041 steps  64 reward -2 next state  4 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2042 steps  65 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2043 steps  66 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2044 steps  67 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2045 steps  68 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2046 steps  69 reward -1 next state  4 agent position  (3, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2047 steps  70 reward -1 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2048 steps  71 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2049 steps  72 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2050 steps  73 reward -2 next state  4 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2051 steps  74 reward -1 next state  4 agent position  (3, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.20012940953454655 agent memory len 1407 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.20012940953454655 agent memory len 1542 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.20012940953454655 agent memory len 2052 steps  75 reward -1 next state  4 agent position  (4, 9)
max steps reached
total rewards -95
epsilon  0.19085668881220727
epsilon  0.19085668881220727
epsilon  0.19085668881220727
Episode number:  35
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.19085668881220727 agent memory len 1408 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.19085668881220727 agent memory len 1543 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.19085668881220727 agent memory len 2053 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.19085668881220727 agent memory len 1409 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.19085668881220727 agent memory len 1544 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.19085668881220727 agent memory len 2054 steps  2 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.19085668881220727 agent memory len 1410 steps  3 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.19085668881220727 agent memory len 1545 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.19085668881220727 agent memory len 2055 steps  3 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.19085668881220727 agent memory len 1411 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.19085668881220727 agent memory len 1546 steps  4 reward -1 next state  1 agent position  (2, 7)
agent epsilon  0.19085668881220727 agent memory len 2056 steps  4 reward -1 next state  2 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'U']
agent epsilon  0.19085668881220727 agent memory len 1412 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.19085668881220727 agent memory len 1547 steps  5 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.19085668881220727 agent memory len 2057 steps  5 reward -1 next state  2 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'R']
agent epsilon  0.19085668881220727 agent memory len 1413 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.19085668881220727 agent memory len 1548 steps  6 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.19085668881220727 agent memory len 2058 steps  6 reward -1 next state  3 agent position  (5, 2)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.19085668881220727 agent memory len 1414 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.19085668881220727 agent memory len 1549 steps  7 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.19085668881220727 agent memory len 2059 steps  7 reward -1 next state  3 agent position  (4, 2)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'L', 'R']
agent epsilon  0.19085668881220727 agent memory len 1415 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 1550 steps  8 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.19085668881220727 agent memory len 2060 steps  8 reward 0 next state  3 agent position  (4, 3)
 is_terminal [True, False, False]
actions ['S', 'D', 'R']
agent epsilon  0.19085668881220727 agent memory len 1415 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 1551 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 2061 steps  9 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.19085668881220727 agent memory len 1415 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 1552 steps  10 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.19085668881220727 agent memory len 2062 steps  10 reward 0 next state  4 agent position  (4, 3)
 is_terminal [True, False, False]
actions ['S', 'D', 'R']
agent epsilon  0.19085668881220727 agent memory len 1415 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 1553 steps  11 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.19085668881220727 agent memory len 2063 steps  11 reward 0 next state  5 agent position  (4, 4)
 is_terminal [True, False, False]
random action 2
landmark captured 2
landmark captured 1
agent reached landmark-------------------------------- 1
agent reached landmark-------------------------------- 2
actions ['S', 'R', 'R']
agent epsilon  0.19085668881220727 agent memory len 1415 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.19085668881220727 agent memory len 1554 steps  12 reward 10 next state  4 agent position  (5, 4)
agent epsilon  0.19085668881220727 agent memory len 2064 steps  12 reward 10 next state  5 agent position  (4, 5)
-----------------------------------all agents reached landmark--------------------------------
total rewards 13
epsilon  0.1820362040159407
epsilon  0.1820362040159407
epsilon  0.1820362040159407
Episode number:  36
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.1820362040159407 agent memory len 1416 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.1820362040159407 agent memory len 1555 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.1820362040159407 agent memory len 2065 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'R']
agent epsilon  0.1820362040159407 agent memory len 1417 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.1820362040159407 agent memory len 1556 steps  2 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.1820362040159407 agent memory len 2066 steps  2 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.1820362040159407 agent memory len 1418 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.1820362040159407 agent memory len 1557 steps  3 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.1820362040159407 agent memory len 2067 steps  3 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'U']
agent epsilon  0.1820362040159407 agent memory len 1419 steps  4 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.1820362040159407 agent memory len 1558 steps  4 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.1820362040159407 agent memory len 2068 steps  4 reward -1 next state  3 agent position  (7, 2)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.1820362040159407 agent memory len 1420 steps  5 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.1820362040159407 agent memory len 1559 steps  5 reward -1 next state  2 agent position  (4, 9)
agent epsilon  0.1820362040159407 agent memory len 2069 steps  5 reward -1 next state  4 agent position  (7, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  0.1820362040159407 agent memory len 1421 steps  6 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.1820362040159407 agent memory len 1560 steps  6 reward -2 next state  2 agent position  (4, 9)
agent epsilon  0.1820362040159407 agent memory len 2070 steps  6 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'U']
agent epsilon  0.1820362040159407 agent memory len 1422 steps  7 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.1820362040159407 agent memory len 1561 steps  7 reward -1 next state  3 agent position  (4, 8)
agent epsilon  0.1820362040159407 agent memory len 2071 steps  7 reward -1 next state  4 agent position  (7, 3)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.1820362040159407 agent memory len 1423 steps  8 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.1820362040159407 agent memory len 1562 steps  8 reward -1 next state  3 agent position  (4, 7)
agent epsilon  0.1820362040159407 agent memory len 2072 steps  8 reward -1 next state  4 agent position  (7, 4)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'U']
agent epsilon  0.1820362040159407 agent memory len 1424 steps  9 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.1820362040159407 agent memory len 1563 steps  9 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.1820362040159407 agent memory len 2073 steps  9 reward 0 next state  4 agent position  (6, 4)
 is_terminal [False, False, False]
landmark captured 0
landmark captured 1
agent reached landmark-------------------------------- 0
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'R']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  10 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  10 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2074 steps  10 reward 0 next state  4 agent position  (6, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2075 steps  11 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2076 steps  12 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2077 steps  13 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2078 steps  14 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2079 steps  15 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2080 steps  16 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2081 steps  17 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2082 steps  18 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2083 steps  19 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2084 steps  20 reward -1 next state  4 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2085 steps  21 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2086 steps  22 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2087 steps  23 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2088 steps  24 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2089 steps  25 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2090 steps  26 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2091 steps  27 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2092 steps  28 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2093 steps  29 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2094 steps  30 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2095 steps  31 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2096 steps  32 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2097 steps  33 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2098 steps  34 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2099 steps  35 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2100 steps  36 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2101 steps  37 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2102 steps  38 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2103 steps  39 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2104 steps  40 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2105 steps  41 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2106 steps  42 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2107 steps  43 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2108 steps  44 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2109 steps  45 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2110 steps  46 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2111 steps  47 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2112 steps  48 reward -1 next state  4 agent position  (3, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2113 steps  49 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2114 steps  50 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2115 steps  51 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2116 steps  52 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2117 steps  53 reward -1 next state  4 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2118 steps  54 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2119 steps  55 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2120 steps  56 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2121 steps  57 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2122 steps  58 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2123 steps  59 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2124 steps  60 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2125 steps  61 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2126 steps  62 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2127 steps  63 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2128 steps  64 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2129 steps  65 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2130 steps  66 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2131 steps  67 reward -1 next state  4 agent position  (5, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2132 steps  68 reward -1 next state  4 agent position  (4, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2133 steps  69 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2134 steps  70 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2135 steps  71 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2136 steps  72 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2137 steps  73 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2138 steps  74 reward -1 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1820362040159407 agent memory len 1425 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1820362040159407 agent memory len 1564 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1820362040159407 agent memory len 2139 steps  75 reward -1 next state  4 agent position  (4, 6)
max steps reached
total rewards -64
epsilon  0.17364589933937066
epsilon  0.17364589933937066
epsilon  0.17364589933937066
Episode number:  37
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.17364589933937066 agent memory len 1426 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.17364589933937066 agent memory len 1565 steps  1 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.17364589933937066 agent memory len 2140 steps  1 reward -1 next state  0 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.17364589933937066 agent memory len 1427 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.17364589933937066 agent memory len 1566 steps  2 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.17364589933937066 agent memory len 2141 steps  2 reward -1 next state  1 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.17364589933937066 agent memory len 1428 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.17364589933937066 agent memory len 1567 steps  3 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.17364589933937066 agent memory len 2142 steps  3 reward -1 next state  2 agent position  (9, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.17364589933937066 agent memory len 1429 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.17364589933937066 agent memory len 1568 steps  4 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.17364589933937066 agent memory len 2143 steps  4 reward -1 next state  2 agent position  (9, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.17364589933937066 agent memory len 1430 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.17364589933937066 agent memory len 1569 steps  5 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.17364589933937066 agent memory len 2144 steps  5 reward -1 next state  2 agent position  (9, 5)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'U']
agent epsilon  0.17364589933937066 agent memory len 1431 steps  6 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.17364589933937066 agent memory len 1570 steps  6 reward -1 next state  1 agent position  (2, 6)
agent epsilon  0.17364589933937066 agent memory len 2145 steps  6 reward -1 next state  2 agent position  (8, 5)
 is_terminal [False, False, False]
actions ['R', 'L', 'L']
agent epsilon  0.17364589933937066 agent memory len 1432 steps  7 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.17364589933937066 agent memory len 1571 steps  7 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.17364589933937066 agent memory len 2146 steps  7 reward -1 next state  2 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.17364589933937066 agent memory len 1433 steps  8 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.17364589933937066 agent memory len 1572 steps  8 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.17364589933937066 agent memory len 2147 steps  8 reward -1 next state  2 agent position  (8, 5)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'U']
agent epsilon  0.17364589933937066 agent memory len 1434 steps  9 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.17364589933937066 agent memory len 1573 steps  9 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.17364589933937066 agent memory len 2148 steps  9 reward -1 next state  3 agent position  (7, 5)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.17364589933937066 agent memory len 1435 steps  10 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 1574 steps  10 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.17364589933937066 agent memory len 2149 steps  10 reward 0 next state  3 agent position  (6, 5)
 is_terminal [False, False, False]
random action 1
landmark captured 2
agent reached landmark-------------------------------- 0
actions ['R', 'D', 'U']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  11 reward 10 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1575 steps  11 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.17364589933937066 agent memory len 2150 steps  11 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  12 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1576 steps  12 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 2151 steps  12 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'R', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  13 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1577 steps  13 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 2152 steps  13 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  14 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1578 steps  14 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 2153 steps  14 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, False]
actions ['S', 'R', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  15 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1579 steps  15 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 2154 steps  15 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  16 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1580 steps  16 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 2155 steps  16 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'R', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  17 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1581 steps  17 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 2156 steps  17 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  18 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1582 steps  18 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 2157 steps  18 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, False]
actions ['S', 'R', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  19 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1583 steps  19 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 2158 steps  19 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'L', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  20 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1584 steps  20 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.17364589933937066 agent memory len 2159 steps  20 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, False]
actions ['S', 'R', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  21 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1585 steps  21 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 2160 steps  21 reward 0 next state  5 agent position  (5, 5)
 is_terminal [True, False, False]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['S', 'U', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  22 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  22 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2161 steps  22 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  23 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2162 steps  23 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  24 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2163 steps  24 reward 0 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  25 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2164 steps  25 reward 0 next state  4 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  26 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2165 steps  26 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  27 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2166 steps  27 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  28 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2167 steps  28 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  29 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2168 steps  29 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  30 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2169 steps  30 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'U']
agent epsilon  0.17364589933937066 agent memory len 1436 steps  31 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.17364589933937066 agent memory len 1586 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.17364589933937066 agent memory len 2170 steps  31 reward 10 next state  4 agent position  (4, 5)
-----------------------------------all agents reached landmark--------------------------------
total rewards 4
epsilon  0.16566479465049133
epsilon  0.16566479465049133
epsilon  0.16566479465049133
Episode number:  38
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.16566479465049133 agent memory len 1437 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.16566479465049133 agent memory len 1587 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.16566479465049133 agent memory len 2171 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.16566479465049133 agent memory len 1438 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.16566479465049133 agent memory len 1588 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.16566479465049133 agent memory len 2172 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.16566479465049133 agent memory len 1439 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.16566479465049133 agent memory len 1589 steps  3 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.16566479465049133 agent memory len 2173 steps  3 reward -2 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.16566479465049133 agent memory len 1440 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.16566479465049133 agent memory len 1590 steps  4 reward -1 next state  1 agent position  (2, 8)
agent epsilon  0.16566479465049133 agent memory len 2174 steps  4 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.16566479465049133 agent memory len 1441 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.16566479465049133 agent memory len 1591 steps  5 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.16566479465049133 agent memory len 2175 steps  5 reward -1 next state  2 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.16566479465049133 agent memory len 1442 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.16566479465049133 agent memory len 1592 steps  6 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.16566479465049133 agent memory len 2176 steps  6 reward -1 next state  2 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'U']
agent epsilon  0.16566479465049133 agent memory len 1443 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.16566479465049133 agent memory len 1593 steps  7 reward -1 next state  3 agent position  (2, 5)
agent epsilon  0.16566479465049133 agent memory len 2177 steps  7 reward -1 next state  2 agent position  (5, 2)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'D', 'R']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1594 steps  8 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.16566479465049133 agent memory len 2178 steps  8 reward 0 next state  3 agent position  (5, 3)
 is_terminal [True, False, False]
random action 1
actions ['S', 'R', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1595 steps  9 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.16566479465049133 agent memory len 2179 steps  9 reward 0 next state  3 agent position  (4, 3)
 is_terminal [True, False, False]
actions ['S', 'L', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1596 steps  10 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.16566479465049133 agent memory len 2180 steps  10 reward -1 next state  3 agent position  (3, 3)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1597 steps  11 reward -1 next state  4 agent position  (2, 5)
agent epsilon  0.16566479465049133 agent memory len 2181 steps  11 reward -1 next state  2 agent position  (3, 3)
 is_terminal [True, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1598 steps  12 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.16566479465049133 agent memory len 2182 steps  12 reward -1 next state  3 agent position  (3, 3)
 is_terminal [True, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'D', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  13 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2183 steps  13 reward -1 next state  4 agent position  (2, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2184 steps  14 reward -1 next state  4 agent position  (3, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2185 steps  15 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2186 steps  16 reward -1 next state  4 agent position  (2, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2187 steps  17 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2188 steps  18 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2189 steps  19 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2190 steps  20 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2191 steps  21 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2192 steps  22 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2193 steps  23 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2194 steps  24 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2195 steps  25 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2196 steps  26 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2197 steps  27 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2198 steps  28 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2199 steps  29 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2200 steps  30 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2201 steps  31 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2202 steps  32 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2203 steps  33 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2204 steps  34 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2205 steps  35 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2206 steps  36 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2207 steps  37 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2208 steps  38 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2209 steps  39 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2210 steps  40 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2211 steps  41 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2212 steps  42 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2213 steps  43 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2214 steps  44 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2215 steps  45 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2216 steps  46 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2217 steps  47 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2218 steps  48 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2219 steps  49 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2220 steps  50 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2221 steps  51 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2222 steps  52 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2223 steps  53 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2224 steps  54 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2225 steps  55 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2226 steps  56 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2227 steps  57 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2228 steps  58 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2229 steps  59 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2230 steps  60 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2231 steps  61 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2232 steps  62 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2233 steps  63 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2234 steps  64 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2235 steps  65 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2236 steps  66 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2237 steps  67 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2238 steps  68 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2239 steps  69 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2240 steps  70 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2241 steps  71 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2242 steps  72 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2243 steps  73 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2244 steps  74 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.16566479465049133 agent memory len 1444 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.16566479465049133 agent memory len 1599 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.16566479465049133 agent memory len 2245 steps  75 reward 0 next state  4 agent position  (4, 4)
max steps reached
total rewards -47
epsilon  0.1580729330304087
epsilon  0.1580729330304087
epsilon  0.1580729330304087
Episode number:  39
 is_terminal [False, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.1580729330304087 agent memory len 1445 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.1580729330304087 agent memory len 1600 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.1580729330304087 agent memory len 2246 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.1580729330304087 agent memory len 1446 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.1580729330304087 agent memory len 1601 steps  2 reward -1 next state  1 agent position  (2, 9)
agent epsilon  0.1580729330304087 agent memory len 2247 steps  2 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.1580729330304087 agent memory len 1447 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.1580729330304087 agent memory len 1602 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.1580729330304087 agent memory len 2248 steps  3 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.1580729330304087 agent memory len 1448 steps  4 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.1580729330304087 agent memory len 1603 steps  4 reward -2 next state  2 agent position  (3, 9)
agent epsilon  0.1580729330304087 agent memory len 2249 steps  4 reward -1 next state  3 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.1580729330304087 agent memory len 1449 steps  5 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.1580729330304087 agent memory len 1604 steps  5 reward -1 next state  2 agent position  (3, 8)
agent epsilon  0.1580729330304087 agent memory len 2250 steps  5 reward -1 next state  3 agent position  (7, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.1580729330304087 agent memory len 1450 steps  6 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.1580729330304087 agent memory len 1605 steps  6 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.1580729330304087 agent memory len 2251 steps  6 reward -1 next state  3 agent position  (7, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.1580729330304087 agent memory len 1451 steps  7 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.1580729330304087 agent memory len 1606 steps  7 reward 0 next state  3 agent position  (3, 6)
agent epsilon  0.1580729330304087 agent memory len 2252 steps  7 reward 0 next state  3 agent position  (6, 4)
 is_terminal [False, False, False]
random action 1
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'U', 'U']
agent epsilon  0.1580729330304087 agent memory len 1452 steps  8 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.1580729330304087 agent memory len 1607 steps  8 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  8 reward 10 next state  2 agent position  (5, 4)
 is_terminal [False, False, True]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'D', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  9 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1608 steps  9 reward 0 next state  4 agent position  (3, 6)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  9 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1609 steps  10 reward 0 next state  4 agent position  (3, 5)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  10 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1610 steps  11 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  11 reward 0 next state  3 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1611 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  12 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1612 steps  13 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  13 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1613 steps  14 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  14 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1614 steps  15 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  15 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1615 steps  16 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  16 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1616 steps  17 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  17 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1617 steps  18 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  18 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1618 steps  19 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  19 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1619 steps  20 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  20 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1620 steps  21 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  21 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1621 steps  22 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  22 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1622 steps  23 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  23 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1623 steps  24 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  24 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1624 steps  25 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  25 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1625 steps  26 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  26 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1626 steps  27 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  27 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1627 steps  28 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  28 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1628 steps  29 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  29 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1629 steps  30 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  30 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1630 steps  31 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  31 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1631 steps  32 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  32 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1632 steps  33 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  33 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1633 steps  34 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  34 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1634 steps  35 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  35 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1635 steps  36 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  36 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1636 steps  37 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  37 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1637 steps  38 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  38 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1638 steps  39 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  39 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1639 steps  40 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  40 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1640 steps  41 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  41 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1641 steps  42 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  42 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1642 steps  43 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  43 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1643 steps  44 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  44 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1644 steps  45 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  45 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1645 steps  46 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  46 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1646 steps  47 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  47 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1647 steps  48 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  48 reward 0 next state  4 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1648 steps  49 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  49 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1649 steps  50 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  50 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1650 steps  51 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  51 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1651 steps  52 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  52 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1652 steps  53 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  53 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1653 steps  54 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  54 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1654 steps  55 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  55 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1655 steps  56 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  56 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1656 steps  57 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  57 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1657 steps  58 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  58 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1658 steps  59 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  59 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1659 steps  60 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  60 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1660 steps  61 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  61 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1661 steps  62 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  62 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1662 steps  63 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  63 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1663 steps  64 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  64 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1664 steps  65 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  65 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1665 steps  66 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  66 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1666 steps  67 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  67 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1667 steps  68 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  68 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1668 steps  69 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  69 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1669 steps  70 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  70 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1670 steps  71 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  71 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1671 steps  72 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  72 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1672 steps  73 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  73 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1673 steps  74 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  74 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.1580729330304087 agent memory len 1453 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1580729330304087 agent memory len 1674 steps  75 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.1580729330304087 agent memory len 2253 steps  75 reward 0 next state  5 agent position  (5, 4)
max steps reached
total rewards -38
epsilon  0.15085133087064842
epsilon  0.15085133087064842
epsilon  0.15085133087064842
Episode number:  40
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'U']
agent epsilon  0.15085133087064842 agent memory len 1454 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.15085133087064842 agent memory len 1675 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.15085133087064842 agent memory len 2254 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['D', 'D', 'U']
agent epsilon  0.15085133087064842 agent memory len 1455 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.15085133087064842 agent memory len 1676 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.15085133087064842 agent memory len 2255 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'U']
agent epsilon  0.15085133087064842 agent memory len 1456 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.15085133087064842 agent memory len 1677 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.15085133087064842 agent memory len 2256 steps  3 reward -1 next state  2 agent position  (6, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'R']
agent epsilon  0.15085133087064842 agent memory len 1457 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.15085133087064842 agent memory len 1678 steps  4 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.15085133087064842 agent memory len 2257 steps  4 reward -1 next state  2 agent position  (6, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.15085133087064842 agent memory len 1458 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.15085133087064842 agent memory len 1679 steps  5 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.15085133087064842 agent memory len 2258 steps  5 reward -1 next state  2 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'U']
agent epsilon  0.15085133087064842 agent memory len 1459 steps  6 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.15085133087064842 agent memory len 1680 steps  6 reward -1 next state  0 agent position  (3, 8)
agent epsilon  0.15085133087064842 agent memory len 2259 steps  6 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'R']
agent epsilon  0.15085133087064842 agent memory len 1460 steps  7 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.15085133087064842 agent memory len 1681 steps  7 reward -1 next state  0 agent position  (4, 8)
agent epsilon  0.15085133087064842 agent memory len 2260 steps  7 reward -1 next state  4 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'R']
agent epsilon  0.15085133087064842 agent memory len 1461 steps  8 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.15085133087064842 agent memory len 1682 steps  8 reward -1 next state  0 agent position  (5, 8)
agent epsilon  0.15085133087064842 agent memory len 2261 steps  8 reward 0 next state  5 agent position  (6, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.15085133087064842 agent memory len 1462 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.15085133087064842 agent memory len 1683 steps  9 reward -1 next state  1 agent position  (5, 7)
agent epsilon  0.15085133087064842 agent memory len 2262 steps  9 reward 0 next state  5 agent position  (5, 3)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['R', 'S', 'R']
agent epsilon  0.15085133087064842 agent memory len 1463 steps  10 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.15085133087064842 agent memory len 1684 steps  10 reward -1 next state  2 agent position  (5, 7)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  10 reward 10 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1464 steps  11 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.15085133087064842 agent memory len 1685 steps  11 reward 0 next state  3 agent position  (5, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  11 reward 0 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.15085133087064842 agent memory len 1465 steps  12 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.15085133087064842 agent memory len 1686 steps  12 reward 0 next state  2 agent position  (5, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  12 reward 0 next state  5 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.15085133087064842 agent memory len 1466 steps  13 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.15085133087064842 agent memory len 1687 steps  13 reward -1 next state  3 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  13 reward 0 next state  6 agent position  (5, 4)
 is_terminal [False, False, True]
actions ['R', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1467 steps  14 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.15085133087064842 agent memory len 1688 steps  14 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  14 reward 0 next state  6 agent position  (5, 4)
 is_terminal [False, False, True]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'U', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  15 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1689 steps  15 reward 0 next state  4 agent position  (5, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  15 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1690 steps  16 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  16 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1691 steps  17 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  17 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1692 steps  18 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  18 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1693 steps  19 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  19 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1694 steps  20 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  20 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1695 steps  21 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  21 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1696 steps  22 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  22 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1697 steps  23 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  23 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1698 steps  24 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  24 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1699 steps  25 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  25 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1700 steps  26 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  26 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1701 steps  27 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  27 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1702 steps  28 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  28 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1703 steps  29 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  29 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1704 steps  30 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  30 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1705 steps  31 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  31 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1706 steps  32 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  32 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1707 steps  33 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  33 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1708 steps  34 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  34 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1709 steps  35 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  35 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1710 steps  36 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  36 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1711 steps  37 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  37 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1712 steps  38 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  38 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1713 steps  39 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  39 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1714 steps  40 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  40 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1715 steps  41 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  41 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1716 steps  42 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  42 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  43 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1717 steps  43 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  43 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  44 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1718 steps  44 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  44 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  45 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1719 steps  45 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  45 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  46 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1720 steps  46 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  46 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  47 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1721 steps  47 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  47 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  48 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1722 steps  48 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  48 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  49 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1723 steps  49 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  49 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  50 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1724 steps  50 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  50 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  51 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1725 steps  51 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  51 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  52 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1726 steps  52 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  52 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  53 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1727 steps  53 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  53 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  54 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1728 steps  54 reward 0 next state  4 agent position  (5, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  54 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  55 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1729 steps  55 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  55 reward 0 next state  5 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  56 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1730 steps  56 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  56 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  57 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1731 steps  57 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  57 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  58 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1732 steps  58 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  58 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  59 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1733 steps  59 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  59 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  60 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1734 steps  60 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  60 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  61 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1735 steps  61 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  61 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  62 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1736 steps  62 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  62 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  63 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1737 steps  63 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  63 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  64 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1738 steps  64 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  64 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  65 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1739 steps  65 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  65 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  66 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1740 steps  66 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  66 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  67 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1741 steps  67 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  67 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  68 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1742 steps  68 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  68 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  69 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1743 steps  69 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  69 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  70 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1744 steps  70 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  70 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  71 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1745 steps  71 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  71 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  72 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1746 steps  72 reward -1 next state  4 agent position  (6, 7)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  72 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  73 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1747 steps  73 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  73 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  74 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1748 steps  74 reward -1 next state  4 agent position  (6, 5)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  74 reward 0 next state  6 agent position  (5, 4)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.15085133087064842 agent memory len 1468 steps  75 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.15085133087064842 agent memory len 1749 steps  75 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.15085133087064842 agent memory len 2263 steps  75 reward 0 next state  6 agent position  (5, 4)
max steps reached
total rewards -66
epsilon  0.1439819304042466
epsilon  0.1439819304042466
epsilon  0.1439819304042466
Episode number:  41
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.1439819304042466 agent memory len 1469 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.1439819304042466 agent memory len 1750 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.1439819304042466 agent memory len 2264 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'U']
agent epsilon  0.1439819304042466 agent memory len 1470 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.1439819304042466 agent memory len 1751 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.1439819304042466 agent memory len 2265 steps  2 reward -1 next state  2 agent position  (7, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.1439819304042466 agent memory len 1471 steps  3 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.1439819304042466 agent memory len 1752 steps  3 reward -1 next state  0 agent position  (3, 9)
agent epsilon  0.1439819304042466 agent memory len 2266 steps  3 reward -1 next state  3 agent position  (7, 0)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.1439819304042466 agent memory len 1472 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.1439819304042466 agent memory len 1753 steps  4 reward -1 next state  1 agent position  (3, 8)
agent epsilon  0.1439819304042466 agent memory len 2267 steps  4 reward -1 next state  3 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.1439819304042466 agent memory len 1473 steps  5 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.1439819304042466 agent memory len 1754 steps  5 reward -1 next state  2 agent position  (3, 7)
agent epsilon  0.1439819304042466 agent memory len 2268 steps  5 reward -1 next state  3 agent position  (6, 1)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.1439819304042466 agent memory len 1474 steps  6 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.1439819304042466 agent memory len 1755 steps  6 reward 0 next state  2 agent position  (3, 6)
agent epsilon  0.1439819304042466 agent memory len 2269 steps  6 reward -1 next state  3 agent position  (5, 1)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.1439819304042466 agent memory len 1475 steps  7 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.1439819304042466 agent memory len 1756 steps  7 reward 0 next state  2 agent position  (3, 5)
agent epsilon  0.1439819304042466 agent memory len 2270 steps  7 reward -1 next state  3 agent position  (5, 2)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.1439819304042466 agent memory len 1476 steps  8 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.1439819304042466 agent memory len 1757 steps  8 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.1439819304042466 agent memory len 2271 steps  8 reward 0 next state  3 agent position  (5, 3)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['D', 'D', 'U']
agent epsilon  0.1439819304042466 agent memory len 1477 steps  9 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  9 reward 10 next state  3 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2272 steps  9 reward 0 next state  4 agent position  (4, 3)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.1439819304042466 agent memory len 1478 steps  10 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2273 steps  10 reward 0 next state  4 agent position  (4, 4)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1479 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2274 steps  11 reward 0 next state  4 agent position  (3, 4)
 is_terminal [False, True, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['R', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  12 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  12 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2275 steps  12 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  13 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2276 steps  13 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  14 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2277 steps  14 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  15 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2278 steps  15 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  16 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2279 steps  16 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  17 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2280 steps  17 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  18 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2281 steps  18 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  19 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2282 steps  19 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  20 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2283 steps  20 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  21 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2284 steps  21 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  22 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2285 steps  22 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  23 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2286 steps  23 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  24 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2287 steps  24 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  25 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2288 steps  25 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  26 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2289 steps  26 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  27 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2290 steps  27 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  28 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2291 steps  28 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  29 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2292 steps  29 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  30 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2293 steps  30 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  31 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2294 steps  31 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  32 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2295 steps  32 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  33 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2296 steps  33 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  34 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2297 steps  34 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  35 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2298 steps  35 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  36 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2299 steps  36 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  37 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2300 steps  37 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  38 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2301 steps  38 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  39 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2302 steps  39 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  40 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2303 steps  40 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  41 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2304 steps  41 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  42 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2305 steps  42 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  43 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  43 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2306 steps  43 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  44 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  44 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2307 steps  44 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  45 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  45 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2308 steps  45 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  46 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  46 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2309 steps  46 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  47 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  47 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2310 steps  47 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  48 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  48 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2311 steps  48 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  49 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  49 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2312 steps  49 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  50 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  50 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2313 steps  50 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  51 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  51 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2314 steps  51 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  52 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  52 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2315 steps  52 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  53 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  53 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2316 steps  53 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  54 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  54 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2317 steps  54 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  55 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  55 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2318 steps  55 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  56 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  56 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2319 steps  56 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  57 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  57 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2320 steps  57 reward -1 next state  4 agent position  (2, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  58 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  58 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2321 steps  58 reward -1 next state  4 agent position  (2, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  59 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  59 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2322 steps  59 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  60 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  60 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2323 steps  60 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  61 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  61 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2324 steps  61 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  62 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  62 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2325 steps  62 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  63 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  63 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2326 steps  63 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  64 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  64 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2327 steps  64 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  65 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  65 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2328 steps  65 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  66 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  66 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2329 steps  66 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  67 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  67 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2330 steps  67 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  68 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  68 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2331 steps  68 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  69 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  69 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2332 steps  69 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  70 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  70 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2333 steps  70 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  71 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  71 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2334 steps  71 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  72 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  72 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2335 steps  72 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  73 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  73 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2336 steps  73 reward 0 next state  4 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  74 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  74 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2337 steps  74 reward -1 next state  4 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.1439819304042466 agent memory len 1480 steps  75 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.1439819304042466 agent memory len 1758 steps  75 reward 0 next state  5 agent position  (4, 4)
agent epsilon  0.1439819304042466 agent memory len 2338 steps  75 reward 0 next state  4 agent position  (4, 4)
max steps reached
total rewards -36
epsilon  0.13744755455192614
epsilon  0.13744755455192614
epsilon  0.13744755455192614
Episode number:  42
 is_terminal [False, False, False]
actions ['D', 'D', 'S']
agent epsilon  0.13744755455192614 agent memory len 1481 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.13744755455192614 agent memory len 1759 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.13744755455192614 agent memory len 2339 steps  1 reward -1 next state  1 agent position  (9, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.13744755455192614 agent memory len 1482 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.13744755455192614 agent memory len 1760 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.13744755455192614 agent memory len 2340 steps  2 reward -1 next state  2 agent position  (8, 0)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.13744755455192614 agent memory len 1483 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.13744755455192614 agent memory len 1761 steps  3 reward -1 next state  1 agent position  (2, 8)
agent epsilon  0.13744755455192614 agent memory len 2341 steps  3 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.13744755455192614 agent memory len 1484 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.13744755455192614 agent memory len 1762 steps  4 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.13744755455192614 agent memory len 2342 steps  4 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1485 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.13744755455192614 agent memory len 1763 steps  5 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.13744755455192614 agent memory len 2343 steps  5 reward -1 next state  2 agent position  (8, 2)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'R']
agent epsilon  0.13744755455192614 agent memory len 1486 steps  6 reward 0 next state  3 agent position  (3, 3)
agent epsilon  0.13744755455192614 agent memory len 1764 steps  6 reward -1 next state  3 agent position  (2, 7)
agent epsilon  0.13744755455192614 agent memory len 2344 steps  6 reward -1 next state  2 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.13744755455192614 agent memory len 1487 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.13744755455192614 agent memory len 1765 steps  7 reward -1 next state  3 agent position  (2, 6)
agent epsilon  0.13744755455192614 agent memory len 2345 steps  7 reward -1 next state  2 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.13744755455192614 agent memory len 1488 steps  8 reward 0 next state  5 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 1766 steps  8 reward -1 next state  3 agent position  (2, 5)
agent epsilon  0.13744755455192614 agent memory len 2346 steps  8 reward -1 next state  2 agent position  (7, 4)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 0
actions ['R', 'L', 'R']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  9 reward 10 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1767 steps  9 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.13744755455192614 agent memory len 2347 steps  9 reward -1 next state  2 agent position  (7, 5)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  10 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1768 steps  10 reward 0 next state  4 agent position  (3, 4)
agent epsilon  0.13744755455192614 agent memory len 2348 steps  10 reward -1 next state  3 agent position  (6, 5)
 is_terminal [True, False, False]
actions ['S', 'L', 'U']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  11 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1769 steps  11 reward 0 next state  4 agent position  (3, 3)
agent epsilon  0.13744755455192614 agent memory len 2349 steps  11 reward 0 next state  3 agent position  (5, 5)
 is_terminal [True, False, False]
actions ['S', 'D', 'R']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  12 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1770 steps  12 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.13744755455192614 agent memory len 2350 steps  12 reward 0 next state  4 agent position  (5, 6)
 is_terminal [True, False, False]
actions ['S', 'D', 'U']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  13 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1771 steps  13 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 2351 steps  13 reward 0 next state  5 agent position  (4, 6)
 is_terminal [True, False, False]
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['S', 'R', 'L']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  14 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1772 steps  14 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  14 reward 10 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  15 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1773 steps  15 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  15 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  16 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1774 steps  16 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  16 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  17 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1775 steps  17 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  17 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  18 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1776 steps  18 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  18 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  19 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1777 steps  19 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  19 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  20 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1778 steps  20 reward 0 next state  4 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  20 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  21 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1779 steps  21 reward 0 next state  4 agent position  (5, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  21 reward 0 next state  5 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  22 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1780 steps  22 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  22 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  23 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1781 steps  23 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  23 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  24 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1782 steps  24 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  24 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  25 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1783 steps  25 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  25 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  26 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1784 steps  26 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  26 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  27 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1785 steps  27 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  27 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  28 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1786 steps  28 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  28 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  29 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1787 steps  29 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  29 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  30 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1788 steps  30 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  30 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  31 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1789 steps  31 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  31 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  32 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1790 steps  32 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  32 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  33 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1791 steps  33 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  33 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  34 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1792 steps  34 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  34 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  35 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1793 steps  35 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  35 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  36 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1794 steps  36 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  36 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  37 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1795 steps  37 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  37 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  38 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1796 steps  38 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  38 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  39 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1797 steps  39 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  39 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  40 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1798 steps  40 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  40 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  41 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1799 steps  41 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  41 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  42 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1800 steps  42 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  42 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  43 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1801 steps  43 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  43 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  44 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1802 steps  44 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  44 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  45 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1803 steps  45 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  45 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  46 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1804 steps  46 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  46 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  47 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1805 steps  47 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  47 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  48 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1806 steps  48 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  48 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  49 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1807 steps  49 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  49 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  50 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1808 steps  50 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  50 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  51 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1809 steps  51 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  51 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  52 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1810 steps  52 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  52 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  53 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1811 steps  53 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  53 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  54 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1812 steps  54 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  54 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  55 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1813 steps  55 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  55 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  56 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1814 steps  56 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  56 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  57 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1815 steps  57 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  57 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  58 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1816 steps  58 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  58 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  59 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1817 steps  59 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  59 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  60 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1818 steps  60 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  60 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  61 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1819 steps  61 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  61 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  62 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1820 steps  62 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  62 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  63 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1821 steps  63 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  63 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  64 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1822 steps  64 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  64 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  65 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1823 steps  65 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  65 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  66 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1824 steps  66 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  66 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  67 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1825 steps  67 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  67 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  68 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1826 steps  68 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  68 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  69 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1827 steps  69 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  69 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  70 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1828 steps  70 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  70 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  71 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1829 steps  71 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  71 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  72 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1830 steps  72 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  72 reward 0 next state  6 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  73 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1831 steps  73 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  73 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  74 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1832 steps  74 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  74 reward 0 next state  7 agent position  (4, 5)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.13744755455192614 agent memory len 1489 steps  75 reward 0 next state  5 agent position  (5, 4)
agent epsilon  0.13744755455192614 agent memory len 1833 steps  75 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.13744755455192614 agent memory len 2352 steps  75 reward 0 next state  7 agent position  (4, 5)
max steps reached
total rewards -57
epsilon  0.13123186397045208
epsilon  0.13123186397045208
epsilon  0.13123186397045208
Episode number:  43
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.13123186397045208 agent memory len 1490 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.13123186397045208 agent memory len 1834 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.13123186397045208 agent memory len 2353 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.13123186397045208 agent memory len 1491 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.13123186397045208 agent memory len 1835 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.13123186397045208 agent memory len 2354 steps  2 reward -1 next state  2 agent position  (9, 2)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.13123186397045208 agent memory len 1492 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.13123186397045208 agent memory len 1836 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.13123186397045208 agent memory len 2355 steps  3 reward -1 next state  3 agent position  (9, 3)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.13123186397045208 agent memory len 1493 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.13123186397045208 agent memory len 1837 steps  4 reward -1 next state  1 agent position  (3, 8)
agent epsilon  0.13123186397045208 agent memory len 2356 steps  4 reward -1 next state  3 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.13123186397045208 agent memory len 1494 steps  5 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.13123186397045208 agent memory len 1838 steps  5 reward -1 next state  2 agent position  (3, 7)
agent epsilon  0.13123186397045208 agent memory len 2357 steps  5 reward -1 next state  3 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.13123186397045208 agent memory len 1495 steps  6 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.13123186397045208 agent memory len 1839 steps  6 reward 0 next state  2 agent position  (3, 6)
agent epsilon  0.13123186397045208 agent memory len 2358 steps  6 reward -1 next state  3 agent position  (7, 4)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.13123186397045208 agent memory len 1496 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.13123186397045208 agent memory len 1840 steps  7 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.13123186397045208 agent memory len 2359 steps  7 reward -1 next state  4 agent position  (7, 4)
 is_terminal [False, False, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'D', 'R']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1841 steps  8 reward 0 next state  4 agent position  (5, 6)
agent epsilon  0.13123186397045208 agent memory len 2360 steps  8 reward -1 next state  5 agent position  (7, 5)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1842 steps  9 reward 0 next state  4 agent position  (5, 5)
agent epsilon  0.13123186397045208 agent memory len 2361 steps  9 reward 0 next state  5 agent position  (6, 5)
 is_terminal [True, False, False]
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'U', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  10 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2362 steps  10 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2363 steps  11 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  12 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  12 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2364 steps  12 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  13 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  13 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2365 steps  13 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  14 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  14 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2366 steps  14 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  15 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  15 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2367 steps  15 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  16 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  16 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2368 steps  16 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  17 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  17 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2369 steps  17 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  18 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  18 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2370 steps  18 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  19 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  19 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2371 steps  19 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  20 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  20 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2372 steps  20 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  21 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  21 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2373 steps  21 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  22 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  22 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2374 steps  22 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  23 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  23 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2375 steps  23 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  24 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  24 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2376 steps  24 reward -1 next state  4 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  25 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  25 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2377 steps  25 reward -1 next state  4 agent position  (3, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  26 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  26 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2378 steps  26 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  27 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  27 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2379 steps  27 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  28 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  28 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2380 steps  28 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  29 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  29 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2381 steps  29 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  30 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  30 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2382 steps  30 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  31 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  31 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2383 steps  31 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  32 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  32 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2384 steps  32 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  33 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  33 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2385 steps  33 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  34 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  34 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2386 steps  34 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  35 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  35 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2387 steps  35 reward -1 next state  4 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  36 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  36 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2388 steps  36 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  37 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  37 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2389 steps  37 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  38 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  38 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2390 steps  38 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  39 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  39 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2391 steps  39 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  40 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  40 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2392 steps  40 reward 0 next state  4 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  41 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  41 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2393 steps  41 reward 0 next state  4 agent position  (5, 5)
 is_terminal [True, True, False]
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'L']
agent epsilon  0.13123186397045208 agent memory len 1497 steps  42 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.13123186397045208 agent memory len 1843 steps  42 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.13123186397045208 agent memory len 2394 steps  42 reward 10 next state  4 agent position  (5, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards 6
epsilon  0.125319316195762
epsilon  0.125319316195762
epsilon  0.125319316195762
Episode number:  44
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.125319316195762 agent memory len 1498 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.125319316195762 agent memory len 1844 steps  1 reward -1 next state  0 agent position  (1, 9)
agent epsilon  0.125319316195762 agent memory len 2395 steps  1 reward -1 next state  1 agent position  (8, 0)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.125319316195762 agent memory len 1499 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.125319316195762 agent memory len 1845 steps  2 reward -1 next state  0 agent position  (1, 8)
agent epsilon  0.125319316195762 agent memory len 2396 steps  2 reward -1 next state  1 agent position  (8, 1)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.125319316195762 agent memory len 1500 steps  3 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.125319316195762 agent memory len 1846 steps  3 reward -1 next state  0 agent position  (2, 8)
agent epsilon  0.125319316195762 agent memory len 2397 steps  3 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.125319316195762 agent memory len 1501 steps  4 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.125319316195762 agent memory len 1847 steps  4 reward -1 next state  0 agent position  (3, 8)
agent epsilon  0.125319316195762 agent memory len 2398 steps  4 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.125319316195762 agent memory len 1502 steps  5 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.125319316195762 agent memory len 1848 steps  5 reward -1 next state  1 agent position  (4, 8)
agent epsilon  0.125319316195762 agent memory len 2399 steps  5 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.125319316195762 agent memory len 1503 steps  6 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.125319316195762 agent memory len 1849 steps  6 reward -1 next state  2 agent position  (4, 7)
agent epsilon  0.125319316195762 agent memory len 2400 steps  6 reward -1 next state  4 agent position  (7, 3)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.125319316195762 agent memory len 1504 steps  7 reward 0 next state  4 agent position  (4, 3)
agent epsilon  0.125319316195762 agent memory len 1850 steps  7 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.125319316195762 agent memory len 2401 steps  7 reward 0 next state  4 agent position  (6, 3)
 is_terminal [False, False, False]
random action 2
landmark captured 0
landmark captured 1
agent reached landmark-------------------------------- 0
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'D']
agent epsilon  0.125319316195762 agent memory len 1505 steps  8 reward 10 next state  4 agent position  (4, 4)
agent epsilon  0.125319316195762 agent memory len 1851 steps  8 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.125319316195762 agent memory len 2402 steps  8 reward -1 next state  4 agent position  (7, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.125319316195762 agent memory len 1505 steps  9 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.125319316195762 agent memory len 1851 steps  9 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.125319316195762 agent memory len 2403 steps  9 reward 0 next state  4 agent position  (6, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.125319316195762 agent memory len 1505 steps  10 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.125319316195762 agent memory len 1851 steps  10 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.125319316195762 agent memory len 2404 steps  10 reward 0 next state  4 agent position  (6, 4)
 is_terminal [True, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'U']
agent epsilon  0.125319316195762 agent memory len 1505 steps  11 reward 0 next state  4 agent position  (4, 4)
agent epsilon  0.125319316195762 agent memory len 1851 steps  11 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.125319316195762 agent memory len 2405 steps  11 reward 10 next state  4 agent position  (5, 4)
-----------------------------------all agents reached landmark--------------------------------
total rewards 11
epsilon  0.11969512677871053
epsilon  0.11969512677871053
epsilon  0.11969512677871053
Episode number:  45
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.11969512677871053 agent memory len 1506 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.11969512677871053 agent memory len 1852 steps  1 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.11969512677871053 agent memory len 2406 steps  1 reward -1 next state  1 agent position  (9, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.11969512677871053 agent memory len 1507 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.11969512677871053 agent memory len 1853 steps  2 reward -1 next state  0 agent position  (2, 9)
agent epsilon  0.11969512677871053 agent memory len 2407 steps  2 reward -1 next state  2 agent position  (8, 1)
 is_terminal [False, False, False]
actions ['R', 'D', 'R']
agent epsilon  0.11969512677871053 agent memory len 1508 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.11969512677871053 agent memory len 1854 steps  3 reward -1 next state  1 agent position  (3, 9)
agent epsilon  0.11969512677871053 agent memory len 2408 steps  3 reward -1 next state  3 agent position  (8, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.11969512677871053 agent memory len 1509 steps  4 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.11969512677871053 agent memory len 1855 steps  4 reward -1 next state  2 agent position  (4, 9)
agent epsilon  0.11969512677871053 agent memory len 2409 steps  4 reward -1 next state  4 agent position  (8, 3)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.11969512677871053 agent memory len 1510 steps  5 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.11969512677871053 agent memory len 1856 steps  5 reward -1 next state  2 agent position  (4, 8)
agent epsilon  0.11969512677871053 agent memory len 2410 steps  5 reward -1 next state  4 agent position  (8, 4)
 is_terminal [False, False, False]
actions ['D', 'L', 'U']
agent epsilon  0.11969512677871053 agent memory len 1511 steps  6 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.11969512677871053 agent memory len 1857 steps  6 reward -1 next state  2 agent position  (4, 7)
agent epsilon  0.11969512677871053 agent memory len 2411 steps  6 reward -1 next state  4 agent position  (7, 4)
 is_terminal [False, False, False]
actions ['R', 'L', 'U']
agent epsilon  0.11969512677871053 agent memory len 1512 steps  7 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.11969512677871053 agent memory len 1858 steps  7 reward 0 next state  3 agent position  (4, 6)
agent epsilon  0.11969512677871053 agent memory len 2412 steps  7 reward 0 next state  4 agent position  (6, 4)
 is_terminal [False, False, False]
landmark captured 1
landmark captured 2
agent reached landmark-------------------------------- 1
agent reached landmark-------------------------------- 2
actions ['R', 'L', 'U']
agent epsilon  0.11969512677871053 agent memory len 1513 steps  8 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  8 reward 10 next state  4 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  8 reward 10 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1514 steps  9 reward 0 next state  3 agent position  (3, 4)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  9 reward 0 next state  4 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  9 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1515 steps  10 reward 0 next state  3 agent position  (3, 5)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  10 reward 0 next state  5 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  10 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1516 steps  11 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  11 reward 0 next state  6 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  11 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1517 steps  12 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  12 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  12 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1518 steps  13 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  13 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  13 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1519 steps  14 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  14 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  14 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1520 steps  15 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  15 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  15 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1521 steps  16 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  16 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  16 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1522 steps  17 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  17 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  17 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1523 steps  18 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  18 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  18 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1524 steps  19 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  19 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  19 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1525 steps  20 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  20 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  20 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1526 steps  21 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  21 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  21 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1527 steps  22 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  22 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  22 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1528 steps  23 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  23 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  23 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1529 steps  24 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  24 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  24 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1530 steps  25 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  25 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  25 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1531 steps  26 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  26 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  26 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1532 steps  27 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  27 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  27 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1533 steps  28 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  28 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  28 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1534 steps  29 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  29 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  29 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1535 steps  30 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  30 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  30 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1536 steps  31 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  31 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  31 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1537 steps  32 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  32 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  32 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1538 steps  33 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  33 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  33 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1539 steps  34 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  34 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  34 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1540 steps  35 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  35 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  35 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1541 steps  36 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  36 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  36 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1542 steps  37 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  37 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  37 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1543 steps  38 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  38 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  38 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1544 steps  39 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  39 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  39 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1545 steps  40 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  40 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  40 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1546 steps  41 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  41 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  41 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1547 steps  42 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  42 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  42 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1548 steps  43 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  43 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  43 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1549 steps  44 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  44 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  44 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1550 steps  45 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  45 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  45 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1551 steps  46 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  46 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  46 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1552 steps  47 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  47 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  47 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1553 steps  48 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  48 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  48 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1554 steps  49 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  49 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  49 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1555 steps  50 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  50 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  50 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1556 steps  51 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  51 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  51 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1557 steps  52 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  52 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  52 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1558 steps  53 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  53 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  53 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1559 steps  54 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  54 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  54 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1560 steps  55 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  55 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  55 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1561 steps  56 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  56 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  56 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1562 steps  57 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  57 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  57 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1563 steps  58 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  58 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  58 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1564 steps  59 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  59 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  59 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1565 steps  60 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  60 reward 0 next state  9 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  60 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1566 steps  61 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  61 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  61 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1567 steps  62 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  62 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  62 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1568 steps  63 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  63 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  63 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1569 steps  64 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  64 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  64 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1570 steps  65 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  65 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  65 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1571 steps  66 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  66 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  66 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1572 steps  67 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  67 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  67 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1573 steps  68 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  68 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  68 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1574 steps  69 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  69 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  69 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1575 steps  70 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  70 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  70 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1576 steps  71 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  71 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  71 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1577 steps  72 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  72 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  72 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1578 steps  73 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  73 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  73 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1579 steps  74 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  74 reward 0 next state  7 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  74 reward 0 next state  4 agent position  (5, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.11969512677871053 agent memory len 1580 steps  75 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.11969512677871053 agent memory len 1859 steps  75 reward 0 next state  8 agent position  (4, 5)
agent epsilon  0.11969512677871053 agent memory len 2413 steps  75 reward 0 next state  4 agent position  (5, 4)
max steps reached
total rewards -64
epsilon  0.11434523231624569
epsilon  0.11434523231624569
