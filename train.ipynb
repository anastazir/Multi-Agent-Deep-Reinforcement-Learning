{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "import  numpy as np\n",
    "from    config          import *\n",
    "from    agent           import Agent\n",
    "from    enviroment      import Enviroment\n",
    "from    IPython.display import clear_output\n",
    "from    matplotlib      import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = GRID_SIZE\n",
    "num_col = grid_size\n",
    "\n",
    "possibleActions = POSSIBLE_ACTIONS\n",
    "\n",
    "action_space_dict = {\n",
    "    \"U\" : 0,\n",
    "    \"D\" : 1,\n",
    "    \"L\" : 2,\n",
    "    \"R\" : 3,\n",
    "    \"S\" : 4\n",
    "}\n",
    "TYPE              = \"random\"\n",
    "n_agents          = N_AGENTS\n",
    "allplayerpos      = PLAYER_POS[: n_agents]\n",
    "enemy_list_pos    = ENEMY_POS[: n_agents]\n",
    "batch_size        = BATCH_SIZE\n",
    "replay_memory_len = REPLAY_MEMORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(state_num):\n",
    "    return int(state_num/num_col), state_num%num_col\n",
    "\n",
    "def state_encode(row,col):\n",
    "    return row*num_col + col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    total_step = 0\n",
    "    rewards_list = []\n",
    "    timesteps_list = []\n",
    "    total_steps = 1\n",
    "    for episode in range(1, EPISODES):\n",
    "        print(\"Episode number: \", episode)\n",
    "\n",
    "        reward_all = 0\n",
    "        time_step = 1\n",
    "        i = 0\n",
    "\n",
    "        for agent in all_agents:\n",
    "            agent.terminal = False\n",
    "        \n",
    "        [states, enemy_states] = env.reset()\n",
    "        print(\"enemy states: \", enemy_states)\n",
    "        for agent in all_agents:\n",
    "            agent.set_pos(allplayerpos[i])\n",
    "            i = i + 1\n",
    "\n",
    "        done = [False for _ in range(n_agents)]\n",
    "\n",
    "        while not any(done):\n",
    "\n",
    "            # env.render(clear=True)\n",
    "            actions = []\n",
    "            for agent in all_agents:\n",
    "                actions.append(agent.act(np.concatenate((states,enemy_states)).ravel(), possibleActions))\n",
    "\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            for agent in all_agents:\n",
    "\n",
    "                next_states = np.array(next_states)\n",
    "                next_states = next_states.ravel()\n",
    "\n",
    "                agent.set_pos(decode_state(next_states[agent.index]))\n",
    "                agent.store(np.concatenate((states,enemy_states)).ravel(), action_space_dict[actions[agent.index]], \\\n",
    "                rewards[agent.index], np.concatenate((next_states,enemy_states)).ravel(), done[agent.index])\n",
    "\n",
    "                if done[agent.index] == True:\n",
    "                    agent.terminal = True\n",
    "                    print(\"agent reached landmark\")\n",
    "                    print(\"updating target model\")\n",
    "                    agent.alighn_target_model()\n",
    "            print(\"actions\", actions)\n",
    "\n",
    "            print(\"agent epsilon \", all_agents[0].epsilon, \"agent memory len\",\\\n",
    "                len(all_agents[0].expirience_replay), \"steps \", time_step,\\\n",
    "                \"reward\", rewards[0], \"next state \", next_states[0], \"agent position \", all_agents[0].return_coordinates())\n",
    "\n",
    "            if time_step >= TIME_STEPS:\n",
    "                print(\"max steps reached\")\n",
    "                break\n",
    "\n",
    "\n",
    "            total_step += 1\n",
    "            time_step += 1\n",
    "            total_steps+1\n",
    "            states = next_states\n",
    "            reward_all += sum(rewards)\n",
    "\n",
    "            if time_step % REPLAY_STEPS == 0:\n",
    "                for agent in all_agents:\n",
    "                    agent.save_model()\n",
    "\n",
    "        print(\"total rewards\", reward_all)\n",
    "        for agent in all_agents:\n",
    "            agent.decay_epsilon(episode)\n",
    "            agent.retrain()\n",
    "        rewards_list.append(reward_all)\n",
    "        timesteps_list.append(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "-\t-\tP\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\tX\t\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_agents= []\n",
    "\n",
    "for index in range(n_agents):\n",
    "\n",
    "    all_agents.append(Agent(index, allplayerpos[index]))\n",
    "\n",
    "\n",
    "initial_states = []\n",
    "for agent in all_agents:\n",
    "    initial_states.append(state_encode(agent.x, agent.y))\n",
    "\n",
    "enemy_states = []\n",
    "for enemy_pos in enemy_list_pos:\n",
    "    enemy_states.append(state_encode(enemy_pos[0], enemy_pos[1]))\n",
    "\n",
    "env = Enviroment(initial_states = initial_states, enemy_states = enemy_states, type = \"random\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  1\n",
      "enemy states:  [21]\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 1 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 2 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 8 steps  8 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 9 steps  9 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 14 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 16 steps  16 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 18 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 20 steps  20 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  11 agent position  (2, 1)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  2\n",
      "enemy states:  [20]\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 27 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 28 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 29 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 30 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 31 steps  5 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 32 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 33 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 34 steps  8 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 35 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 36 steps  10 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 37 steps  11 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 38 steps  12 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 39 steps  13 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 40 steps  14 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 41 steps  15 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 42 steps  16 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 43 steps  17 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 44 steps  18 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 45 steps  19 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 46 steps  20 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 47 steps  21 reward -2 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 48 steps  22 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 49 steps  23 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 50 steps  24 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 51 steps  25 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 52 steps  26 reward -1 next state  3 agent position  (0, 3)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  3\n",
      "enemy states:  [13]\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 53 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 54 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 55 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 56 steps  4 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 57 steps  5 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 58 steps  6 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 59 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 60 steps  8 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 61 steps  9 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 62 steps  10 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 63 steps  11 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 64 steps  12 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 65 steps  13 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 66 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 67 steps  15 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 68 steps  16 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 69 steps  17 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 70 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 71 steps  19 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 72 steps  20 reward 0 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 73 steps  21 reward 0 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 74 steps  22 reward 0 next state  8 agent position  (1, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 75 steps  23 reward 0 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 76 steps  24 reward 0 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 77 steps  25 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.9057890438555999 agent memory len 78 steps  26 reward -1 next state  3 agent position  (0, 3)\n",
      "max steps reached\n",
      "total rewards -21\n",
      "Episode number:  4\n",
      "enemy states:  [5]\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 79 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 80 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 81 steps  3 reward 0 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 82 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 83 steps  5 reward 0 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 84 steps  6 reward 0 next state  6 agent position  (1, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 85 steps  7 reward 0 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 86 steps  8 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 87 steps  9 reward 0 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 88 steps  10 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 89 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 90 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 91 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 92 steps  14 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 93 steps  15 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 94 steps  16 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.8621008966608072 agent memory len 95 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 96 steps  18 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 97 steps  19 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 98 steps  20 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.8621008966608072 agent memory len 99 steps  21 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 100 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.8621008966608072 agent memory len 101 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 102 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.8621008966608072 agent memory len 103 steps  25 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.8621008966608072 agent memory len 104 steps  26 reward -1 next state  21 agent position  (4, 1)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  5\n",
      "enemy states:  [17]\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 105 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 106 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 107 steps  3 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 108 steps  4 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 109 steps  5 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 110 steps  6 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.820543445547202 agent memory len 111 steps  7 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 112 steps  8 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 113 steps  9 reward 0 next state  23 agent position  (4, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 114 steps  10 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 115 steps  11 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 116 steps  12 reward 0 next state  18 agent position  (3, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 117 steps  13 reward 10 next state  17 agent position  (3, 2)\n",
      "total rewards 5\n",
      "Episode number:  6\n",
      "enemy states:  [18]\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 118 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 119 steps  2 reward -2 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 120 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 121 steps  4 reward -1 next state  0 agent position  (0, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 122 steps  5 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 123 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 124 steps  7 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 125 steps  8 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 126 steps  9 reward -2 next state  10 agent position  (2, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 127 steps  10 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 128 steps  11 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 129 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 130 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 131 steps  14 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 132 steps  15 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 133 steps  16 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 134 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 135 steps  18 reward -2 next state  15 agent position  (3, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 136 steps  19 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 137 steps  20 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 138 steps  21 reward -1 next state  5 agent position  (1, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 139 steps  22 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 140 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 141 steps  24 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 142 steps  25 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 143 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -32\n",
      "Episode number:  7\n",
      "enemy states:  [15]\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 144 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 145 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 146 steps  3 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 147 steps  4 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 148 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 149 steps  6 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 150 steps  7 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 151 steps  8 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 152 steps  9 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 153 steps  10 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 154 steps  11 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 155 steps  12 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 156 steps  13 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 157 steps  14 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 158 steps  15 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 159 steps  16 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 160 steps  17 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 161 steps  18 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 162 steps  19 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 163 steps  20 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 164 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 165 steps  22 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 166 steps  23 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 167 steps  24 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 168 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 169 steps  26 reward -1 next state  17 agent position  (3, 2)\n",
      "max steps reached\n",
      "total rewards -30\n",
      "Episode number:  8\n",
      "enemy states:  [22]\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 170 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 171 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 172 steps  3 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 173 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 174 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 175 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 176 steps  7 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 177 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 178 steps  9 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.7076412088215263 agent memory len 179 steps  10 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 180 steps  11 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 181 steps  12 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 182 steps  13 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 183 steps  14 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 184 steps  15 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 185 steps  16 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 186 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 187 steps  18 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 188 steps  19 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 189 steps  20 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 190 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 191 steps  22 reward 0 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 192 steps  23 reward 0 next state  17 agent position  (3, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.7076412088215263 agent memory len 193 steps  24 reward 0 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 194 steps  25 reward 0 next state  16 agent position  (3, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.7076412088215263 agent memory len 195 steps  26 reward 0 next state  17 agent position  (3, 2)\n",
      "max steps reached\n",
      "total rewards -24\n",
      "Episode number:  9\n",
      "enemy states:  [20]\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 196 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 197 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 198 steps  3 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 199 steps  4 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 200 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 201 steps  6 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 202 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 203 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 204 steps  9 reward -1 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 205 steps  10 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 206 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 207 steps  12 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 208 steps  13 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 209 steps  14 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 210 steps  15 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 211 steps  16 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 212 steps  17 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 213 steps  18 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 214 steps  19 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 215 steps  20 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 216 steps  21 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 217 steps  22 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 218 steps  23 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 219 steps  24 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 220 steps  25 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 221 steps  26 reward -1 next state  9 agent position  (1, 4)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  10\n",
      "enemy states:  [6]\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 222 steps  1 reward 0 next state  1 agent position  (0, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 223 steps  2 reward 0 next state  2 agent position  (0, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 224 steps  3 reward 0 next state  1 agent position  (0, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 225 steps  4 reward 0 next state  1 agent position  (0, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 226 steps  5 reward 0 next state  0 agent position  (0, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 227 steps  6 reward -2 next state  0 agent position  (0, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 228 steps  7 reward 0 next state  1 agent position  (0, 1)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 229 steps  8 reward 10 next state  6 agent position  (1, 1)\n",
      "total rewards 8\n",
      "Episode number:  11\n",
      "enemy states:  [8]\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 230 steps  1 reward 0 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 231 steps  2 reward 0 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 232 steps  3 reward 0 next state  9 agent position  (1, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 233 steps  4 reward -2 next state  9 agent position  (1, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.6104653531155071 agent memory len 234 steps  5 reward 0 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 235 steps  6 reward 0 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.6104653531155071 agent memory len 236 steps  7 reward 0 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 237 steps  8 reward 0 next state  3 agent position  (0, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 238 steps  9 reward 10 next state  8 agent position  (1, 3)\n",
      "total rewards 8\n",
      "Episode number:  12\n",
      "enemy states:  [10]\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 239 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 240 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 241 steps  3 reward -2 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 242 steps  4 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 243 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 244 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 245 steps  7 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 246 steps  8 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 247 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 248 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 249 steps  11 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 250 steps  12 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 251 steps  13 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 252 steps  14 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 253 steps  15 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 254 steps  16 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 255 steps  17 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 256 steps  18 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 257 steps  19 reward -1 next state  9 agent position  (1, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 258 steps  20 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 259 steps  21 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 260 steps  22 reward -1 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 261 steps  23 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 262 steps  24 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 263 steps  25 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 264 steps  26 reward -1 next state  3 agent position  (0, 3)\n",
      "max steps reached\n",
      "total rewards -31\n",
      "Episode number:  13\n",
      "enemy states:  [8]\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 265 steps  1 reward 0 next state  3 agent position  (0, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 266 steps  2 reward 0 next state  2 agent position  (0, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 267 steps  3 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 268 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 269 steps  5 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 270 steps  6 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 271 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 272 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 273 steps  9 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 274 steps  10 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 275 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 276 steps  12 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 277 steps  13 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 278 steps  14 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 279 steps  15 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 280 steps  16 reward -2 next state  21 agent position  (4, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 281 steps  17 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 282 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 283 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 284 steps  20 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 285 steps  21 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 286 steps  22 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 287 steps  23 reward -2 next state  22 agent position  (4, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 288 steps  24 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 289 steps  25 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 290 steps  26 reward -1 next state  22 agent position  (4, 2)\n",
      "max steps reached\n",
      "total rewards -30\n",
      "Episode number:  14\n",
      "enemy states:  [17]\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 291 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 292 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 293 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 294 steps  4 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 295 steps  5 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 296 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 297 steps  7 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 298 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 299 steps  9 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 300 steps  10 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 301 steps  11 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 302 steps  12 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 303 steps  13 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 304 steps  14 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 305 steps  15 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 306 steps  16 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 307 steps  17 reward -1 next state  14 agent position  (2, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 308 steps  18 reward 0 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 309 steps  19 reward 0 next state  18 agent position  (3, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 310 steps  20 reward 10 next state  17 agent position  (3, 2)\n",
      "total rewards 1\n",
      "Episode number:  15\n",
      "enemy states:  [8]\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 311 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 312 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 313 steps  3 reward -1 next state  6 agent position  (1, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 314 steps  4 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 315 steps  5 reward -1 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 316 steps  6 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 317 steps  7 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 318 steps  8 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 319 steps  9 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 320 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 321 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 322 steps  12 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 323 steps  13 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 324 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 325 steps  15 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 326 steps  16 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 327 steps  17 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 328 steps  18 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 329 steps  19 reward -1 next state  11 agent position  (2, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 330 steps  20 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 331 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 332 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 333 steps  23 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 334 steps  24 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 335 steps  25 reward -1 next state  20 agent position  (4, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 336 steps  26 reward -1 next state  15 agent position  (3, 0)\n",
      "max steps reached\n",
      "total rewards -23\n",
      "Episode number:  16\n",
      "enemy states:  [10]\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 337 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 338 steps  2 reward 0 next state  6 agent position  (1, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 339 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 340 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 341 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 342 steps  6 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 343 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 344 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 345 steps  9 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 346 steps  10 reward -2 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.47764288721360454 agent memory len 347 steps  11 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 348 steps  12 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 349 steps  13 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 350 steps  14 reward -1 next state  19 agent position  (3, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 351 steps  15 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 352 steps  16 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 353 steps  17 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 354 steps  18 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 355 steps  19 reward -2 next state  24 agent position  (4, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 356 steps  20 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 357 steps  21 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 358 steps  22 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 359 steps  23 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 360 steps  24 reward -1 next state  24 agent position  (4, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 361 steps  25 reward -1 next state  23 agent position  (4, 3)\n",
      "actions ['U']\n",
      "agent epsilon  0.47764288721360454 agent memory len 362 steps  26 reward -1 next state  18 agent position  (3, 3)\n",
      "max steps reached\n",
      "total rewards -27\n",
      "Episode number:  17\n",
      "enemy states:  [6]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 363 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 364 steps  2 reward 0 next state  2 agent position  (0, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 365 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 366 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 367 steps  5 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 368 steps  6 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 369 steps  7 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 370 steps  8 reward 0 next state  7 agent position  (1, 2)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 371 steps  9 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 372 steps  10 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 373 steps  11 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 374 steps  12 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 375 steps  13 reward -1 next state  16 agent position  (3, 1)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 376 steps  14 reward -1 next state  15 agent position  (3, 0)\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 377 steps  15 reward 0 next state  10 agent position  (2, 0)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 378 steps  16 reward 0 next state  11 agent position  (2, 1)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 379 steps  17 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 380 steps  18 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 381 steps  19 reward 0 next state  12 agent position  (2, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 382 steps  20 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 383 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 384 steps  22 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 385 steps  23 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 386 steps  24 reward -1 next state  18 agent position  (3, 3)\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 387 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 388 steps  26 reward -1 next state  18 agent position  (3, 3)\n",
      "max steps reached\n",
      "total rewards -14\n",
      "Episode number:  18\n",
      "enemy states:  [8]\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 389 steps  1 reward 0 next state  3 agent position  (0, 3)\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 390 steps  2 reward 0 next state  4 agent position  (0, 4)\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 391 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 392 steps  4 reward 0 next state  3 agent position  (0, 3)\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 393 steps  5 reward 10 next state  8 agent position  (1, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-37800a45c274>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mREPLAY_STEPS\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_agents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\Desktop\\new_env\\agent.py\u001b[0m in \u001b[0;36mretrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpirience_replay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1718\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1720\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1721\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1901\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \"\"\"\n\u001b[1;32m-> 1903\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   5067\u001b[0m               type(self._map_func.output_structure)))\n\u001b[0;32m   5068\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5069\u001b[1;33m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[0m\u001b[0;32m   5070\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5071\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2094\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2097\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FlatMapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([32, 32])]\n"
     ]
    }
   ],
   "source": [
    "a = [32 ,32]\n",
    "b = [np.array(a).ravel()]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
