{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "import  numpy as np\n",
    "from    config          import *\n",
    "from    agent           import Agent\n",
    "from    enviroment      import Enviroment\n",
    "from    IPython.display import clear_output\n",
    "from    matplotlib      import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = GRID_SIZE\n",
    "num_col = grid_size\n",
    "\n",
    "possibleActions = POSSIBLE_ACTIONS\n",
    "\n",
    "action_space_dict = {\n",
    "    \"U\" : 0,\n",
    "    \"D\" : 1,\n",
    "    \"L\" : 2,\n",
    "    \"R\" : 3,\n",
    "    \"S\" : 4\n",
    "}\n",
    "n_agents          = N_AGENTS\n",
    "allplayerpos      = PLAYER_POS[: n_agents]\n",
    "enemy_list_pos    = ENEMY_POS[: n_agents]\n",
    "batch_size        = BATCH_SIZE\n",
    "replay_memory_len = REPLAY_MEMORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(state_num):\n",
    "    return int(state_num/num_col), state_num%num_col\n",
    "\n",
    "def state_encode(row,col):\n",
    "    return row*num_col + col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [24]\n",
      "--------------------------------------------\n",
      "P\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\tX\t\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_agents = []\n",
    "for i in range(0, N_AGENTS):\n",
    "    all_agents.append(Agent(i, allplayerpos[i]))\n",
    "\n",
    "\n",
    "initial_states = []\n",
    "for agent in all_agents:\n",
    "    initial_states.append(state_encode(agent.x, agent.y))\n",
    "\n",
    "enemy_states = []\n",
    "for enemy_pos in enemy_list_pos:\n",
    "    enemy_states.append(state_encode(enemy_pos[0], enemy_pos[1]))\n",
    "print(initial_states, enemy_states)\n",
    "env = Enviroment(initial_states = initial_states, enemy_states = enemy_states)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    total_step = 0\n",
    "    rewards_list = []\n",
    "    timesteps_list = []\n",
    "    total_steps = 1\n",
    "    for episode in range(1, EPISODES):\n",
    "        print(\"Episode number: \", episode)\n",
    "\n",
    "        reward_all = 0\n",
    "        time_step = 1\n",
    "        for agent in all_agents:\n",
    "            agent.terminal = False\n",
    "        \n",
    "        [old_map, states, enemy_states] = env.reset()\n",
    "        # print(\"player states: \", states)\n",
    "        for agent in all_agents:\n",
    "            agent.set_pos(allplayerpos[agent.index])\n",
    "\n",
    "        done = [False for _ in range(n_agents)]\n",
    "        is_terminal = [False for _ in range(n_agents)]\n",
    "\n",
    "        while not all(is_terminal):\n",
    "\n",
    "            print(\" is_terminal\",  is_terminal)\n",
    "\n",
    "            # env.render(clear=True)\n",
    "            actions = []\n",
    "            for agent in all_agents:\n",
    "\n",
    "                actions.append(agent.act(old_map, possibleActions))\n",
    "\n",
    "            new_map, next_states, rewards, done = env.step(actions)\n",
    "\n",
    "            for agent in all_agents:\n",
    "                agent.set_pos(decode_state(next_states[agent.index]))\n",
    "\n",
    "            for agent in all_agents:\n",
    "                agent.store(new_map, rewards[agent.index], \\\n",
    "                done[agent.index], old_map, action_space_dict[actions[agent.index]])\n",
    "\n",
    "                if done[agent.index] == True:\n",
    "                    agent.terminal = True\n",
    "                    is_terminal[agent.index] = True\n",
    "                    print(\"agent reached landmark--------------------------------\", agent.index)\n",
    "\n",
    "            print(\"actions\", actions)\n",
    "\n",
    "            for agent in all_agents:\n",
    "                print(\"agent epsilon \", agent.epsilon, \"agent memory len\",\\\n",
    "                    len(agent.expirience_replay), \"steps \", time_step,\\\n",
    "                    \"reward\", rewards[agent.index], \"next state \", next_states[agent.index], \"agent position \",\\\n",
    "                    agent.return_coordinates())\n",
    "\n",
    "            if time_step >= TIME_STEPS:\n",
    "                print(\"max steps reached\")\n",
    "                break\n",
    "\n",
    "            old_map = new_map\n",
    "\n",
    "            total_step += 1\n",
    "            time_step += 1\n",
    "            total_steps+1\n",
    "            reward_all += sum(rewards)\n",
    "\n",
    "            if all(is_terminal):\n",
    "                print(\"-----------------------------------all agents reached landmark--------------------------------\")\n",
    "                for agent in all_agents:\n",
    "                    agent.save_model()\n",
    "        print(\"total rewards\", reward_all)\n",
    "        for agent in all_agents:\n",
    "            agent.retrain(episode)\n",
    "\n",
    "        rewards_list.append(reward_all)\n",
    "        timesteps_list.append(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  1\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 1 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 2 steps  2 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 7 steps  7 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 8 steps  8 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 9 steps  9 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 14 steps  14 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 18 steps  18 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 20 steps  20 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 24 steps  24 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 28 steps  28 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 31 steps  31 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 33 steps  33 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 35 steps  35 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 36 steps  36 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 38 steps  38 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 39 steps  39 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 40 steps  40 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 41 steps  41 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 42 steps  42 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 43 steps  43 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 44 steps  44 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 45 steps  45 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 46 steps  46 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 47 steps  47 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 48 steps  48 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 49 steps  49 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 50 steps  50 reward -1 next state  5 agent position  (1, 0)\n",
      "max steps reached\n",
      "total rewards -57\n",
      "Episode number:  2\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 51 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 52 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 53 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 54 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 55 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 56 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 57 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 58 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 59 steps  9 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 60 steps  10 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards 2\n",
      "Episode number:  3\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 61 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 62 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 63 steps  3 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 64 steps  4 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 65 steps  5 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 66 steps  6 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 67 steps  7 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 68 steps  8 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 69 steps  9 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 70 steps  10 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 71 steps  11 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 72 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 73 steps  13 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 74 steps  14 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 75 steps  15 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 76 steps  16 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 77 steps  17 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 78 steps  18 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 79 steps  19 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 80 steps  20 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 81 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 82 steps  22 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 83 steps  23 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 84 steps  24 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 85 steps  25 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 86 steps  26 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 87 steps  27 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 88 steps  28 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 89 steps  29 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 90 steps  30 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 91 steps  31 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 92 steps  32 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 93 steps  33 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 94 steps  34 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -26\n",
      "Episode number:  4\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 95 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 96 steps  2 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 97 steps  3 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 98 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 99 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 100 steps  6 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 101 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 102 steps  8 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 103 steps  9 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 104 steps  10 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 105 steps  11 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 106 steps  12 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 107 steps  13 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 108 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 109 steps  15 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 110 steps  16 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 111 steps  17 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 112 steps  18 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 113 steps  19 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 114 steps  20 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 115 steps  21 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -10\n",
      "Episode number:  5\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 116 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 117 steps  2 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 118 steps  3 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 119 steps  4 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 120 steps  5 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 121 steps  6 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 122 steps  7 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 123 steps  8 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 124 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 125 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 126 steps  11 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 127 steps  12 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 128 steps  13 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 129 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 130 steps  15 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 131 steps  16 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 132 steps  17 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 133 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 134 steps  19 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 135 steps  20 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 136 steps  21 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 137 steps  22 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 138 steps  23 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 139 steps  24 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 140 steps  25 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 141 steps  26 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 142 steps  27 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 143 steps  28 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 144 steps  29 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 145 steps  30 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 146 steps  31 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 147 steps  32 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 148 steps  33 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 149 steps  34 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 150 steps  35 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 151 steps  36 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 152 steps  37 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 153 steps  38 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 154 steps  39 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 155 steps  40 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 156 steps  41 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 157 steps  42 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 158 steps  43 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 159 steps  44 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 160 steps  45 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 161 steps  46 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 162 steps  47 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 163 steps  48 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 164 steps  49 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 165 steps  50 reward 0 next state  19 agent position  (3, 4)\n",
      "max steps reached\n",
      "total rewards -51\n",
      "Episode number:  6\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 166 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 167 steps  2 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 168 steps  3 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 169 steps  4 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 170 steps  5 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 171 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 172 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 173 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 174 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 175 steps  10 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 176 steps  11 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 177 steps  12 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 178 steps  13 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 179 steps  14 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 180 steps  15 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 181 steps  16 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 182 steps  17 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 183 steps  18 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 184 steps  19 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 185 steps  20 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 186 steps  21 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 187 steps  22 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 188 steps  23 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 189 steps  24 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 190 steps  25 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 191 steps  26 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 192 steps  27 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 193 steps  28 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 194 steps  29 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 195 steps  30 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 196 steps  31 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -24\n",
      "Episode number:  7\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 197 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 198 steps  2 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 199 steps  3 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 200 steps  4 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 201 steps  5 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 202 steps  6 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 203 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 204 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 205 steps  9 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 206 steps  10 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 207 steps  11 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 208 steps  12 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 209 steps  13 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 210 steps  14 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 211 steps  15 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 212 steps  16 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 213 steps  17 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 214 steps  18 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 215 steps  19 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 216 steps  20 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 217 steps  21 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 218 steps  22 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 219 steps  23 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 220 steps  24 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 221 steps  25 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 222 steps  26 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 223 steps  27 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 224 steps  28 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 225 steps  29 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 226 steps  30 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 227 steps  31 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 228 steps  32 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 229 steps  33 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 230 steps  34 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 231 steps  35 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 232 steps  36 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 233 steps  37 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 234 steps  38 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 235 steps  39 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 236 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 237 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 238 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 239 steps  43 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 240 steps  44 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 241 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 242 steps  46 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 243 steps  47 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 244 steps  48 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 245 steps  49 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 246 steps  50 reward -1 next state  21 agent position  (4, 1)\n",
      "max steps reached\n",
      "total rewards -70\n",
      "Episode number:  8\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 247 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 248 steps  2 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 249 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 250 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 251 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 252 steps  6 reward -2 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 253 steps  7 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 254 steps  8 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 255 steps  9 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 256 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 257 steps  11 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 258 steps  12 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 259 steps  13 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 260 steps  14 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 261 steps  15 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 262 steps  16 reward -2 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 263 steps  17 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 264 steps  18 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 265 steps  19 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 266 steps  20 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 267 steps  21 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 268 steps  22 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 269 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 270 steps  24 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 271 steps  25 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 272 steps  26 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 273 steps  27 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 274 steps  28 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 275 steps  29 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 276 steps  30 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 277 steps  31 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 278 steps  32 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 279 steps  33 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 280 steps  34 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 281 steps  35 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 282 steps  36 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 283 steps  37 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 284 steps  38 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 285 steps  39 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 286 steps  40 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -40\n",
      "epsilon  0.6736168455752829\n",
      "Episode number:  9\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 287 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 288 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 289 steps  3 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 290 steps  4 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 291 steps  5 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 292 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 293 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 294 steps  8 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 295 steps  9 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 296 steps  10 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 297 steps  11 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 298 steps  12 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 299 steps  13 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 300 steps  14 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 301 steps  15 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -8\n",
      "epsilon  0.6412518701055556\n",
      "Episode number:  10\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 302 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 303 steps  2 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 304 steps  3 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 305 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 306 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 307 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 308 steps  7 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 309 steps  8 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 310 steps  9 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 311 steps  10 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 312 steps  11 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 313 steps  12 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -1\n",
      "epsilon  0.6104653531155071\n",
      "Episode number:  11\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 314 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 315 steps  2 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 316 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 317 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 318 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 319 steps  6 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 320 steps  7 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 321 steps  8 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards 5\n",
      "epsilon  0.5811803122766818\n",
      "Episode number:  12\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 322 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 323 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 324 steps  3 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 325 steps  4 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 326 steps  5 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 327 steps  6 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 328 steps  7 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 329 steps  8 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 330 steps  9 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 331 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 332 steps  11 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 333 steps  12 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 334 steps  13 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 335 steps  14 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 336 steps  15 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 337 steps  16 reward -2 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 338 steps  17 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -9\n",
      "epsilon  0.5533235197330861\n",
      "Episode number:  13\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 339 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 340 steps  2 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 341 steps  3 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 342 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 343 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 344 steps  6 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 345 steps  7 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 346 steps  8 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 347 steps  9 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 348 steps  10 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 349 steps  11 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 350 steps  12 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 351 steps  13 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -3\n",
      "epsilon  0.5268253189934059\n",
      "Episode number:  14\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 352 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 353 steps  2 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 354 steps  3 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 355 steps  4 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 356 steps  5 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 357 steps  6 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 358 steps  7 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 359 steps  8 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 360 steps  9 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 361 steps  10 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 362 steps  11 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 363 steps  12 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 364 steps  13 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 365 steps  14 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 366 steps  15 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 367 steps  16 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 368 steps  17 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 369 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 370 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 371 steps  20 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 372 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 373 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 374 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 375 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 376 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 377 steps  26 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 378 steps  27 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 379 steps  28 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 380 steps  29 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 381 steps  30 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 382 steps  31 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 383 steps  32 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 384 steps  33 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 385 steps  34 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 386 steps  35 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 387 steps  36 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 388 steps  37 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 389 steps  38 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 390 steps  39 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 391 steps  40 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 392 steps  41 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 393 steps  42 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 394 steps  43 reward -2 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 395 steps  44 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 396 steps  45 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 397 steps  46 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 398 steps  47 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 399 steps  48 reward -2 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 400 steps  49 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 401 steps  50 reward -1 next state  17 agent position  (3, 2)\n",
      "max steps reached\n",
      "total rewards -66\n",
      "epsilon  0.5016194507534953\n",
      "Episode number:  15\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 402 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 403 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 404 steps  3 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 405 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 406 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 407 steps  6 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 408 steps  7 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 409 steps  8 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 410 steps  9 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 411 steps  10 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 412 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 413 steps  12 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 414 steps  13 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 415 steps  14 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 416 steps  15 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 417 steps  16 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 418 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 419 steps  18 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 420 steps  19 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 421 steps  20 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 422 steps  21 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 423 steps  22 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 424 steps  23 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 425 steps  24 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 426 steps  25 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 427 steps  26 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 428 steps  27 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 429 steps  28 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 430 steps  29 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 431 steps  30 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 432 steps  31 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 433 steps  32 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 434 steps  33 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 435 steps  34 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 436 steps  35 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 437 steps  36 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 438 steps  37 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 439 steps  38 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 440 steps  39 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 441 steps  40 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 442 steps  41 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 443 steps  42 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 444 steps  43 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 445 steps  44 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 446 steps  45 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 447 steps  46 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 448 steps  47 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 449 steps  48 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 450 steps  49 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 451 steps  50 reward -2 next state  14 agent position  (2, 4)\n",
      "max steps reached\n",
      "total rewards -72\n",
      "epsilon  0.47764288721360454\n",
      "Episode number:  16\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 452 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 453 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 454 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 455 steps  4 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 456 steps  5 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 457 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 458 steps  7 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 459 steps  8 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 460 steps  9 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 461 steps  10 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 462 steps  11 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 463 steps  12 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards 1\n",
      "epsilon  0.45483567447604933\n",
      "Episode number:  17\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 464 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 465 steps  2 reward -2 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 466 steps  3 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 467 steps  4 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 468 steps  5 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 469 steps  6 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 470 steps  7 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 471 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 472 steps  9 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 473 steps  10 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 474 steps  11 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 475 steps  12 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 476 steps  13 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 477 steps  14 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 478 steps  15 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 479 steps  16 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 480 steps  17 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 481 steps  18 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 482 steps  19 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 483 steps  20 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 484 steps  21 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 485 steps  22 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 486 steps  23 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 487 steps  24 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 488 steps  25 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 489 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 490 steps  27 reward -1 next state  11 agent position  (2, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 491 steps  28 reward -1 next state  12 agent position  (2, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 492 steps  29 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 493 steps  30 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 494 steps  31 reward -2 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 495 steps  32 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 496 steps  33 reward -2 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 497 steps  34 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 498 steps  35 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 499 steps  36 reward -2 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 500 steps  37 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -37\n",
      "epsilon  0.4331407826292394\n",
      "Episode number:  18\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 501 steps  1 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 502 steps  2 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 503 steps  3 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 504 steps  4 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 505 steps  5 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 506 steps  6 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 507 steps  7 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 508 steps  8 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 509 steps  9 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 510 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 511 steps  11 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 512 steps  12 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 513 steps  13 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 514 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 515 steps  15 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 516 steps  16 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 517 steps  17 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 518 steps  18 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 519 steps  19 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 520 steps  20 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 521 steps  21 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 522 steps  22 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 523 steps  23 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 524 steps  24 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 525 steps  25 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 526 steps  26 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 527 steps  27 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 528 steps  28 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 529 steps  29 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 530 steps  30 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 531 steps  31 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 532 steps  32 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 533 steps  33 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 534 steps  34 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 535 steps  35 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 536 steps  36 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 537 steps  37 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 538 steps  38 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 539 steps  39 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 540 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 541 steps  41 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 542 steps  42 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 543 steps  43 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 544 steps  44 reward -1 next state  22 agent position  (4, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 545 steps  45 reward -1 next state  17 agent position  (3, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 546 steps  46 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 547 steps  47 reward 0 next state  23 agent position  (4, 3)\n",
      " is_terminal [False]\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['R']\n",
      "agent epsilon  0.4331407826292394 agent memory len 548 steps  48 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -63\n",
      "epsilon  0.4125039631431931\n",
      "Episode number:  19\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 549 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 550 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 551 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 552 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 553 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 554 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 555 steps  7 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 556 steps  8 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 557 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 558 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 559 steps  11 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 560 steps  12 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 561 steps  13 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 562 steps  14 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 563 steps  15 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 564 steps  16 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 565 steps  17 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 566 steps  18 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 567 steps  19 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 568 steps  20 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 569 steps  21 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 570 steps  22 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 571 steps  23 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 572 steps  24 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 573 steps  25 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 574 steps  26 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 575 steps  27 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 576 steps  28 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 577 steps  29 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 578 steps  30 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 579 steps  31 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 580 steps  32 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 581 steps  33 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 582 steps  34 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 583 steps  35 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 584 steps  36 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 585 steps  37 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 586 steps  38 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 587 steps  39 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 588 steps  40 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 589 steps  41 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 590 steps  42 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 591 steps  43 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 592 steps  44 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 593 steps  45 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 594 steps  46 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 595 steps  47 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 596 steps  48 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 597 steps  49 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 598 steps  50 reward -1 next state  3 agent position  (0, 3)\n",
      "max steps reached\n",
      "total rewards -76\n",
      "epsilon  0.3928736132199562\n",
      "Episode number:  20\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 599 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 600 steps  2 reward -1 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 601 steps  3 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 602 steps  4 reward -2 next state  10 agent position  (2, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 603 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 604 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 605 steps  7 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 606 steps  8 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 607 steps  9 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 608 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 609 steps  11 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 610 steps  12 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 611 steps  13 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 612 steps  14 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 613 steps  15 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 614 steps  16 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 615 steps  17 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 616 steps  18 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 617 steps  19 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 618 steps  20 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 619 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 620 steps  22 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 621 steps  23 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 622 steps  24 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 623 steps  25 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 624 steps  26 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 625 steps  27 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 626 steps  28 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 627 steps  29 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 628 steps  30 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 629 steps  31 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 630 steps  32 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 631 steps  33 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 632 steps  34 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 633 steps  35 reward -2 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 634 steps  36 reward -1 next state  16 agent position  (3, 1)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 635 steps  37 reward -1 next state  21 agent position  (4, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 636 steps  38 reward -1 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 637 steps  39 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 638 steps  40 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 639 steps  41 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 640 steps  42 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 641 steps  43 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 642 steps  44 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 643 steps  45 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 644 steps  46 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 645 steps  47 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 646 steps  48 reward -2 next state  20 agent position  (4, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 647 steps  49 reward -1 next state  15 agent position  (3, 0)\n",
      " is_terminal [False]\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 648 steps  50 reward -1 next state  20 agent position  (4, 0)\n",
      "max steps reached\n",
      "total rewards -87\n",
      "epsilon  0.3742006467597279\n",
      "Episode number:  21\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 649 steps  1 reward -1 next state  5 agent position  (1, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 650 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3742006467597279 agent memory len 651 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 652 steps  4 reward -2 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3742006467597279 agent memory len 653 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 654 steps  6 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 655 steps  7 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 656 steps  8 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 657 steps  9 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 658 steps  10 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 659 steps  11 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3742006467597279 agent memory len 660 steps  12 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 661 steps  13 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 662 steps  14 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3742006467597279 agent memory len 663 steps  15 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 664 steps  16 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 665 steps  17 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 666 steps  18 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 667 steps  19 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 668 steps  20 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 669 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 670 steps  22 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 671 steps  23 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 672 steps  24 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3742006467597279 agent memory len 673 steps  25 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 674 steps  26 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 675 steps  27 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 676 steps  28 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3742006467597279 agent memory len 677 steps  29 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 678 steps  30 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 679 steps  31 reward -2 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3742006467597279 agent memory len 680 steps  32 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3742006467597279 agent memory len 681 steps  33 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 682 steps  34 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 683 steps  35 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 684 steps  36 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3742006467597279 agent memory len 685 steps  37 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 686 steps  38 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 687 steps  39 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 688 steps  40 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 689 steps  41 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 690 steps  42 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 691 steps  43 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 692 steps  44 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 693 steps  45 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 694 steps  46 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 695 steps  47 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 696 steps  48 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 697 steps  49 reward -2 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['U']\n",
      "agent epsilon  0.3742006467597279 agent memory len 698 steps  50 reward -2 next state  0 agent position  (0, 0)\n",
      "max steps reached\n",
      "total rewards -84\n",
      "epsilon  0.35643837162004377\n",
      "Episode number:  22\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 699 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 700 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 701 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.35643837162004377 agent memory len 702 steps  4 reward -2 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 703 steps  5 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 704 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 705 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.35643837162004377 agent memory len 706 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 707 steps  9 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 708 steps  10 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 709 steps  11 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 710 steps  12 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.35643837162004377 agent memory len 711 steps  13 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 712 steps  14 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 713 steps  15 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 714 steps  16 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 715 steps  17 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 716 steps  18 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 717 steps  19 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 718 steps  20 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 719 steps  21 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 720 steps  22 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 721 steps  23 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 722 steps  24 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 723 steps  25 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 724 steps  26 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 725 steps  27 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 726 steps  28 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 727 steps  29 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 728 steps  30 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 729 steps  31 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 730 steps  32 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 731 steps  33 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.35643837162004377 agent memory len 732 steps  34 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 733 steps  35 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 734 steps  36 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 735 steps  37 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 736 steps  38 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 737 steps  39 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 738 steps  40 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 739 steps  41 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "landmark captured 0\n",
      "agent reached landmark-------------------------------- 0\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 740 steps  42 reward 10 next state  24 agent position  (4, 4)\n",
      "-----------------------------------all agents reached landmark--------------------------------\n",
      "total rewards -62\n",
      "epsilon  0.3395423728610988\n",
      "Episode number:  23\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 741 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 742 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 743 steps  3 reward -1 next state  1 agent position  (0, 1)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 744 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 745 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 746 steps  6 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 747 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 748 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 749 steps  9 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 750 steps  10 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 751 steps  11 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 752 steps  12 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 753 steps  13 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 754 steps  14 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 755 steps  15 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 756 steps  16 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 757 steps  17 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 758 steps  18 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 759 steps  19 reward -1 next state  13 agent position  (2, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3395423728610988 agent memory len 760 steps  20 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 761 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 762 steps  22 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 763 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 764 steps  24 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 765 steps  25 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 766 steps  26 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 767 steps  27 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 768 steps  28 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 769 steps  29 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 770 steps  30 reward 0 next state  18 agent position  (3, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 771 steps  31 reward 0 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 772 steps  32 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 773 steps  33 reward -2 next state  19 agent position  (3, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3395423728610988 agent memory len 774 steps  34 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3395423728610988 agent memory len 775 steps  35 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 776 steps  36 reward -1 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 777 steps  37 reward -2 next state  14 agent position  (2, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3395423728610988 agent memory len 778 steps  38 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 779 steps  39 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 780 steps  40 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 781 steps  41 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 782 steps  42 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 783 steps  43 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 784 steps  44 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 785 steps  45 reward -2 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['L']\n",
      "agent epsilon  0.3395423728610988 agent memory len 786 steps  46 reward -1 next state  8 agent position  (1, 3)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 787 steps  47 reward -1 next state  9 agent position  (1, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['U']\n",
      "agent epsilon  0.3395423728610988 agent memory len 788 steps  48 reward -1 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 789 steps  49 reward -2 next state  4 agent position  (0, 4)\n",
      " is_terminal [False]\n",
      "random action\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 790 steps  50 reward -2 next state  4 agent position  (0, 4)\n",
      "max steps reached\n",
      "total rewards -67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7112c4c02726>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total rewards\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_agents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mrewards_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\Desktop\\new_env\\agent.py\u001b[0m in \u001b[0;36mretrain\u001b[1;34m(self, episode)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m# print(\"targetf\", target_f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# print(\"target\", target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[1;31m# print(\"action\", action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = env.reset_map.reshape(-1, GRID_SIZE, GRID_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x289090e76a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(map[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.enemy_states\n",
    "env.agents_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
