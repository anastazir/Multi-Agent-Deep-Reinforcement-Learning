Episode number:  1
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 1 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 1 steps  1 reward -2 next state  1 agent position  (0, 5)
agent epsilon  1.0 agent memory len 1 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  1.0 agent memory len 2 steps  2 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 2 steps  2 reward -2 next state  1 agent position  (0, 5)
agent epsilon  1.0 agent memory len 2 steps  2 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 3 steps  3 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  1 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 6 steps  6 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 7 steps  7 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 7 steps  7 reward -1 next state  1 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 8 steps  8 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 8 steps  8 reward -1 next state  0 agent position  (1, 6)
agent epsilon  1.0 agent memory len 8 steps  8 reward -1 next state  1 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'D']
agent epsilon  1.0 agent memory len 9 steps  9 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 9 steps  9 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 9 steps  9 reward -1 next state  0 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  1 agent position  (0, 7)
agent epsilon  1.0 agent memory len 10 steps  10 reward -1 next state  0 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 11 steps  11 reward -1 next state  1 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  2 agent position  (0, 7)
agent epsilon  1.0 agent memory len 12 steps  12 reward -1 next state  0 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  2 agent position  (0, 8)
agent epsilon  1.0 agent memory len 13 steps  13 reward -1 next state  0 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 14 steps  14 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 14 steps  14 reward -2 next state  3 agent position  (0, 8)
agent epsilon  1.0 agent memory len 14 steps  14 reward -1 next state  0 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  3 agent position  (1, 8)
agent epsilon  1.0 agent memory len 15 steps  15 reward -1 next state  1 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  4 agent position  (1, 7)
agent epsilon  1.0 agent memory len 16 steps  16 reward -1 next state  1 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  3 agent position  (1, 7)
agent epsilon  1.0 agent memory len 17 steps  17 reward -1 next state  1 agent position  (4, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 18 steps  18 reward -2 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 18 steps  18 reward -1 next state  3 agent position  (0, 7)
agent epsilon  1.0 agent memory len 18 steps  18 reward -1 next state  0 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 19 steps  19 reward -2 next state  3 agent position  (0, 7)
agent epsilon  1.0 agent memory len 19 steps  19 reward -1 next state  0 agent position  (3, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 20 steps  20 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 20 steps  20 reward -1 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 20 steps  20 reward -1 next state  0 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 21 steps  21 reward -1 next state  0 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  5 agent position  (1, 8)
agent epsilon  1.0 agent memory len 22 steps  22 reward -1 next state  1 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  6 agent position  (0, 8)
agent epsilon  1.0 agent memory len 23 steps  23 reward -1 next state  0 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  7 agent position  (0, 9)
agent epsilon  1.0 agent memory len 24 steps  24 reward -1 next state  0 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 25 steps  25 reward -2 next state  7 agent position  (0, 9)
agent epsilon  1.0 agent memory len 25 steps  25 reward -1 next state  0 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 26 steps  26 reward -2 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  7 agent position  (0, 9)
agent epsilon  1.0 agent memory len 26 steps  26 reward -1 next state  0 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  6 agent position  (0, 9)
agent epsilon  1.0 agent memory len 27 steps  27 reward -1 next state  0 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'U']
agent epsilon  1.0 agent memory len 28 steps  28 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 28 steps  28 reward -1 next state  6 agent position  (1, 9)
agent epsilon  1.0 agent memory len 28 steps  28 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  6 agent position  (1, 8)
agent epsilon  1.0 agent memory len 29 steps  29 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'L']
agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  6 agent position  (2, 8)
agent epsilon  1.0 agent memory len 30 steps  30 reward -1 next state  2 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  1.0 agent memory len 31 steps  31 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 31 steps  31 reward -1 next state  6 agent position  (2, 7)
agent epsilon  1.0 agent memory len 31 steps  31 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  6 agent position  (2, 8)
agent epsilon  1.0 agent memory len 32 steps  32 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  1.0 agent memory len 33 steps  33 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 33 steps  33 reward -1 next state  6 agent position  (1, 8)
agent epsilon  1.0 agent memory len 33 steps  33 reward -1 next state  1 agent position  (3, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  7 agent position  (1, 8)
agent epsilon  1.0 agent memory len 34 steps  34 reward -1 next state  1 agent position  (4, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  1.0 agent memory len 35 steps  35 reward -2 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 35 steps  35 reward -1 next state  7 agent position  (1, 7)
agent epsilon  1.0 agent memory len 35 steps  35 reward -1 next state  1 agent position  (4, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 36 steps  36 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 36 steps  36 reward -1 next state  7 agent position  (0, 7)
agent epsilon  1.0 agent memory len 36 steps  36 reward -1 next state  0 agent position  (5, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  7 agent position  (0, 6)
agent epsilon  1.0 agent memory len 37 steps  37 reward -1 next state  0 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 38 steps  38 reward -1 next state  0 agent position  (0, 8)
agent epsilon  1.0 agent memory len 38 steps  38 reward -2 next state  8 agent position  (0, 6)
agent epsilon  1.0 agent memory len 38 steps  38 reward 0 next state  0 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  1.0 agent memory len 39 steps  39 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 39 steps  39 reward -1 next state  9 agent position  (0, 5)
agent epsilon  1.0 agent memory len 39 steps  39 reward -1 next state  0 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'R']
agent epsilon  1.0 agent memory len 40 steps  40 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 40 steps  40 reward -1 next state  9 agent position  (0, 4)
agent epsilon  1.0 agent memory len 40 steps  40 reward -1 next state  0 agent position  (5, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 41 steps  41 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 41 steps  41 reward -1 next state  9 agent position  (0, 4)
agent epsilon  1.0 agent memory len 41 steps  41 reward -1 next state  0 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 42 steps  42 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 42 steps  42 reward -1 next state  9 agent position  (0, 5)
agent epsilon  1.0 agent memory len 42 steps  42 reward -1 next state  0 agent position  (4, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  1.0 agent memory len 43 steps  43 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 43 steps  43 reward -2 next state  9 agent position  (0, 5)
agent epsilon  1.0 agent memory len 43 steps  43 reward -1 next state  0 agent position  (4, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  1.0 agent memory len 44 steps  44 reward -2 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 44 steps  44 reward -1 next state  9 agent position  (0, 4)
agent epsilon  1.0 agent memory len 44 steps  44 reward -1 next state  0 agent position  (3, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  1.0 agent memory len 45 steps  45 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 45 steps  45 reward -2 next state  9 agent position  (0, 4)
agent epsilon  1.0 agent memory len 45 steps  45 reward -1 next state  0 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 46 steps  46 reward -1 next state  2 agent position  (2, 9)
agent epsilon  1.0 agent memory len 46 steps  46 reward -1 next state  9 agent position  (0, 3)
agent epsilon  1.0 agent memory len 46 steps  46 reward -1 next state  0 agent position  (4, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 47 steps  47 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 47 steps  47 reward -1 next state  8 agent position  (0, 3)
agent epsilon  1.0 agent memory len 47 steps  47 reward -1 next state  0 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 48 steps  48 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 48 steps  48 reward -1 next state  7 agent position  (0, 2)
agent epsilon  1.0 agent memory len 48 steps  48 reward -1 next state  0 agent position  (5, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 49 steps  49 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 49 steps  49 reward -1 next state  7 agent position  (0, 3)
agent epsilon  1.0 agent memory len 49 steps  49 reward -1 next state  0 agent position  (4, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  1.0 agent memory len 50 steps  50 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 50 steps  50 reward -2 next state  7 agent position  (0, 3)
agent epsilon  1.0 agent memory len 50 steps  50 reward -1 next state  0 agent position  (4, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'D']
agent epsilon  1.0 agent memory len 51 steps  51 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 51 steps  51 reward -2 next state  7 agent position  (0, 3)
agent epsilon  1.0 agent memory len 51 steps  51 reward -1 next state  0 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  1.0 agent memory len 52 steps  52 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 52 steps  52 reward -2 next state  7 agent position  (0, 3)
agent epsilon  1.0 agent memory len 52 steps  52 reward -1 next state  0 agent position  (5, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 53 steps  53 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 53 steps  53 reward -1 next state  6 agent position  (0, 2)
agent epsilon  1.0 agent memory len 53 steps  53 reward 0 next state  0 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 54 steps  54 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 54 steps  54 reward -1 next state  7 agent position  (1, 2)
agent epsilon  1.0 agent memory len 54 steps  54 reward 0 next state  1 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 55 steps  55 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 55 steps  55 reward -1 next state  6 agent position  (2, 2)
agent epsilon  1.0 agent memory len 55 steps  55 reward 0 next state  2 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  1.0 agent memory len 56 steps  56 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 56 steps  56 reward -1 next state  6 agent position  (2, 3)
agent epsilon  1.0 agent memory len 56 steps  56 reward 0 next state  2 agent position  (6, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 57 steps  57 reward -2 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 57 steps  57 reward -1 next state  6 agent position  (2, 3)
agent epsilon  1.0 agent memory len 57 steps  57 reward -1 next state  2 agent position  (5, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 58 steps  58 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 58 steps  58 reward -1 next state  6 agent position  (2, 3)
agent epsilon  1.0 agent memory len 58 steps  58 reward -1 next state  2 agent position  (5, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 59 steps  59 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 59 steps  59 reward -1 next state  5 agent position  (2, 2)
agent epsilon  1.0 agent memory len 59 steps  59 reward -1 next state  2 agent position  (5, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 60 steps  60 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 60 steps  60 reward -1 next state  6 agent position  (3, 2)
agent epsilon  1.0 agent memory len 60 steps  60 reward -1 next state  3 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  1.0 agent memory len 61 steps  61 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 61 steps  61 reward -1 next state  7 agent position  (4, 2)
agent epsilon  1.0 agent memory len 61 steps  61 reward 0 next state  4 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  1.0 agent memory len 62 steps  62 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 62 steps  62 reward -1 next state  7 agent position  (5, 2)
agent epsilon  1.0 agent memory len 62 steps  62 reward 0 next state  5 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['S', 'R', 'D']
agent epsilon  1.0 agent memory len 63 steps  63 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 63 steps  63 reward -1 next state  7 agent position  (5, 3)
agent epsilon  1.0 agent memory len 63 steps  63 reward 10 next state  5 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 64 steps  64 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 64 steps  64 reward -1 next state  8 agent position  (6, 3)
agent epsilon  1.0 agent memory len 63 steps  64 reward -1 next state  6 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 65 steps  65 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 65 steps  65 reward -1 next state  8 agent position  (5, 3)
agent epsilon  1.0 agent memory len 63 steps  65 reward -1 next state  5 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 66 steps  66 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 66 steps  66 reward -1 next state  7 agent position  (5, 2)
agent epsilon  1.0 agent memory len 63 steps  66 reward -1 next state  5 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 67 steps  67 reward -1 next state  3 agent position  (3, 7)
agent epsilon  1.0 agent memory len 67 steps  67 reward -1 next state  7 agent position  (5, 3)
agent epsilon  1.0 agent memory len 63 steps  67 reward -1 next state  5 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 68 steps  68 reward -1 next state  4 agent position  (4, 7)
agent epsilon  1.0 agent memory len 68 steps  68 reward -1 next state  7 agent position  (5, 3)
agent epsilon  1.0 agent memory len 63 steps  68 reward -1 next state  5 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'U', 'S']
agent epsilon  1.0 agent memory len 69 steps  69 reward -1 next state  3 agent position  (3, 7)
agent epsilon  1.0 agent memory len 69 steps  69 reward -1 next state  7 agent position  (4, 3)
agent epsilon  1.0 agent memory len 63 steps  69 reward -1 next state  4 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  1.0 agent memory len 70 steps  70 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 70 steps  70 reward -1 next state  7 agent position  (4, 2)
agent epsilon  1.0 agent memory len 63 steps  70 reward -1 next state  4 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 71 steps  71 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 71 steps  71 reward -1 next state  7 agent position  (3, 2)
agent epsilon  1.0 agent memory len 63 steps  71 reward -1 next state  3 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 72 steps  72 reward -1 next state  3 agent position  (3, 7)
agent epsilon  1.0 agent memory len 72 steps  72 reward -1 next state  7 agent position  (3, 3)
agent epsilon  1.0 agent memory len 63 steps  72 reward -1 next state  3 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 73 steps  73 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 73 steps  73 reward -1 next state  7 agent position  (3, 3)
agent epsilon  1.0 agent memory len 63 steps  73 reward -1 next state  3 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'U', 'S']
agent epsilon  1.0 agent memory len 74 steps  74 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 74 steps  74 reward -1 next state  8 agent position  (2, 3)
agent epsilon  1.0 agent memory len 63 steps  74 reward -1 next state  2 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 75 steps  75 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 75 steps  75 reward -1 next state  7 agent position  (2, 3)
agent epsilon  1.0 agent memory len 63 steps  75 reward -1 next state  2 agent position  (7, 4)
max steps reached
total rewards -226
Episode number:  2
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 76 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 76 steps  1 reward -2 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 64 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 77 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 77 steps  2 reward -1 next state  1 agent position  (0, 5)
agent epsilon  1.0 agent memory len 65 steps  2 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  1.0 agent memory len 78 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 78 steps  3 reward -2 next state  2 agent position  (0, 5)
agent epsilon  1.0 agent memory len 66 steps  3 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'S']
agent epsilon  1.0 agent memory len 79 steps  4 reward -2 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 79 steps  4 reward -1 next state  2 agent position  (1, 5)
agent epsilon  1.0 agent memory len 67 steps  4 reward -1 next state  1 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'U']
agent epsilon  1.0 agent memory len 80 steps  5 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 80 steps  5 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 68 steps  5 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 81 steps  6 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 81 steps  6 reward -1 next state  1 agent position  (0, 4)
agent epsilon  1.0 agent memory len 69 steps  6 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  1.0 agent memory len 82 steps  7 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 82 steps  7 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 70 steps  7 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 83 steps  8 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 83 steps  8 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 71 steps  8 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  1.0 agent memory len 84 steps  9 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 84 steps  9 reward -2 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 72 steps  9 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 85 steps  10 reward -2 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 85 steps  10 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 73 steps  10 reward -1 next state  0 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 86 steps  11 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 86 steps  11 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 74 steps  11 reward -1 next state  0 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 87 steps  12 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 87 steps  12 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 75 steps  12 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'L']
agent epsilon  1.0 agent memory len 88 steps  13 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 88 steps  13 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 76 steps  13 reward -1 next state  1 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 89 steps  14 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 89 steps  14 reward -1 next state  0 agent position  (2, 5)
agent epsilon  1.0 agent memory len 77 steps  14 reward -1 next state  2 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'R']
agent epsilon  1.0 agent memory len 90 steps  15 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 90 steps  15 reward -1 next state  0 agent position  (1, 5)
agent epsilon  1.0 agent memory len 78 steps  15 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  1.0 agent memory len 91 steps  16 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 91 steps  16 reward -1 next state  0 agent position  (1, 6)
agent epsilon  1.0 agent memory len 79 steps  16 reward -1 next state  1 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 92 steps  17 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 92 steps  17 reward -1 next state  0 agent position  (2, 6)
agent epsilon  1.0 agent memory len 80 steps  17 reward -1 next state  2 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  1.0 agent memory len 93 steps  18 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 93 steps  18 reward -1 next state  1 agent position  (2, 7)
agent epsilon  1.0 agent memory len 81 steps  18 reward -1 next state  2 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 94 steps  19 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 94 steps  19 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 82 steps  19 reward -1 next state  2 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  1.0 agent memory len 95 steps  20 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 95 steps  20 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 83 steps  20 reward -1 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  1.0 agent memory len 96 steps  21 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 96 steps  21 reward -1 next state  2 agent position  (3, 9)
agent epsilon  1.0 agent memory len 84 steps  21 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 97 steps  22 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 97 steps  22 reward -1 next state  2 agent position  (3, 9)
agent epsilon  1.0 agent memory len 85 steps  22 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 98 steps  23 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 98 steps  23 reward -1 next state  3 agent position  (3, 9)
agent epsilon  1.0 agent memory len 86 steps  23 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 99 steps  24 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 99 steps  24 reward -1 next state  3 agent position  (4, 9)
agent epsilon  1.0 agent memory len 87 steps  24 reward -1 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 100 steps  25 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 100 steps  25 reward -1 next state  2 agent position  (4, 9)
agent epsilon  1.0 agent memory len 88 steps  25 reward -1 next state  4 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 101 steps  26 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 101 steps  26 reward -1 next state  2 agent position  (3, 9)
agent epsilon  1.0 agent memory len 89 steps  26 reward -1 next state  3 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'R']
agent epsilon  1.0 agent memory len 102 steps  27 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 102 steps  27 reward -1 next state  1 agent position  (2, 9)
agent epsilon  1.0 agent memory len 90 steps  27 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  1.0 agent memory len 103 steps  28 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 103 steps  28 reward -1 next state  0 agent position  (2, 8)
agent epsilon  1.0 agent memory len 91 steps  28 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 104 steps  29 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 104 steps  29 reward -1 next state  1 agent position  (2, 8)
agent epsilon  1.0 agent memory len 92 steps  29 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  1.0 agent memory len 105 steps  30 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 105 steps  30 reward -1 next state  0 agent position  (1, 8)
agent epsilon  1.0 agent memory len 93 steps  30 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 106 steps  31 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 106 steps  31 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 94 steps  31 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 107 steps  32 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 107 steps  32 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 95 steps  32 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  1.0 agent memory len 108 steps  33 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 108 steps  33 reward -1 next state  2 agent position  (1, 8)
agent epsilon  1.0 agent memory len 96 steps  33 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 109 steps  34 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 109 steps  34 reward -1 next state  2 agent position  (1, 8)
agent epsilon  1.0 agent memory len 97 steps  34 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 110 steps  35 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 110 steps  35 reward -1 next state  2 agent position  (1, 7)
agent epsilon  1.0 agent memory len 98 steps  35 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 111 steps  36 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 111 steps  36 reward -1 next state  2 agent position  (1, 7)
agent epsilon  1.0 agent memory len 99 steps  36 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  1.0 agent memory len 112 steps  37 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 112 steps  37 reward -1 next state  1 agent position  (2, 7)
agent epsilon  1.0 agent memory len 100 steps  37 reward -1 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 113 steps  38 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 113 steps  38 reward -1 next state  1 agent position  (3, 7)
agent epsilon  1.0 agent memory len 101 steps  38 reward -1 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  1.0 agent memory len 114 steps  39 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 114 steps  39 reward -1 next state  0 agent position  (3, 8)
agent epsilon  1.0 agent memory len 102 steps  39 reward -1 next state  3 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  1.0 agent memory len 115 steps  40 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 115 steps  40 reward -1 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 103 steps  40 reward -1 next state  3 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  1.0 agent memory len 116 steps  41 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 116 steps  41 reward -1 next state  0 agent position  (3, 8)
agent epsilon  1.0 agent memory len 104 steps  41 reward -1 next state  3 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  1.0 agent memory len 117 steps  42 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 117 steps  42 reward -1 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 105 steps  42 reward -1 next state  3 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  1.0 agent memory len 118 steps  43 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 118 steps  43 reward -1 next state  1 agent position  (4, 9)
agent epsilon  1.0 agent memory len 106 steps  43 reward -1 next state  4 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  1.0 agent memory len 119 steps  44 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 119 steps  44 reward -2 next state  1 agent position  (4, 9)
agent epsilon  1.0 agent memory len 107 steps  44 reward -1 next state  4 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 120 steps  45 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 120 steps  45 reward -1 next state  0 agent position  (4, 8)
agent epsilon  1.0 agent memory len 108 steps  45 reward -1 next state  4 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 121 steps  46 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 121 steps  46 reward -1 next state  0 agent position  (4, 7)
agent epsilon  1.0 agent memory len 109 steps  46 reward -1 next state  4 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  1.0 agent memory len 122 steps  47 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 122 steps  47 reward -1 next state  1 agent position  (5, 7)
agent epsilon  1.0 agent memory len 110 steps  47 reward -1 next state  5 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 123 steps  48 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 123 steps  48 reward -1 next state  1 agent position  (5, 7)
agent epsilon  1.0 agent memory len 111 steps  48 reward -1 next state  5 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'S']
agent epsilon  1.0 agent memory len 124 steps  49 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 124 steps  49 reward -1 next state  2 agent position  (4, 7)
agent epsilon  1.0 agent memory len 112 steps  49 reward -1 next state  4 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'D']
agent epsilon  1.0 agent memory len 125 steps  50 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 125 steps  50 reward -1 next state  2 agent position  (5, 7)
agent epsilon  1.0 agent memory len 113 steps  50 reward -1 next state  5 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 126 steps  51 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 126 steps  51 reward -1 next state  2 agent position  (5, 8)
agent epsilon  1.0 agent memory len 114 steps  51 reward -1 next state  5 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 127 steps  52 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 127 steps  52 reward -1 next state  3 agent position  (5, 8)
agent epsilon  1.0 agent memory len 115 steps  52 reward -1 next state  5 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 128 steps  53 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 128 steps  53 reward -1 next state  3 agent position  (5, 8)
agent epsilon  1.0 agent memory len 116 steps  53 reward -1 next state  5 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'S']
agent epsilon  1.0 agent memory len 129 steps  54 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 129 steps  54 reward -1 next state  3 agent position  (5, 7)
agent epsilon  1.0 agent memory len 117 steps  54 reward -1 next state  5 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  1.0 agent memory len 130 steps  55 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 130 steps  55 reward -1 next state  2 agent position  (5, 8)
agent epsilon  1.0 agent memory len 118 steps  55 reward -1 next state  5 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 131 steps  56 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 131 steps  56 reward -1 next state  2 agent position  (5, 7)
agent epsilon  1.0 agent memory len 119 steps  56 reward -1 next state  5 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'L']
agent epsilon  1.0 agent memory len 132 steps  57 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 132 steps  57 reward -1 next state  1 agent position  (5, 8)
agent epsilon  1.0 agent memory len 120 steps  57 reward -1 next state  5 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  1.0 agent memory len 133 steps  58 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 133 steps  58 reward -1 next state  1 agent position  (4, 8)
agent epsilon  1.0 agent memory len 121 steps  58 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  1.0 agent memory len 134 steps  59 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 134 steps  59 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 122 steps  59 reward -1 next state  4 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 135 steps  60 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 135 steps  60 reward -1 next state  0 agent position  (4, 8)
agent epsilon  1.0 agent memory len 123 steps  60 reward -1 next state  4 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 136 steps  61 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 136 steps  61 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 124 steps  61 reward -1 next state  4 agent position  (3, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  1.0 agent memory len 137 steps  62 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 137 steps  62 reward -1 next state  0 agent position  (4, 8)
agent epsilon  1.0 agent memory len 125 steps  62 reward -1 next state  4 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  1.0 agent memory len 138 steps  63 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 138 steps  63 reward -1 next state  0 agent position  (3, 8)
agent epsilon  1.0 agent memory len 126 steps  63 reward -1 next state  3 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  1.0 agent memory len 139 steps  64 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 139 steps  64 reward -1 next state  1 agent position  (3, 9)
agent epsilon  1.0 agent memory len 127 steps  64 reward -1 next state  3 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'S']
agent epsilon  1.0 agent memory len 140 steps  65 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 140 steps  65 reward -1 next state  2 agent position  (2, 9)
agent epsilon  1.0 agent memory len 128 steps  65 reward -1 next state  2 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 141 steps  66 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 141 steps  66 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 129 steps  66 reward -1 next state  2 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  1.0 agent memory len 142 steps  67 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 142 steps  67 reward -1 next state  1 agent position  (2, 7)
agent epsilon  1.0 agent memory len 130 steps  67 reward -1 next state  2 agent position  (2, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 143 steps  68 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 143 steps  68 reward -1 next state  0 agent position  (2, 7)
agent epsilon  1.0 agent memory len 131 steps  68 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  1.0 agent memory len 144 steps  69 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 144 steps  69 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 132 steps  69 reward -1 next state  1 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  1.0 agent memory len 145 steps  70 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 145 steps  70 reward -1 next state  0 agent position  (1, 8)
agent epsilon  1.0 agent memory len 133 steps  70 reward -1 next state  1 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 146 steps  71 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 146 steps  71 reward -1 next state  0 agent position  (1, 7)
agent epsilon  1.0 agent memory len 134 steps  71 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  1.0 agent memory len 147 steps  72 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 147 steps  72 reward -1 next state  1 agent position  (0, 7)
agent epsilon  1.0 agent memory len 135 steps  72 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 148 steps  73 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 148 steps  73 reward -1 next state  1 agent position  (0, 6)
agent epsilon  1.0 agent memory len 136 steps  73 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 149 steps  74 reward -2 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 149 steps  74 reward -1 next state  1 agent position  (0, 6)
agent epsilon  1.0 agent memory len 137 steps  74 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 150 steps  75 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 150 steps  75 reward -1 next state  1 agent position  (0, 6)
agent epsilon  1.0 agent memory len 138 steps  75 reward -1 next state  0 agent position  (1, 7)
max steps reached
total rewards -240
Episode number:  3
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 151 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 151 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 139 steps  1 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 152 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 152 steps  2 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 140 steps  2 reward -2 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 153 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 153 steps  3 reward -1 next state  0 agent position  (1, 4)
agent epsilon  1.0 agent memory len 141 steps  3 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 154 steps  4 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 154 steps  4 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 142 steps  4 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  1.0 agent memory len 155 steps  5 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 155 steps  5 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 143 steps  5 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 156 steps  6 reward -2 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 156 steps  6 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 144 steps  6 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 157 steps  7 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 157 steps  7 reward -2 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 145 steps  7 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 158 steps  8 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 158 steps  8 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 146 steps  8 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 159 steps  9 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 159 steps  9 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 147 steps  9 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'D']
agent epsilon  1.0 agent memory len 160 steps  10 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 160 steps  10 reward -1 next state  0 agent position  (1, 2)
agent epsilon  1.0 agent memory len 148 steps  10 reward -1 next state  1 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  1.0 agent memory len 161 steps  11 reward -2 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 161 steps  11 reward -1 next state  0 agent position  (2, 2)
agent epsilon  1.0 agent memory len 149 steps  11 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  1.0 agent memory len 162 steps  12 reward -2 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 162 steps  12 reward -1 next state  0 agent position  (3, 2)
agent epsilon  1.0 agent memory len 150 steps  12 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'D']
agent epsilon  1.0 agent memory len 163 steps  13 reward -1 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 163 steps  13 reward -1 next state  0 agent position  (3, 2)
agent epsilon  1.0 agent memory len 151 steps  13 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  1.0 agent memory len 164 steps  14 reward -2 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 164 steps  14 reward -1 next state  0 agent position  (3, 3)
agent epsilon  1.0 agent memory len 152 steps  14 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 165 steps  15 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 165 steps  15 reward -1 next state  0 agent position  (3, 4)
agent epsilon  1.0 agent memory len 153 steps  15 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 166 steps  16 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 166 steps  16 reward -1 next state  1 agent position  (3, 4)
agent epsilon  1.0 agent memory len 154 steps  16 reward -1 next state  3 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 167 steps  17 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 167 steps  17 reward -1 next state  0 agent position  (3, 4)
agent epsilon  1.0 agent memory len 155 steps  17 reward -2 next state  3 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  1.0 agent memory len 168 steps  18 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 168 steps  18 reward -1 next state  1 agent position  (2, 4)
agent epsilon  1.0 agent memory len 156 steps  18 reward -1 next state  2 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  1.0 agent memory len 169 steps  19 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 169 steps  19 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 157 steps  19 reward -1 next state  1 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  1.0 agent memory len 170 steps  20 reward -1 next state  4 agent position  (4, 2)
agent epsilon  1.0 agent memory len 170 steps  20 reward -1 next state  2 agent position  (0, 4)
agent epsilon  1.0 agent memory len 158 steps  20 reward -1 next state  0 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'R']
agent epsilon  1.0 agent memory len 171 steps  21 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 171 steps  21 reward -1 next state  2 agent position  (1, 4)
agent epsilon  1.0 agent memory len 159 steps  21 reward -1 next state  1 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'L']
agent epsilon  1.0 agent memory len 172 steps  22 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 172 steps  22 reward -1 next state  3 agent position  (1, 3)
agent epsilon  1.0 agent memory len 160 steps  22 reward -1 next state  1 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  1.0 agent memory len 173 steps  23 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 173 steps  23 reward -1 next state  3 agent position  (1, 3)
agent epsilon  1.0 agent memory len 161 steps  23 reward 0 next state  1 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 174 steps  24 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 174 steps  24 reward -1 next state  2 agent position  (1, 3)
agent epsilon  1.0 agent memory len 162 steps  24 reward 0 next state  1 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'U']
agent epsilon  1.0 agent memory len 175 steps  25 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 175 steps  25 reward -1 next state  2 agent position  (1, 4)
agent epsilon  1.0 agent memory len 163 steps  25 reward -1 next state  1 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 176 steps  26 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 176 steps  26 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 164 steps  26 reward -1 next state  2 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 177 steps  27 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 177 steps  27 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 165 steps  27 reward -1 next state  2 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  1.0 agent memory len 178 steps  28 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 178 steps  28 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 166 steps  28 reward -1 next state  2 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  1.0 agent memory len 179 steps  29 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 179 steps  29 reward -1 next state  2 agent position  (1, 4)
agent epsilon  1.0 agent memory len 167 steps  29 reward -1 next state  1 agent position  (5, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 180 steps  30 reward -2 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 180 steps  30 reward -1 next state  2 agent position  (1, 5)
agent epsilon  1.0 agent memory len 168 steps  30 reward -1 next state  1 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  1.0 agent memory len 181 steps  31 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 181 steps  31 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 169 steps  31 reward -1 next state  1 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 182 steps  32 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 182 steps  32 reward -1 next state  1 agent position  (0, 4)
agent epsilon  1.0 agent memory len 170 steps  32 reward -1 next state  0 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  1.0 agent memory len 183 steps  33 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 183 steps  33 reward -2 next state  2 agent position  (0, 4)
agent epsilon  1.0 agent memory len 171 steps  33 reward -1 next state  0 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  1.0 agent memory len 184 steps  34 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 184 steps  34 reward -1 next state  3 agent position  (0, 5)
agent epsilon  1.0 agent memory len 172 steps  34 reward -1 next state  0 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  1.0 agent memory len 185 steps  35 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 185 steps  35 reward -2 next state  2 agent position  (0, 5)
agent epsilon  1.0 agent memory len 173 steps  35 reward 0 next state  0 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['L', 'L', 'D']
agent epsilon  1.0 agent memory len 186 steps  36 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 186 steps  36 reward -1 next state  1 agent position  (0, 4)
agent epsilon  1.0 agent memory len 174 steps  36 reward 10 next state  0 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 187 steps  37 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 187 steps  37 reward -2 next state  1 agent position  (0, 4)
agent epsilon  1.0 agent memory len 174 steps  37 reward -1 next state  0 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 188 steps  38 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 188 steps  38 reward -1 next state  2 agent position  (1, 4)
agent epsilon  1.0 agent memory len 174 steps  38 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 189 steps  39 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 189 steps  39 reward -1 next state  2 agent position  (1, 5)
agent epsilon  1.0 agent memory len 174 steps  39 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 190 steps  40 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 190 steps  40 reward -1 next state  2 agent position  (1, 6)
agent epsilon  1.0 agent memory len 174 steps  40 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 191 steps  41 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 191 steps  41 reward -1 next state  3 agent position  (1, 6)
agent epsilon  1.0 agent memory len 174 steps  41 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 192 steps  42 reward -2 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 192 steps  42 reward -1 next state  3 agent position  (1, 7)
agent epsilon  1.0 agent memory len 174 steps  42 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  1.0 agent memory len 193 steps  43 reward -2 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 193 steps  43 reward -1 next state  3 agent position  (1, 6)
agent epsilon  1.0 agent memory len 174 steps  43 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'L', 'S']
agent epsilon  1.0 agent memory len 194 steps  44 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 194 steps  44 reward -1 next state  2 agent position  (1, 5)
agent epsilon  1.0 agent memory len 174 steps  44 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 195 steps  45 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 195 steps  45 reward -1 next state  2 agent position  (1, 5)
agent epsilon  1.0 agent memory len 174 steps  45 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 196 steps  46 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 196 steps  46 reward -1 next state  2 agent position  (1, 4)
agent epsilon  1.0 agent memory len 174 steps  46 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 197 steps  47 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 197 steps  47 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 174 steps  47 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 198 steps  48 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 198 steps  48 reward -1 next state  2 agent position  (3, 4)
agent epsilon  1.0 agent memory len 174 steps  48 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 199 steps  49 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 199 steps  49 reward -1 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 174 steps  49 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 200 steps  50 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 200 steps  50 reward -1 next state  1 agent position  (3, 4)
agent epsilon  1.0 agent memory len 174 steps  50 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'D', 'S']
agent epsilon  1.0 agent memory len 201 steps  51 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 201 steps  51 reward -1 next state  1 agent position  (4, 4)
agent epsilon  1.0 agent memory len 174 steps  51 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 202 steps  52 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 202 steps  52 reward -1 next state  0 agent position  (4, 4)
agent epsilon  1.0 agent memory len 174 steps  52 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 203 steps  53 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 203 steps  53 reward -1 next state  0 agent position  (5, 4)
agent epsilon  1.0 agent memory len 174 steps  53 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 204 steps  54 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 204 steps  54 reward -1 next state  0 agent position  (5, 5)
agent epsilon  1.0 agent memory len 174 steps  54 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 205 steps  55 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 205 steps  55 reward -1 next state  0 agent position  (5, 5)
agent epsilon  1.0 agent memory len 174 steps  55 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 206 steps  56 reward -2 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 206 steps  56 reward 0 next state  0 agent position  (6, 5)
agent epsilon  1.0 agent memory len 174 steps  56 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'R', 'S']
agent epsilon  1.0 agent memory len 207 steps  57 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 207 steps  57 reward -1 next state  0 agent position  (6, 6)
agent epsilon  1.0 agent memory len 174 steps  57 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 208 steps  58 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 208 steps  58 reward -1 next state  0 agent position  (6, 6)
agent epsilon  1.0 agent memory len 174 steps  58 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  1.0 agent memory len 209 steps  59 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 209 steps  59 reward -1 next state  0 agent position  (5, 6)
agent epsilon  1.0 agent memory len 174 steps  59 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 210 steps  60 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 210 steps  60 reward -1 next state  1 agent position  (6, 6)
agent epsilon  1.0 agent memory len 174 steps  60 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 211 steps  61 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 211 steps  61 reward -1 next state  0 agent position  (7, 6)
agent epsilon  1.0 agent memory len 174 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  1.0 agent memory len 212 steps  62 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 212 steps  62 reward -1 next state  0 agent position  (8, 6)
agent epsilon  1.0 agent memory len 174 steps  62 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 213 steps  63 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 213 steps  63 reward -1 next state  0 agent position  (9, 6)
agent epsilon  1.0 agent memory len 174 steps  63 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'U', 'S']
agent epsilon  1.0 agent memory len 214 steps  64 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 214 steps  64 reward -1 next state  0 agent position  (8, 6)
agent epsilon  1.0 agent memory len 174 steps  64 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 215 steps  65 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 215 steps  65 reward -1 next state  0 agent position  (7, 6)
agent epsilon  1.0 agent memory len 174 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'R', 'S']
agent epsilon  1.0 agent memory len 216 steps  66 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 216 steps  66 reward -1 next state  0 agent position  (7, 7)
agent epsilon  1.0 agent memory len 174 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 217 steps  67 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 217 steps  67 reward -1 next state  0 agent position  (7, 7)
agent epsilon  1.0 agent memory len 174 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  1.0 agent memory len 218 steps  68 reward -1 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 218 steps  68 reward -1 next state  0 agent position  (8, 7)
agent epsilon  1.0 agent memory len 174 steps  68 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 219 steps  69 reward -2 next state  5 agent position  (5, 0)
agent epsilon  1.0 agent memory len 219 steps  69 reward -1 next state  0 agent position  (9, 7)
agent epsilon  1.0 agent memory len 174 steps  69 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'U', 'S']
agent epsilon  1.0 agent memory len 220 steps  70 reward -1 next state  4 agent position  (4, 0)
agent epsilon  1.0 agent memory len 220 steps  70 reward -1 next state  0 agent position  (8, 7)
agent epsilon  1.0 agent memory len 174 steps  70 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  1.0 agent memory len 221 steps  71 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 221 steps  71 reward -1 next state  1 agent position  (8, 6)
agent epsilon  1.0 agent memory len 174 steps  71 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'R', 'S']
agent epsilon  1.0 agent memory len 222 steps  72 reward -1 next state  4 agent position  (4, 1)
agent epsilon  1.0 agent memory len 222 steps  72 reward -1 next state  1 agent position  (8, 7)
agent epsilon  1.0 agent memory len 174 steps  72 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 223 steps  73 reward -1 next state  5 agent position  (5, 1)
agent epsilon  1.0 agent memory len 223 steps  73 reward -1 next state  1 agent position  (8, 7)
agent epsilon  1.0 agent memory len 174 steps  73 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  1.0 agent memory len 224 steps  74 reward -1 next state  5 agent position  (5, 2)
agent epsilon  1.0 agent memory len 224 steps  74 reward -1 next state  2 agent position  (8, 6)
agent epsilon  1.0 agent memory len 174 steps  74 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  1.0 agent memory len 225 steps  75 reward -1 next state  6 agent position  (6, 2)
agent epsilon  1.0 agent memory len 225 steps  75 reward 0 next state  2 agent position  (8, 5)
agent epsilon  1.0 agent memory len 174 steps  75 reward -1 next state  8 agent position  (8, 8)
max steps reached
total rewards -222
Episode number:  4
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  1.0 agent memory len 226 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 226 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  1.0 agent memory len 175 steps  1 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 227 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 227 steps  2 reward -1 next state  0 agent position  (1, 6)
agent epsilon  1.0 agent memory len 176 steps  2 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 228 steps  3 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 228 steps  3 reward -1 next state  1 agent position  (2, 6)
agent epsilon  1.0 agent memory len 177 steps  3 reward -1 next state  2 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  1.0 agent memory len 229 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 229 steps  4 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 178 steps  4 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  1.0 agent memory len 230 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 230 steps  5 reward -1 next state  0 agent position  (2, 6)
agent epsilon  1.0 agent memory len 179 steps  5 reward -1 next state  2 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  1.0 agent memory len 231 steps  6 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 231 steps  6 reward -1 next state  0 agent position  (3, 6)
agent epsilon  1.0 agent memory len 180 steps  6 reward -1 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'S']
agent epsilon  1.0 agent memory len 232 steps  7 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 232 steps  7 reward -1 next state  0 agent position  (4, 6)
agent epsilon  1.0 agent memory len 181 steps  7 reward -1 next state  4 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  1.0 agent memory len 233 steps  8 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 233 steps  8 reward -1 next state  0 agent position  (4, 5)
agent epsilon  1.0 agent memory len 182 steps  8 reward -1 next state  4 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 234 steps  9 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 234 steps  9 reward -1 next state  1 agent position  (4, 6)
agent epsilon  1.0 agent memory len 183 steps  9 reward -1 next state  4 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  1.0 agent memory len 235 steps  10 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 235 steps  10 reward -1 next state  2 agent position  (3, 6)
agent epsilon  1.0 agent memory len 184 steps  10 reward -2 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  1.0 agent memory len 236 steps  11 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 236 steps  11 reward -1 next state  2 agent position  (3, 5)
agent epsilon  1.0 agent memory len 185 steps  11 reward -2 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  1.0 agent memory len 237 steps  12 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 237 steps  12 reward -1 next state  2 agent position  (3, 6)
agent epsilon  1.0 agent memory len 186 steps  12 reward -1 next state  3 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'R']
agent epsilon  1.0 agent memory len 238 steps  13 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 238 steps  13 reward -1 next state  1 agent position  (4, 6)
agent epsilon  1.0 agent memory len 187 steps  13 reward -1 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  1.0 agent memory len 239 steps  14 reward -1 next state  1 agent position  (1, 1)
agent epsilon  1.0 agent memory len 239 steps  14 reward -1 next state  1 agent position  (4, 5)
agent epsilon  1.0 agent memory len 188 steps  14 reward -1 next state  4 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  1.0 agent memory len 240 steps  15 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 240 steps  15 reward -1 next state  2 agent position  (5, 5)
agent epsilon  1.0 agent memory len 189 steps  15 reward -1 next state  5 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  1.0 agent memory len 241 steps  16 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 241 steps  16 reward -1 next state  2 agent position  (5, 4)
agent epsilon  1.0 agent memory len 190 steps  16 reward -1 next state  5 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 242 steps  17 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 242 steps  17 reward -1 next state  2 agent position  (5, 4)
agent epsilon  1.0 agent memory len 191 steps  17 reward -1 next state  5 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  1.0 agent memory len 243 steps  18 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 243 steps  18 reward -1 next state  2 agent position  (5, 5)
agent epsilon  1.0 agent memory len 192 steps  18 reward -1 next state  5 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 244 steps  19 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 244 steps  19 reward -1 next state  2 agent position  (5, 4)
agent epsilon  1.0 agent memory len 193 steps  19 reward -1 next state  5 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  1.0 agent memory len 245 steps  20 reward -1 next state  0 agent position  (0, 2)
agent epsilon  1.0 agent memory len 245 steps  20 reward -1 next state  2 agent position  (5, 3)
agent epsilon  1.0 agent memory len 194 steps  20 reward -1 next state  5 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'L']
agent epsilon  1.0 agent memory len 246 steps  21 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 246 steps  21 reward -1 next state  2 agent position  (5, 4)
agent epsilon  1.0 agent memory len 195 steps  21 reward -1 next state  5 agent position  (0, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  1.0 agent memory len 247 steps  22 reward -1 next state  1 agent position  (1, 2)
agent epsilon  1.0 agent memory len 247 steps  22 reward -1 next state  2 agent position  (5, 3)
agent epsilon  1.0 agent memory len 196 steps  22 reward -1 next state  5 agent position  (0, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  1.0 agent memory len 248 steps  23 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 248 steps  23 reward -1 next state  3 agent position  (5, 4)
agent epsilon  1.0 agent memory len 197 steps  23 reward -1 next state  5 agent position  (0, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  1.0 agent memory len 249 steps  24 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 249 steps  24 reward -1 next state  3 agent position  (5, 3)
agent epsilon  1.0 agent memory len 198 steps  24 reward -1 next state  5 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  1.0 agent memory len 250 steps  25 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 250 steps  25 reward 0 next state  3 agent position  (6, 3)
agent epsilon  1.0 agent memory len 199 steps  25 reward -1 next state  6 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  1.0 agent memory len 251 steps  26 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 251 steps  26 reward 0 next state  4 agent position  (7, 3)
agent epsilon  1.0 agent memory len 200 steps  26 reward -1 next state  7 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 252 steps  27 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 252 steps  27 reward 0 next state  4 agent position  (7, 3)
agent epsilon  1.0 agent memory len 201 steps  27 reward -1 next state  7 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['U', 'R', 'R']
agent epsilon  1.0 agent memory len 253 steps  28 reward -2 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  28 reward 10 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 202 steps  28 reward -1 next state  7 agent position  (1, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 254 steps  29 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  29 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 203 steps  29 reward -1 next state  7 agent position  (1, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 255 steps  30 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 253 steps  30 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 204 steps  30 reward -1 next state  7 agent position  (1, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 256 steps  31 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 253 steps  31 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 205 steps  31 reward -1 next state  7 agent position  (2, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 257 steps  32 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 253 steps  32 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 206 steps  32 reward -1 next state  7 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  1.0 agent memory len 258 steps  33 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  33 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 207 steps  33 reward -1 next state  7 agent position  (2, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 259 steps  34 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  34 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 208 steps  34 reward -1 next state  7 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 260 steps  35 reward -2 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  35 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 209 steps  35 reward -1 next state  7 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 261 steps  36 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 253 steps  36 reward -1 next state  6 agent position  (7, 4)
agent epsilon  1.0 agent memory len 210 steps  36 reward -1 next state  7 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 262 steps  37 reward -2 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 253 steps  37 reward -1 next state  6 agent position  (7, 4)
agent epsilon  1.0 agent memory len 211 steps  37 reward -1 next state  7 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 263 steps  38 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  38 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 212 steps  38 reward -1 next state  7 agent position  (3, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 264 steps  39 reward -2 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  39 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 213 steps  39 reward -1 next state  7 agent position  (3, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  1.0 agent memory len 265 steps  40 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  40 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 214 steps  40 reward -1 next state  7 agent position  (3, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 266 steps  41 reward -2 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  41 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 215 steps  41 reward -1 next state  7 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  1.0 agent memory len 267 steps  42 reward -2 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  42 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 216 steps  42 reward -1 next state  7 agent position  (3, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 268 steps  43 reward -2 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  43 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 217 steps  43 reward -1 next state  7 agent position  (2, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 269 steps  44 reward -1 next state  0 agent position  (0, 5)
agent epsilon  1.0 agent memory len 253 steps  44 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 218 steps  44 reward -1 next state  7 agent position  (2, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'R']
agent epsilon  1.0 agent memory len 270 steps  45 reward -1 next state  0 agent position  (0, 4)
agent epsilon  1.0 agent memory len 253 steps  45 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 219 steps  45 reward -1 next state  7 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  1.0 agent memory len 271 steps  46 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 253 steps  46 reward -1 next state  3 agent position  (7, 4)
agent epsilon  1.0 agent memory len 220 steps  46 reward -1 next state  7 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  1.0 agent memory len 272 steps  47 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 253 steps  47 reward -1 next state  3 agent position  (7, 4)
agent epsilon  1.0 agent memory len 221 steps  47 reward -1 next state  7 agent position  (1, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  1.0 agent memory len 273 steps  48 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 253 steps  48 reward -1 next state  3 agent position  (7, 4)
agent epsilon  1.0 agent memory len 222 steps  48 reward -1 next state  7 agent position  (1, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 274 steps  49 reward -1 next state  0 agent position  (0, 3)
agent epsilon  1.0 agent memory len 253 steps  49 reward -1 next state  3 agent position  (7, 4)
agent epsilon  1.0 agent memory len 223 steps  49 reward -1 next state  7 agent position  (2, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  1.0 agent memory len 275 steps  50 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 253 steps  50 reward -1 next state  3 agent position  (7, 4)
agent epsilon  1.0 agent memory len 224 steps  50 reward -1 next state  7 agent position  (1, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 276 steps  51 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 253 steps  51 reward -1 next state  4 agent position  (7, 4)
agent epsilon  1.0 agent memory len 225 steps  51 reward -1 next state  7 agent position  (0, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 277 steps  52 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 253 steps  52 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 226 steps  52 reward -1 next state  7 agent position  (1, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 278 steps  53 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 253 steps  53 reward -1 next state  5 agent position  (7, 4)
agent epsilon  1.0 agent memory len 227 steps  53 reward -1 next state  7 agent position  (1, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 279 steps  54 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 253 steps  54 reward -1 next state  6 agent position  (7, 4)
agent epsilon  1.0 agent memory len 228 steps  54 reward -1 next state  7 agent position  (1, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 280 steps  55 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 253 steps  55 reward -1 next state  7 agent position  (7, 4)
agent epsilon  1.0 agent memory len 229 steps  55 reward -1 next state  7 agent position  (2, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 281 steps  56 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  56 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 230 steps  56 reward -1 next state  7 agent position  (3, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  1.0 agent memory len 282 steps  57 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 253 steps  57 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 231 steps  57 reward -1 next state  7 agent position  (3, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  1.0 agent memory len 283 steps  58 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  58 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 232 steps  58 reward -1 next state  7 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  1.0 agent memory len 284 steps  59 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 253 steps  59 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 233 steps  59 reward -1 next state  7 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  1.0 agent memory len 285 steps  60 reward -1 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 253 steps  60 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 234 steps  60 reward -1 next state  7 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'S']
agent epsilon  1.0 agent memory len 286 steps  61 reward -2 next state  1 agent position  (1, 9)
agent epsilon  1.0 agent memory len 253 steps  61 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 235 steps  61 reward -1 next state  7 agent position  (1, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 287 steps  62 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 253 steps  62 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 236 steps  62 reward -1 next state  7 agent position  (2, 3)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'R']
agent epsilon  1.0 agent memory len 288 steps  63 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 253 steps  63 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 237 steps  63 reward -1 next state  7 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  1.0 agent memory len 289 steps  64 reward -1 next state  0 agent position  (0, 8)
agent epsilon  1.0 agent memory len 253 steps  64 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 238 steps  64 reward -1 next state  7 agent position  (2, 4)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 290 steps  65 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  65 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 239 steps  65 reward -1 next state  7 agent position  (2, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  1.0 agent memory len 291 steps  66 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 253 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  1.0 agent memory len 240 steps  66 reward -1 next state  7 agent position  (3, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 292 steps  67 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  67 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 241 steps  67 reward -1 next state  7 agent position  (4, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  1.0 agent memory len 293 steps  68 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 253 steps  68 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 242 steps  68 reward -1 next state  7 agent position  (4, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  1.0 agent memory len 294 steps  69 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  69 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 243 steps  69 reward -1 next state  7 agent position  (4, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  1.0 agent memory len 295 steps  70 reward -1 next state  2 agent position  (2, 8)
agent epsilon  1.0 agent memory len 253 steps  70 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 244 steps  70 reward -1 next state  7 agent position  (4, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 296 steps  71 reward -1 next state  1 agent position  (1, 8)
agent epsilon  1.0 agent memory len 253 steps  71 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 245 steps  71 reward -1 next state  7 agent position  (3, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  1.0 agent memory len 297 steps  72 reward -1 next state  0 agent position  (0, 8)
agent epsilon  1.0 agent memory len 253 steps  72 reward -1 next state  8 agent position  (7, 4)
agent epsilon  1.0 agent memory len 246 steps  72 reward -1 next state  7 agent position  (4, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  1.0 agent memory len 298 steps  73 reward -1 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 253 steps  73 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 247 steps  73 reward -1 next state  7 agent position  (5, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  1.0 agent memory len 299 steps  74 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 253 steps  74 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 248 steps  74 reward -1 next state  7 agent position  (4, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  1.0 agent memory len 300 steps  75 reward -2 next state  0 agent position  (0, 9)
agent epsilon  1.0 agent memory len 253 steps  75 reward -1 next state  9 agent position  (7, 4)
agent epsilon  1.0 agent memory len 249 steps  75 reward -1 next state  7 agent position  (4, 5)
max steps reached
total rewards -222
epsilon  0.820543445547202
Episode number:  5
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.820543445547202 agent memory len 301 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 254 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 250 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'D']
agent epsilon  0.820543445547202 agent memory len 302 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 255 steps  2 reward -2 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 251 steps  2 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'U']
agent epsilon  0.820543445547202 agent memory len 303 steps  3 reward -2 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 256 steps  3 reward -1 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 252 steps  3 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.820543445547202 agent memory len 304 steps  4 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 257 steps  4 reward -2 next state  0 agent position  (0, 7)
agent epsilon  1.0 agent memory len 253 steps  4 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.820543445547202 agent memory len 305 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 258 steps  5 reward -1 next state  0 agent position  (0, 6)
agent epsilon  1.0 agent memory len 254 steps  5 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.820543445547202 agent memory len 306 steps  6 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 259 steps  6 reward -1 next state  0 agent position  (1, 6)
agent epsilon  1.0 agent memory len 255 steps  6 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.820543445547202 agent memory len 307 steps  7 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 260 steps  7 reward -1 next state  0 agent position  (2, 6)
agent epsilon  1.0 agent memory len 256 steps  7 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 308 steps  8 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 261 steps  8 reward -1 next state  0 agent position  (3, 6)
agent epsilon  1.0 agent memory len 257 steps  8 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 309 steps  9 reward -2 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 262 steps  9 reward -1 next state  0 agent position  (4, 6)
agent epsilon  1.0 agent memory len 258 steps  9 reward -1 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 310 steps  10 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 263 steps  10 reward -1 next state  0 agent position  (4, 7)
agent epsilon  1.0 agent memory len 259 steps  10 reward -2 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  0.820543445547202 agent memory len 311 steps  11 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 264 steps  11 reward -1 next state  1 agent position  (4, 8)
agent epsilon  1.0 agent memory len 260 steps  11 reward -2 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.820543445547202 agent memory len 312 steps  12 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 265 steps  12 reward -1 next state  1 agent position  (5, 8)
agent epsilon  1.0 agent memory len 261 steps  12 reward -1 next state  5 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 313 steps  13 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 266 steps  13 reward -1 next state  0 agent position  (5, 9)
agent epsilon  1.0 agent memory len 262 steps  13 reward -1 next state  5 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.820543445547202 agent memory len 314 steps  14 reward -1 next state  0 agent position  (0, 1)
agent epsilon  1.0 agent memory len 267 steps  14 reward -1 next state  1 agent position  (4, 9)
agent epsilon  1.0 agent memory len 263 steps  14 reward -2 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  0.820543445547202 agent memory len 315 steps  15 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 268 steps  15 reward -2 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 264 steps  15 reward -1 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.820543445547202 agent memory len 316 steps  16 reward -1 next state  0 agent position  (0, 0)
agent epsilon  1.0 agent memory len 269 steps  16 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 265 steps  16 reward -2 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.820543445547202 agent memory len 317 steps  17 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 270 steps  17 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 266 steps  17 reward -2 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.820543445547202 agent memory len 318 steps  18 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 271 steps  18 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 267 steps  18 reward -1 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  0.820543445547202 agent memory len 319 steps  19 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 272 steps  19 reward -1 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 268 steps  19 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 320 steps  20 reward -2 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 273 steps  20 reward -2 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 269 steps  20 reward -2 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'S']
agent epsilon  0.820543445547202 agent memory len 321 steps  21 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 274 steps  21 reward -2 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 270 steps  21 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 322 steps  22 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 275 steps  22 reward -2 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 271 steps  22 reward -2 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'U']
agent epsilon  0.820543445547202 agent memory len 323 steps  23 reward -1 next state  1 agent position  (1, 0)
agent epsilon  1.0 agent memory len 276 steps  23 reward -2 next state  0 agent position  (3, 9)
agent epsilon  1.0 agent memory len 272 steps  23 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 324 steps  24 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 277 steps  24 reward -1 next state  0 agent position  (4, 9)
agent epsilon  1.0 agent memory len 273 steps  24 reward -1 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.820543445547202 agent memory len 325 steps  25 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 278 steps  25 reward -1 next state  1 agent position  (4, 8)
agent epsilon  1.0 agent memory len 274 steps  25 reward -2 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  0.820543445547202 agent memory len 326 steps  26 reward -1 next state  2 agent position  (2, 0)
agent epsilon  1.0 agent memory len 279 steps  26 reward -1 next state  0 agent position  (3, 8)
agent epsilon  1.0 agent memory len 275 steps  26 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.820543445547202 agent memory len 327 steps  27 reward -1 next state  2 agent position  (2, 1)
agent epsilon  1.0 agent memory len 280 steps  27 reward -1 next state  1 agent position  (3, 8)
agent epsilon  1.0 agent memory len 276 steps  27 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.820543445547202 agent memory len 328 steps  28 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 281 steps  28 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 277 steps  28 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.820543445547202 agent memory len 329 steps  29 reward -1 next state  3 agent position  (3, 2)
agent epsilon  1.0 agent memory len 282 steps  29 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 278 steps  29 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.820543445547202 agent memory len 330 steps  30 reward -1 next state  2 agent position  (2, 2)
agent epsilon  1.0 agent memory len 283 steps  30 reward -1 next state  2 agent position  (3, 8)
agent epsilon  1.0 agent memory len 279 steps  30 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 331 steps  31 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 284 steps  31 reward -1 next state  3 agent position  (4, 8)
agent epsilon  1.0 agent memory len 280 steps  31 reward -1 next state  4 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.820543445547202 agent memory len 332 steps  32 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 285 steps  32 reward -1 next state  3 agent position  (3, 8)
agent epsilon  1.0 agent memory len 281 steps  32 reward -1 next state  3 agent position  (0, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 333 steps  33 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 286 steps  33 reward -1 next state  4 agent position  (4, 8)
agent epsilon  1.0 agent memory len 282 steps  33 reward -1 next state  4 agent position  (0, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.820543445547202 agent memory len 334 steps  34 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 287 steps  34 reward -1 next state  5 agent position  (3, 8)
agent epsilon  1.0 agent memory len 283 steps  34 reward -1 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 335 steps  35 reward -1 next state  2 agent position  (2, 6)
agent epsilon  1.0 agent memory len 288 steps  35 reward -1 next state  6 agent position  (3, 9)
agent epsilon  1.0 agent memory len 284 steps  35 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.820543445547202 agent memory len 336 steps  36 reward -1 next state  2 agent position  (2, 6)
agent epsilon  1.0 agent memory len 289 steps  36 reward -1 next state  6 agent position  (2, 9)
agent epsilon  1.0 agent memory len 285 steps  36 reward -1 next state  2 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.820543445547202 agent memory len 337 steps  37 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 290 steps  37 reward -1 next state  5 agent position  (1, 9)
agent epsilon  1.0 agent memory len 286 steps  37 reward -2 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  0.820543445547202 agent memory len 338 steps  38 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 291 steps  38 reward -2 next state  4 agent position  (1, 9)
agent epsilon  1.0 agent memory len 287 steps  38 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.820543445547202 agent memory len 339 steps  39 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 292 steps  39 reward -1 next state  5 agent position  (1, 8)
agent epsilon  1.0 agent memory len 288 steps  39 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  0.820543445547202 agent memory len 340 steps  40 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 293 steps  40 reward -1 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 289 steps  40 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.820543445547202 agent memory len 341 steps  41 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 294 steps  41 reward -1 next state  4 agent position  (0, 9)
agent epsilon  1.0 agent memory len 290 steps  41 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.820543445547202 agent memory len 342 steps  42 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 295 steps  42 reward -1 next state  4 agent position  (0, 9)
agent epsilon  1.0 agent memory len 291 steps  42 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.820543445547202 agent memory len 343 steps  43 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 296 steps  43 reward -1 next state  4 agent position  (0, 8)
agent epsilon  1.0 agent memory len 292 steps  43 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'R']
agent epsilon  0.820543445547202 agent memory len 344 steps  44 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 297 steps  44 reward -1 next state  4 agent position  (0, 7)
agent epsilon  1.0 agent memory len 293 steps  44 reward -2 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 345 steps  45 reward -1 next state  1 agent position  (1, 3)
agent epsilon  1.0 agent memory len 298 steps  45 reward -1 next state  3 agent position  (1, 7)
agent epsilon  1.0 agent memory len 294 steps  45 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.820543445547202 agent memory len 346 steps  46 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 299 steps  46 reward -1 next state  4 agent position  (1, 7)
agent epsilon  1.0 agent memory len 295 steps  46 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.820543445547202 agent memory len 347 steps  47 reward -1 next state  1 agent position  (1, 4)
agent epsilon  1.0 agent memory len 300 steps  47 reward -1 next state  4 agent position  (1, 7)
agent epsilon  1.0 agent memory len 296 steps  47 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.820543445547202 agent memory len 348 steps  48 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 301 steps  48 reward -1 next state  5 agent position  (0, 7)
agent epsilon  1.0 agent memory len 297 steps  48 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.820543445547202 agent memory len 349 steps  49 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 302 steps  49 reward -1 next state  5 agent position  (1, 7)
agent epsilon  1.0 agent memory len 298 steps  49 reward -2 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.820543445547202 agent memory len 350 steps  50 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 303 steps  50 reward -1 next state  5 agent position  (1, 6)
agent epsilon  1.0 agent memory len 299 steps  50 reward -1 next state  1 agent position  (2, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.820543445547202 agent memory len 351 steps  51 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 304 steps  51 reward -1 next state  5 agent position  (2, 6)
agent epsilon  1.0 agent memory len 300 steps  51 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.820543445547202 agent memory len 352 steps  52 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 305 steps  52 reward -1 next state  5 agent position  (2, 6)
agent epsilon  1.0 agent memory len 301 steps  52 reward -1 next state  2 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.820543445547202 agent memory len 353 steps  53 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 306 steps  53 reward -1 next state  5 agent position  (3, 6)
agent epsilon  1.0 agent memory len 302 steps  53 reward -1 next state  3 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.820543445547202 agent memory len 354 steps  54 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 307 steps  54 reward -1 next state  4 agent position  (3, 5)
agent epsilon  1.0 agent memory len 303 steps  54 reward -1 next state  3 agent position  (1, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 355 steps  55 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 308 steps  55 reward -1 next state  5 agent position  (4, 5)
agent epsilon  1.0 agent memory len 304 steps  55 reward -1 next state  4 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.820543445547202 agent memory len 356 steps  56 reward -1 next state  3 agent position  (3, 5)
agent epsilon  1.0 agent memory len 309 steps  56 reward -1 next state  5 agent position  (4, 4)
agent epsilon  1.0 agent memory len 305 steps  56 reward -1 next state  4 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.820543445547202 agent memory len 357 steps  57 reward -1 next state  3 agent position  (3, 5)
agent epsilon  1.0 agent memory len 310 steps  57 reward -1 next state  5 agent position  (4, 3)
agent epsilon  1.0 agent memory len 306 steps  57 reward -1 next state  4 agent position  (1, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.820543445547202 agent memory len 358 steps  58 reward -1 next state  3 agent position  (3, 6)
agent epsilon  1.0 agent memory len 311 steps  58 reward -1 next state  6 agent position  (5, 3)
agent epsilon  1.0 agent memory len 307 steps  58 reward -1 next state  5 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.820543445547202 agent memory len 359 steps  59 reward -1 next state  3 agent position  (3, 6)
agent epsilon  1.0 agent memory len 312 steps  59 reward -1 next state  6 agent position  (5, 3)
agent epsilon  1.0 agent memory len 308 steps  59 reward -1 next state  5 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.820543445547202 agent memory len 360 steps  60 reward -1 next state  3 agent position  (3, 7)
agent epsilon  1.0 agent memory len 313 steps  60 reward -1 next state  7 agent position  (4, 3)
agent epsilon  1.0 agent memory len 309 steps  60 reward -1 next state  4 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.820543445547202 agent memory len 361 steps  61 reward -1 next state  2 agent position  (2, 7)
agent epsilon  1.0 agent memory len 314 steps  61 reward -1 next state  7 agent position  (5, 3)
agent epsilon  1.0 agent memory len 310 steps  61 reward -1 next state  5 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'R']
agent epsilon  0.820543445547202 agent memory len 362 steps  62 reward -1 next state  1 agent position  (1, 7)
agent epsilon  1.0 agent memory len 315 steps  62 reward -1 next state  7 agent position  (4, 3)
agent epsilon  1.0 agent memory len 311 steps  62 reward -1 next state  4 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'S']
agent epsilon  0.820543445547202 agent memory len 363 steps  63 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 316 steps  63 reward -1 next state  6 agent position  (3, 3)
agent epsilon  1.0 agent memory len 312 steps  63 reward -1 next state  3 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'U', 'U']
agent epsilon  0.820543445547202 agent memory len 364 steps  64 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 317 steps  64 reward -1 next state  5 agent position  (2, 3)
agent epsilon  1.0 agent memory len 313 steps  64 reward -1 next state  2 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.820543445547202 agent memory len 365 steps  65 reward -1 next state  2 agent position  (2, 5)
agent epsilon  1.0 agent memory len 318 steps  65 reward -1 next state  5 agent position  (2, 3)
agent epsilon  1.0 agent memory len 314 steps  65 reward -1 next state  2 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  0.820543445547202 agent memory len 366 steps  66 reward -1 next state  1 agent position  (1, 5)
agent epsilon  1.0 agent memory len 319 steps  66 reward -1 next state  5 agent position  (2, 2)
agent epsilon  1.0 agent memory len 315 steps  66 reward -1 next state  2 agent position  (3, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.820543445547202 agent memory len 367 steps  67 reward -1 next state  1 agent position  (1, 6)
agent epsilon  1.0 agent memory len 320 steps  67 reward -1 next state  6 agent position  (2, 2)
agent epsilon  1.0 agent memory len 316 steps  67 reward -1 next state  2 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.820543445547202 agent memory len 368 steps  68 reward -1 next state  2 agent position  (2, 6)
agent epsilon  1.0 agent memory len 321 steps  68 reward -1 next state  6 agent position  (2, 1)
agent epsilon  1.0 agent memory len 317 steps  68 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.820543445547202 agent memory len 369 steps  69 reward -1 next state  2 agent position  (2, 6)
agent epsilon  1.0 agent memory len 322 steps  69 reward -1 next state  6 agent position  (3, 1)
agent epsilon  1.0 agent memory len 318 steps  69 reward -1 next state  3 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.820543445547202 agent memory len 370 steps  70 reward -1 next state  3 agent position  (3, 6)
agent epsilon  1.0 agent memory len 323 steps  70 reward -1 next state  6 agent position  (2, 1)
agent epsilon  1.0 agent memory len 319 steps  70 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.820543445547202 agent memory len 371 steps  71 reward -1 next state  3 agent position  (3, 5)
agent epsilon  1.0 agent memory len 324 steps  71 reward -1 next state  5 agent position  (2, 0)
agent epsilon  1.0 agent memory len 320 steps  71 reward -1 next state  2 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.820543445547202 agent memory len 372 steps  72 reward -1 next state  3 agent position  (3, 4)
agent epsilon  1.0 agent memory len 325 steps  72 reward -1 next state  4 agent position  (3, 0)
agent epsilon  1.0 agent memory len 321 steps  72 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.820543445547202 agent memory len 373 steps  73 reward -1 next state  3 agent position  (3, 3)
agent epsilon  1.0 agent memory len 326 steps  73 reward -1 next state  3 agent position  (3, 0)
agent epsilon  1.0 agent memory len 322 steps  73 reward -1 next state  3 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  0.820543445547202 agent memory len 374 steps  74 reward -1 next state  2 agent position  (2, 3)
agent epsilon  1.0 agent memory len 327 steps  74 reward -1 next state  3 agent position  (3, 1)
agent epsilon  1.0 agent memory len 323 steps  74 reward -1 next state  3 agent position  (5, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.820543445547202 agent memory len 375 steps  75 reward -1 next state  2 agent position  (2, 4)
agent epsilon  1.0 agent memory len 328 steps  75 reward -1 next state  4 agent position  (3, 0)
agent epsilon  1.0 agent memory len 324 steps  75 reward -1 next state  3 agent position  (5, 7)
max steps reached
total rewards -253
epsilon  0.7810127752406908
epsilon  0.7810127752406908
epsilon  0.7810127752406908
Episode number:  6
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'U']
agent epsilon  0.7810127752406908 agent memory len 376 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7810127752406908 agent memory len 329 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 325 steps  1 reward -2 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.7810127752406908 agent memory len 377 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 330 steps  2 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 326 steps  2 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7810127752406908 agent memory len 378 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 331 steps  3 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 327 steps  3 reward -2 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'L']
agent epsilon  0.7810127752406908 agent memory len 379 steps  4 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 332 steps  4 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 328 steps  4 reward -1 next state  0 agent position  (0, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 380 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 333 steps  5 reward -2 next state  0 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 329 steps  5 reward -1 next state  0 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 381 steps  6 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 334 steps  6 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.7810127752406908 agent memory len 330 steps  6 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7810127752406908 agent memory len 382 steps  7 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 335 steps  7 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.7810127752406908 agent memory len 331 steps  7 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'U']
agent epsilon  0.7810127752406908 agent memory len 383 steps  8 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 336 steps  8 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 332 steps  8 reward -1 next state  0 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.7810127752406908 agent memory len 384 steps  9 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 337 steps  9 reward -2 next state  1 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 333 steps  9 reward -2 next state  0 agent position  (0, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7810127752406908 agent memory len 385 steps  10 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 338 steps  10 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 334 steps  10 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.7810127752406908 agent memory len 386 steps  11 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 339 steps  11 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 335 steps  11 reward -1 next state  0 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'S']
agent epsilon  0.7810127752406908 agent memory len 387 steps  12 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.7810127752406908 agent memory len 340 steps  12 reward -2 next state  1 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 336 steps  12 reward -1 next state  0 agent position  (1, 6)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 388 steps  13 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7810127752406908 agent memory len 341 steps  13 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 337 steps  13 reward -1 next state  0 agent position  (1, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'R', 'U']
agent epsilon  0.7810127752406908 agent memory len 389 steps  14 reward -2 next state  3 agent position  (3, 0)
agent epsilon  0.7810127752406908 agent memory len 342 steps  14 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7810127752406908 agent memory len 338 steps  14 reward -1 next state  0 agent position  (0, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'L']
agent epsilon  0.7810127752406908 agent memory len 390 steps  15 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 343 steps  15 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.7810127752406908 agent memory len 339 steps  15 reward -1 next state  1 agent position  (0, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 391 steps  16 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7810127752406908 agent memory len 344 steps  16 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.7810127752406908 agent memory len 340 steps  16 reward -1 next state  1 agent position  (0, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 392 steps  17 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7810127752406908 agent memory len 345 steps  17 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.7810127752406908 agent memory len 341 steps  17 reward -1 next state  1 agent position  (1, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 393 steps  18 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.7810127752406908 agent memory len 346 steps  18 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 342 steps  18 reward -1 next state  0 agent position  (1, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 394 steps  19 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.7810127752406908 agent memory len 347 steps  19 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 343 steps  19 reward -1 next state  0 agent position  (2, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.7810127752406908 agent memory len 395 steps  20 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.7810127752406908 agent memory len 348 steps  20 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 344 steps  20 reward -1 next state  0 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.7810127752406908 agent memory len 396 steps  21 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 349 steps  21 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.7810127752406908 agent memory len 345 steps  21 reward -1 next state  1 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'S']
agent epsilon  0.7810127752406908 agent memory len 397 steps  22 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.7810127752406908 agent memory len 350 steps  22 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.7810127752406908 agent memory len 346 steps  22 reward -1 next state  1 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 398 steps  23 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 351 steps  23 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.7810127752406908 agent memory len 347 steps  23 reward -1 next state  1 agent position  (4, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.7810127752406908 agent memory len 399 steps  24 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 352 steps  24 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 348 steps  24 reward -1 next state  1 agent position  (4, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.7810127752406908 agent memory len 400 steps  25 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 353 steps  25 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 349 steps  25 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'D']
agent epsilon  0.7810127752406908 agent memory len 401 steps  26 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 354 steps  26 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 350 steps  26 reward -1 next state  2 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.7810127752406908 agent memory len 402 steps  27 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 355 steps  27 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 351 steps  27 reward -1 next state  2 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 403 steps  28 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 356 steps  28 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 352 steps  28 reward -1 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.7810127752406908 agent memory len 404 steps  29 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 357 steps  29 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 353 steps  29 reward -1 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.7810127752406908 agent memory len 405 steps  30 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.7810127752406908 agent memory len 358 steps  30 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 354 steps  30 reward -1 next state  1 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.7810127752406908 agent memory len 406 steps  31 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7810127752406908 agent memory len 359 steps  31 reward -1 next state  2 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 355 steps  31 reward -1 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 407 steps  32 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7810127752406908 agent memory len 360 steps  32 reward -1 next state  2 agent position  (1, 3)
agent epsilon  0.7810127752406908 agent memory len 356 steps  32 reward -2 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.7810127752406908 agent memory len 408 steps  33 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7810127752406908 agent memory len 361 steps  33 reward -1 next state  2 agent position  (1, 2)
agent epsilon  0.7810127752406908 agent memory len 357 steps  33 reward -1 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.7810127752406908 agent memory len 409 steps  34 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7810127752406908 agent memory len 362 steps  34 reward -1 next state  2 agent position  (1, 1)
agent epsilon  0.7810127752406908 agent memory len 358 steps  34 reward -2 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.7810127752406908 agent memory len 410 steps  35 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.7810127752406908 agent memory len 363 steps  35 reward -1 next state  3 agent position  (1, 1)
agent epsilon  0.7810127752406908 agent memory len 359 steps  35 reward -1 next state  1 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 411 steps  36 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.7810127752406908 agent memory len 364 steps  36 reward -1 next state  2 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 360 steps  36 reward -2 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 412 steps  37 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7810127752406908 agent memory len 365 steps  37 reward -2 next state  2 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 361 steps  37 reward -2 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.7810127752406908 agent memory len 413 steps  38 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 366 steps  38 reward -2 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 362 steps  38 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 1
actions ['S', 'S', 'R']
agent epsilon  0.7810127752406908 agent memory len 414 steps  39 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 367 steps  39 reward -1 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 363 steps  39 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7810127752406908 agent memory len 415 steps  40 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 368 steps  40 reward -2 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 364 steps  40 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 416 steps  41 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.7810127752406908 agent memory len 369 steps  41 reward -1 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 365 steps  41 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.7810127752406908 agent memory len 417 steps  42 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 370 steps  42 reward -1 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 366 steps  42 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.7810127752406908 agent memory len 418 steps  43 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.7810127752406908 agent memory len 371 steps  43 reward -1 next state  2 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 367 steps  43 reward -1 next state  0 agent position  (4, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 419 steps  44 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 372 steps  44 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 368 steps  44 reward -1 next state  0 agent position  (5, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 420 steps  45 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 373 steps  45 reward -1 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 369 steps  45 reward -1 next state  0 agent position  (5, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.7810127752406908 agent memory len 421 steps  46 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.7810127752406908 agent memory len 374 steps  46 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 370 steps  46 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.7810127752406908 agent memory len 422 steps  47 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 375 steps  47 reward -1 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 371 steps  47 reward -1 next state  0 agent position  (5, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  0.7810127752406908 agent memory len 423 steps  48 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 376 steps  48 reward -1 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 372 steps  48 reward -1 next state  0 agent position  (4, 0)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.7810127752406908 agent memory len 424 steps  49 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 377 steps  49 reward -1 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 373 steps  49 reward -1 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 425 steps  50 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 378 steps  50 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 374 steps  50 reward -2 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'L']
agent epsilon  0.7810127752406908 agent memory len 426 steps  51 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.7810127752406908 agent memory len 379 steps  51 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 375 steps  51 reward -2 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.7810127752406908 agent memory len 427 steps  52 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.7810127752406908 agent memory len 380 steps  52 reward -1 next state  2 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 376 steps  52 reward -1 next state  0 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.7810127752406908 agent memory len 428 steps  53 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.7810127752406908 agent memory len 381 steps  53 reward -2 next state  2 agent position  (0, 0)
agent epsilon  0.7810127752406908 agent memory len 377 steps  53 reward -1 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  0.7810127752406908 agent memory len 429 steps  54 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.7810127752406908 agent memory len 382 steps  54 reward -1 next state  3 agent position  (0, 1)
agent epsilon  0.7810127752406908 agent memory len 378 steps  54 reward -1 next state  0 agent position  (2, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.7810127752406908 agent memory len 430 steps  55 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.7810127752406908 agent memory len 383 steps  55 reward -1 next state  4 agent position  (0, 2)
agent epsilon  0.7810127752406908 agent memory len 379 steps  55 reward -1 next state  0 agent position  (3, 0)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.7810127752406908 agent memory len 431 steps  56 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.7810127752406908 agent memory len 384 steps  56 reward -1 next state  5 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 380 steps  56 reward -1 next state  0 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.7810127752406908 agent memory len 432 steps  57 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.7810127752406908 agent memory len 385 steps  57 reward -1 next state  5 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 381 steps  57 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.7810127752406908 agent memory len 433 steps  58 reward 0 next state  6 agent position  (6, 5)
agent epsilon  0.7810127752406908 agent memory len 386 steps  58 reward -1 next state  5 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 382 steps  58 reward -1 next state  0 agent position  (4, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.7810127752406908 agent memory len 434 steps  59 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.7810127752406908 agent memory len 387 steps  59 reward -2 next state  6 agent position  (0, 3)
agent epsilon  0.7810127752406908 agent memory len 383 steps  59 reward -1 next state  0 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.7810127752406908 agent memory len 435 steps  60 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.7810127752406908 agent memory len 388 steps  60 reward -1 next state  6 agent position  (0, 4)
agent epsilon  0.7810127752406908 agent memory len 384 steps  60 reward -1 next state  0 agent position  (3, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  0.7810127752406908 agent memory len 436 steps  61 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.7810127752406908 agent memory len 389 steps  61 reward -1 next state  6 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 385 steps  61 reward -1 next state  0 agent position  (3, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.7810127752406908 agent memory len 437 steps  62 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.7810127752406908 agent memory len 390 steps  62 reward -1 next state  6 agent position  (0, 6)
agent epsilon  0.7810127752406908 agent memory len 386 steps  62 reward -1 next state  0 agent position  (2, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.7810127752406908 agent memory len 438 steps  63 reward 0 next state  7 agent position  (7, 7)
agent epsilon  0.7810127752406908 agent memory len 391 steps  63 reward -2 next state  7 agent position  (0, 6)
agent epsilon  0.7810127752406908 agent memory len 387 steps  63 reward -1 next state  0 agent position  (2, 2)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.7810127752406908 agent memory len 439 steps  64 reward 0 next state  8 agent position  (8, 7)
agent epsilon  0.7810127752406908 agent memory len 392 steps  64 reward -1 next state  7 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 388 steps  64 reward -1 next state  0 agent position  (2, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7810127752406908 agent memory len 440 steps  65 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.7810127752406908 agent memory len 393 steps  65 reward -1 next state  6 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 389 steps  65 reward -1 next state  0 agent position  (2, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.7810127752406908 agent memory len 441 steps  66 reward 0 next state  8 agent position  (8, 5)
agent epsilon  0.7810127752406908 agent memory len 394 steps  66 reward -1 next state  5 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 390 steps  66 reward -1 next state  0 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'L']
agent epsilon  0.7810127752406908 agent memory len 442 steps  67 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.7810127752406908 agent memory len 395 steps  67 reward -1 next state  5 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 391 steps  67 reward -1 next state  0 agent position  (3, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'D', 'R']
agent epsilon  0.7810127752406908 agent memory len 443 steps  68 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.7810127752406908 agent memory len 396 steps  68 reward -1 next state  4 agent position  (1, 5)
agent epsilon  0.7810127752406908 agent memory len 392 steps  68 reward -1 next state  1 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'S']
agent epsilon  0.7810127752406908 agent memory len 444 steps  69 reward 0 next state  8 agent position  (8, 4)
agent epsilon  0.7810127752406908 agent memory len 397 steps  69 reward -1 next state  4 agent position  (0, 5)
agent epsilon  0.7810127752406908 agent memory len 393 steps  69 reward -1 next state  0 agent position  (3, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'R']
agent epsilon  0.7810127752406908 agent memory len 445 steps  70 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.7810127752406908 agent memory len 398 steps  70 reward -1 next state  3 agent position  (1, 5)
agent epsilon  0.7810127752406908 agent memory len 394 steps  70 reward -1 next state  1 agent position  (3, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.7810127752406908 agent memory len 446 steps  71 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.7810127752406908 agent memory len 399 steps  71 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.7810127752406908 agent memory len 395 steps  71 reward -1 next state  2 agent position  (2, 4)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'U']
agent epsilon  0.7810127752406908 agent memory len 447 steps  72 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.7810127752406908 agent memory len 400 steps  72 reward -1 next state  3 agent position  (2, 5)
agent epsilon  0.7810127752406908 agent memory len 396 steps  72 reward -1 next state  2 agent position  (1, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.7810127752406908 agent memory len 448 steps  73 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.7810127752406908 agent memory len 401 steps  73 reward -1 next state  3 agent position  (2, 4)
agent epsilon  0.7810127752406908 agent memory len 397 steps  73 reward -1 next state  2 agent position  (1, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['R', 'L', 'D']
agent epsilon  0.7810127752406908 agent memory len 449 steps  74 reward 10 next state  7 agent position  (7, 4)
agent epsilon  0.7810127752406908 agent memory len 402 steps  74 reward -1 next state  4 agent position  (2, 3)
agent epsilon  0.7810127752406908 agent memory len 398 steps  74 reward -1 next state  2 agent position  (2, 4)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'R']
agent epsilon  0.7810127752406908 agent memory len 449 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7810127752406908 agent memory len 403 steps  75 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.7810127752406908 agent memory len 399 steps  75 reward -1 next state  2 agent position  (2, 5)
max steps reached
total rewards -231
epsilon  0.7434100384749007
epsilon  0.7434100384749007
epsilon  0.7434100384749007
Episode number:  7
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 450 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7434100384749007 agent memory len 404 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 400 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 451 steps  2 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.7434100384749007 agent memory len 405 steps  2 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 401 steps  2 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 452 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.7434100384749007 agent memory len 406 steps  3 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 402 steps  3 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 453 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7434100384749007 agent memory len 407 steps  4 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.7434100384749007 agent memory len 403 steps  4 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 454 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.7434100384749007 agent memory len 408 steps  5 reward -1 next state  0 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 404 steps  5 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.7434100384749007 agent memory len 455 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.7434100384749007 agent memory len 409 steps  6 reward -1 next state  0 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 405 steps  6 reward -1 next state  1 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'R']
agent epsilon  0.7434100384749007 agent memory len 456 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 410 steps  7 reward -1 next state  1 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 406 steps  7 reward -2 next state  0 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  0.7434100384749007 agent memory len 457 steps  8 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7434100384749007 agent memory len 411 steps  8 reward -1 next state  1 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 407 steps  8 reward -1 next state  0 agent position  (3, 8)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'D']
agent epsilon  0.7434100384749007 agent memory len 458 steps  9 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 412 steps  9 reward -1 next state  2 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 408 steps  9 reward -1 next state  0 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 459 steps  10 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 413 steps  10 reward -2 next state  3 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 409 steps  10 reward -1 next state  0 agent position  (4, 8)
 is_terminal [False, False, False]
actions ['R', 'L', 'D']
agent epsilon  0.7434100384749007 agent memory len 460 steps  11 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7434100384749007 agent memory len 414 steps  11 reward -1 next state  4 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 410 steps  11 reward -1 next state  0 agent position  (5, 8)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 461 steps  12 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.7434100384749007 agent memory len 415 steps  12 reward -2 next state  5 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 411 steps  12 reward -1 next state  0 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.7434100384749007 agent memory len 462 steps  13 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 416 steps  13 reward -1 next state  6 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 412 steps  13 reward 0 next state  0 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'U', 'D']
agent epsilon  0.7434100384749007 agent memory len 463 steps  14 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 417 steps  14 reward -2 next state  6 agent position  (0, 3)
agent epsilon  0.7434100384749007 agent memory len 413 steps  14 reward 10 next state  0 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 464 steps  15 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 418 steps  15 reward -1 next state  7 agent position  (1, 3)
agent epsilon  0.7434100384749007 agent memory len 413 steps  15 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 465 steps  16 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 419 steps  16 reward -1 next state  6 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  16 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 466 steps  17 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7434100384749007 agent memory len 420 steps  17 reward -1 next state  6 agent position  (0, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  17 reward -1 next state  0 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 467 steps  18 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 421 steps  18 reward -1 next state  7 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  18 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 468 steps  19 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 422 steps  19 reward -1 next state  7 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  19 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 469 steps  20 reward -2 next state  0 agent position  (0, 7)
agent epsilon  0.7434100384749007 agent memory len 423 steps  20 reward -1 next state  7 agent position  (1, 1)
agent epsilon  0.7434100384749007 agent memory len 413 steps  20 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 470 steps  21 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7434100384749007 agent memory len 424 steps  21 reward -1 next state  8 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  21 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 471 steps  22 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7434100384749007 agent memory len 425 steps  22 reward -1 next state  8 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  22 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 472 steps  23 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.7434100384749007 agent memory len 426 steps  23 reward -1 next state  8 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  23 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 473 steps  24 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 427 steps  24 reward -1 next state  9 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  24 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 474 steps  25 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 428 steps  25 reward -1 next state  9 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  25 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 475 steps  26 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 429 steps  26 reward -1 next state  9 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  26 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 476 steps  27 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 430 steps  27 reward -1 next state  9 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  27 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.7434100384749007 agent memory len 477 steps  28 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.7434100384749007 agent memory len 431 steps  28 reward -1 next state  9 agent position  (1, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  28 reward -1 next state  1 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 478 steps  29 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 432 steps  29 reward -1 next state  9 agent position  (2, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  29 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 479 steps  30 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 433 steps  30 reward -1 next state  9 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  30 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 480 steps  31 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 434 steps  31 reward -1 next state  9 agent position  (3, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  31 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.7434100384749007 agent memory len 481 steps  32 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 435 steps  32 reward -1 next state  9 agent position  (3, 3)
agent epsilon  0.7434100384749007 agent memory len 413 steps  32 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 482 steps  33 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 436 steps  33 reward -1 next state  9 agent position  (4, 3)
agent epsilon  0.7434100384749007 agent memory len 413 steps  33 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.7434100384749007 agent memory len 483 steps  34 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.7434100384749007 agent memory len 437 steps  34 reward -1 next state  9 agent position  (4, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  34 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 484 steps  35 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.7434100384749007 agent memory len 438 steps  35 reward -1 next state  9 agent position  (5, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  35 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 485 steps  36 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.7434100384749007 agent memory len 439 steps  36 reward -1 next state  9 agent position  (6, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  36 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 486 steps  37 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 440 steps  37 reward -1 next state  9 agent position  (7, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  37 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['L', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 487 steps  38 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.7434100384749007 agent memory len 441 steps  38 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  38 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['R', 'D', 'S']
agent epsilon  0.7434100384749007 agent memory len 488 steps  39 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  39 reward 10 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  39 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 489 steps  40 reward -2 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  40 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  40 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 490 steps  41 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  41 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  41 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 491 steps  42 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  42 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  42 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 492 steps  43 reward -2 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  43 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  43 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 493 steps  44 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  44 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  44 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 494 steps  45 reward -2 next state  4 agent position  (4, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  45 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  45 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 495 steps  46 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  46 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  46 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 496 steps  47 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  47 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  47 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 497 steps  48 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  48 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  48 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 498 steps  49 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  49 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  49 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 499 steps  50 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  50 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  50 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 500 steps  51 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  51 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  51 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 501 steps  52 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.7434100384749007 agent memory len 442 steps  52 reward -1 next state  7 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  52 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 502 steps  53 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  53 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  53 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 503 steps  54 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  54 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  54 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 504 steps  55 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  55 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  55 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 505 steps  56 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  56 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  56 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 506 steps  57 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  57 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  57 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 507 steps  58 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  58 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  58 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 508 steps  59 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  59 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  59 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 509 steps  60 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  60 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  60 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 510 steps  61 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  61 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  61 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 511 steps  62 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  62 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  62 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 512 steps  63 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  63 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  63 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 513 steps  64 reward -2 next state  7 agent position  (7, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  64 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  64 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 514 steps  65 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  65 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  65 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 515 steps  66 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  66 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  66 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 516 steps  67 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  67 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  67 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 517 steps  68 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  68 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  68 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 518 steps  69 reward -2 next state  5 agent position  (5, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  69 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  69 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 519 steps  70 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  70 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  70 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 520 steps  71 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  71 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  71 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 521 steps  72 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  72 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  72 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 522 steps  73 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  73 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  73 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 523 steps  74 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.7434100384749007 agent memory len 442 steps  74 reward -1 next state  8 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  74 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.7434100384749007 agent memory len 524 steps  75 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7434100384749007 agent memory len 442 steps  75 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.7434100384749007 agent memory len 413 steps  75 reward -1 next state  9 agent position  (8, 8)
max steps reached
total rewards -220
epsilon  0.7076412088215263
epsilon  0.7076412088215263
epsilon  0.7076412088215263
Episode number:  8
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 525 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.7076412088215263 agent memory len 443 steps  1 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.7076412088215263 agent memory len 414 steps  1 reward -2 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.7076412088215263 agent memory len 526 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.7076412088215263 agent memory len 444 steps  2 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.7076412088215263 agent memory len 415 steps  2 reward -1 next state  0 agent position  (0, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 527 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.7076412088215263 agent memory len 445 steps  3 reward -1 next state  2 agent position  (0, 4)
agent epsilon  0.7076412088215263 agent memory len 416 steps  3 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.7076412088215263 agent memory len 528 steps  4 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.7076412088215263 agent memory len 446 steps  4 reward -2 next state  2 agent position  (0, 4)
agent epsilon  0.7076412088215263 agent memory len 417 steps  4 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.7076412088215263 agent memory len 529 steps  5 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.7076412088215263 agent memory len 447 steps  5 reward -2 next state  2 agent position  (0, 4)
agent epsilon  0.7076412088215263 agent memory len 418 steps  5 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 530 steps  6 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.7076412088215263 agent memory len 448 steps  6 reward -1 next state  3 agent position  (1, 4)
agent epsilon  0.7076412088215263 agent memory len 419 steps  6 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.7076412088215263 agent memory len 531 steps  7 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.7076412088215263 agent memory len 449 steps  7 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.7076412088215263 agent memory len 420 steps  7 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 532 steps  8 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.7076412088215263 agent memory len 450 steps  8 reward -1 next state  4 agent position  (2, 4)
agent epsilon  0.7076412088215263 agent memory len 421 steps  8 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.7076412088215263 agent memory len 533 steps  9 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.7076412088215263 agent memory len 451 steps  9 reward -1 next state  4 agent position  (3, 4)
agent epsilon  0.7076412088215263 agent memory len 422 steps  9 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 534 steps  10 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.7076412088215263 agent memory len 452 steps  10 reward -1 next state  5 agent position  (4, 4)
agent epsilon  0.7076412088215263 agent memory len 423 steps  10 reward -1 next state  4 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 535 steps  11 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.7076412088215263 agent memory len 453 steps  11 reward -1 next state  6 agent position  (5, 4)
agent epsilon  0.7076412088215263 agent memory len 424 steps  11 reward -1 next state  5 agent position  (3, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.7076412088215263 agent memory len 536 steps  12 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 454 steps  12 reward 0 next state  7 agent position  (6, 4)
agent epsilon  0.7076412088215263 agent memory len 425 steps  12 reward -2 next state  6 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 537 steps  13 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.7076412088215263 agent memory len 455 steps  13 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.7076412088215263 agent memory len 426 steps  13 reward -1 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'D', 'D']
agent epsilon  0.7076412088215263 agent memory len 538 steps  14 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  14 reward 10 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 427 steps  14 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 539 steps  15 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  15 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 428 steps  15 reward -1 next state  7 agent position  (4, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 540 steps  16 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  16 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 429 steps  16 reward -1 next state  7 agent position  (4, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 541 steps  17 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  17 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 430 steps  17 reward -1 next state  7 agent position  (5, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 542 steps  18 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  18 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 431 steps  18 reward -1 next state  7 agent position  (5, 7)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 543 steps  19 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  19 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 432 steps  19 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 544 steps  20 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  20 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 433 steps  20 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 545 steps  21 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  21 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 434 steps  21 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 546 steps  22 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  22 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 435 steps  22 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 547 steps  23 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  23 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 436 steps  23 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 548 steps  24 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  24 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 437 steps  24 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 549 steps  25 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  25 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 438 steps  25 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 550 steps  26 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  26 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 439 steps  26 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 551 steps  27 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  27 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 440 steps  27 reward 0 next state  7 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 552 steps  28 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  28 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 441 steps  28 reward -2 next state  7 agent position  (9, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 553 steps  29 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  29 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 442 steps  29 reward 0 next state  7 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 554 steps  30 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  30 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 443 steps  30 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 555 steps  31 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  31 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 444 steps  31 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 556 steps  32 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  32 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 445 steps  32 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 557 steps  33 reward -2 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  33 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 446 steps  33 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 558 steps  34 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  34 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 447 steps  34 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 559 steps  35 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  35 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 448 steps  35 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 560 steps  36 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  36 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 449 steps  36 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 561 steps  37 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  37 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 450 steps  37 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 562 steps  38 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  38 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 451 steps  38 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 563 steps  39 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  39 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 452 steps  39 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 564 steps  40 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  40 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 453 steps  40 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 565 steps  41 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  41 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 454 steps  41 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 566 steps  42 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 455 steps  42 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 567 steps  43 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 456 steps  43 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 568 steps  44 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  44 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 457 steps  44 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 569 steps  45 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 458 steps  45 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 570 steps  46 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  46 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 459 steps  46 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 571 steps  47 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  47 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 460 steps  47 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 572 steps  48 reward -2 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  48 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 461 steps  48 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 573 steps  49 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  49 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 462 steps  49 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 574 steps  50 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 463 steps  50 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 575 steps  51 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  51 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 464 steps  51 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 576 steps  52 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  52 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 465 steps  52 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 577 steps  53 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  53 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 466 steps  53 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 578 steps  54 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  54 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 467 steps  54 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 579 steps  55 reward -2 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  55 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 468 steps  55 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 580 steps  56 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  56 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 469 steps  56 reward -2 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 581 steps  57 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  57 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 470 steps  57 reward -1 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 582 steps  58 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  58 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 471 steps  58 reward -2 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 583 steps  59 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  59 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 472 steps  59 reward -2 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 584 steps  60 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  60 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 473 steps  60 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 585 steps  61 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  61 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 474 steps  61 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 586 steps  62 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  62 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 475 steps  62 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 587 steps  63 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  63 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 476 steps  63 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.7076412088215263 agent memory len 588 steps  64 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.7076412088215263 agent memory len 456 steps  64 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 477 steps  64 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 589 steps  65 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.7076412088215263 agent memory len 456 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 478 steps  65 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 590 steps  66 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  66 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 479 steps  66 reward -1 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 591 steps  67 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  67 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 480 steps  67 reward -2 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 592 steps  68 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.7076412088215263 agent memory len 456 steps  68 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 481 steps  68 reward -2 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 593 steps  69 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  69 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 482 steps  69 reward -1 next state  7 agent position  (9, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 594 steps  70 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  70 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 483 steps  70 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.7076412088215263 agent memory len 595 steps  71 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  71 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 484 steps  71 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 596 steps  72 reward -1 next state  1 agent position  (1, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  72 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 485 steps  72 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.7076412088215263 agent memory len 597 steps  73 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  73 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 486 steps  73 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.7076412088215263 agent memory len 598 steps  74 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  74 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 487 steps  74 reward -2 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.7076412088215263 agent memory len 599 steps  75 reward -1 next state  0 agent position  (0, 9)
agent epsilon  0.7076412088215263 agent memory len 456 steps  75 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.7076412088215263 agent memory len 488 steps  75 reward -1 next state  7 agent position  (9, 6)
max steps reached
total rewards -225
epsilon  0.6736168455752829
epsilon  0.6736168455752829
epsilon  0.6736168455752829
Episode number:  9
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'D']
agent epsilon  0.6736168455752829 agent memory len 600 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.6736168455752829 agent memory len 457 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 489 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.6736168455752829 agent memory len 601 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6736168455752829 agent memory len 458 steps  2 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6736168455752829 agent memory len 490 steps  2 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.6736168455752829 agent memory len 602 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.6736168455752829 agent memory len 459 steps  3 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 491 steps  3 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.6736168455752829 agent memory len 603 steps  4 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.6736168455752829 agent memory len 460 steps  4 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 492 steps  4 reward -1 next state  2 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.6736168455752829 agent memory len 604 steps  5 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6736168455752829 agent memory len 461 steps  5 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 493 steps  5 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.6736168455752829 agent memory len 605 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 462 steps  6 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 494 steps  6 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 606 steps  7 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 463 steps  7 reward -1 next state  0 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 495 steps  7 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 607 steps  8 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6736168455752829 agent memory len 464 steps  8 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 496 steps  8 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'D']
agent epsilon  0.6736168455752829 agent memory len 608 steps  9 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 465 steps  9 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 497 steps  9 reward -1 next state  2 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.6736168455752829 agent memory len 609 steps  10 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 466 steps  10 reward -1 next state  2 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 498 steps  10 reward -1 next state  3 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'L']
agent epsilon  0.6736168455752829 agent memory len 610 steps  11 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 467 steps  11 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 499 steps  11 reward -1 next state  3 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'D']
agent epsilon  0.6736168455752829 agent memory len 611 steps  12 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 468 steps  12 reward -1 next state  3 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 500 steps  12 reward 0 next state  2 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'R']
agent epsilon  0.6736168455752829 agent memory len 612 steps  13 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 469 steps  13 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 501 steps  13 reward 0 next state  2 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'S']
agent epsilon  0.6736168455752829 agent memory len 613 steps  14 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 470 steps  14 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 502 steps  14 reward 0 next state  2 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'L']
agent epsilon  0.6736168455752829 agent memory len 614 steps  15 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6736168455752829 agent memory len 471 steps  15 reward -1 next state  1 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 503 steps  15 reward 0 next state  2 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['L', 'S', 'D']
agent epsilon  0.6736168455752829 agent memory len 615 steps  16 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 472 steps  16 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 504 steps  16 reward 10 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 616 steps  17 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 473 steps  17 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 504 steps  17 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.6736168455752829 agent memory len 617 steps  18 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.6736168455752829 agent memory len 474 steps  18 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 504 steps  18 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.6736168455752829 agent memory len 618 steps  19 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6736168455752829 agent memory len 475 steps  19 reward -1 next state  0 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  19 reward -1 next state  2 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 619 steps  20 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.6736168455752829 agent memory len 476 steps  20 reward -1 next state  1 agent position  (3, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  20 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 620 steps  21 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 477 steps  21 reward -1 next state  2 agent position  (4, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  21 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.6736168455752829 agent memory len 621 steps  22 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.6736168455752829 agent memory len 478 steps  22 reward -1 next state  2 agent position  (4, 1)
agent epsilon  0.6736168455752829 agent memory len 504 steps  22 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 622 steps  23 reward -2 next state  0 agent position  (0, 2)
agent epsilon  0.6736168455752829 agent memory len 479 steps  23 reward -1 next state  2 agent position  (5, 1)
agent epsilon  0.6736168455752829 agent memory len 504 steps  23 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.6736168455752829 agent memory len 623 steps  24 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6736168455752829 agent memory len 480 steps  24 reward -1 next state  3 agent position  (5, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  24 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.6736168455752829 agent memory len 624 steps  25 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 481 steps  25 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  25 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 625 steps  26 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 482 steps  26 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  26 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'L', 'S']
agent epsilon  0.6736168455752829 agent memory len 626 steps  27 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 483 steps  27 reward -1 next state  3 agent position  (5, 1)
agent epsilon  0.6736168455752829 agent memory len 504 steps  27 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 627 steps  28 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 484 steps  28 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.6736168455752829 agent memory len 504 steps  28 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'R', 'S']
agent epsilon  0.6736168455752829 agent memory len 628 steps  29 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 485 steps  29 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  29 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 629 steps  30 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 486 steps  30 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  30 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 630 steps  31 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 487 steps  31 reward 0 next state  4 agent position  (8, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  31 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['D', 'D', 'S']
agent epsilon  0.6736168455752829 agent memory len 631 steps  32 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  32 reward 10 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  32 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 632 steps  33 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  33 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  33 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 633 steps  34 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  34 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  34 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 634 steps  35 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  35 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  35 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 635 steps  36 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  36 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  36 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 636 steps  37 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  37 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  37 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 637 steps  38 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  38 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  38 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 638 steps  39 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  39 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  39 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 639 steps  40 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  40 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  40 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 640 steps  41 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  41 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  41 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 641 steps  42 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  42 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  42 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 642 steps  43 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  43 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  43 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 643 steps  44 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  44 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  44 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 644 steps  45 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  45 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  45 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 645 steps  46 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  46 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  46 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 646 steps  47 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  47 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  47 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 647 steps  48 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  48 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  48 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 648 steps  49 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  49 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  49 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 649 steps  50 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.6736168455752829 agent memory len 488 steps  50 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  50 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 650 steps  51 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  51 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  51 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 651 steps  52 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  52 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  52 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 652 steps  53 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  53 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  53 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 653 steps  54 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  54 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  54 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 654 steps  55 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  55 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  55 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 655 steps  56 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  56 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  56 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 656 steps  57 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  57 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  57 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 657 steps  58 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  58 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  58 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 658 steps  59 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  59 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  59 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 659 steps  60 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  60 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  60 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 660 steps  61 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  61 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  61 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 661 steps  62 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  62 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  62 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 662 steps  63 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  63 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  63 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 663 steps  64 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  64 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  64 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 664 steps  65 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  65 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  65 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 665 steps  66 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  66 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  66 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 666 steps  67 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6736168455752829 agent memory len 488 steps  67 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  67 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 667 steps  68 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  68 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  68 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 668 steps  69 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  69 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  69 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 669 steps  70 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  70 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  70 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 670 steps  71 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.6736168455752829 agent memory len 488 steps  71 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  71 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 671 steps  72 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6736168455752829 agent memory len 488 steps  72 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  72 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 672 steps  73 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  73 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  73 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 673 steps  74 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  74 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  74 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6736168455752829 agent memory len 674 steps  75 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6736168455752829 agent memory len 488 steps  75 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6736168455752829 agent memory len 504 steps  75 reward -1 next state  9 agent position  (8, 8)
max steps reached
total rewards -199
epsilon  0.6412518701055556
epsilon  0.6412518701055556
epsilon  0.6412518701055556
Episode number:  10
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 675 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.6412518701055556 agent memory len 489 steps  1 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.6412518701055556 agent memory len 505 steps  1 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 676 steps  2 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.6412518701055556 agent memory len 490 steps  2 reward -1 next state  2 agent position  (1, 6)
agent epsilon  0.6412518701055556 agent memory len 506 steps  2 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 677 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.6412518701055556 agent memory len 491 steps  3 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.6412518701055556 agent memory len 507 steps  3 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.6412518701055556 agent memory len 678 steps  4 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.6412518701055556 agent memory len 492 steps  4 reward -1 next state  2 agent position  (3, 6)
agent epsilon  0.6412518701055556 agent memory len 508 steps  4 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 679 steps  5 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.6412518701055556 agent memory len 493 steps  5 reward -1 next state  2 agent position  (3, 6)
agent epsilon  0.6412518701055556 agent memory len 509 steps  5 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 680 steps  6 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 494 steps  6 reward -1 next state  3 agent position  (4, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  6 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.6412518701055556 agent memory len 681 steps  7 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 495 steps  7 reward -1 next state  3 agent position  (5, 6)
agent epsilon  0.6412518701055556 agent memory len 511 steps  7 reward -1 next state  5 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 682 steps  8 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6412518701055556 agent memory len 496 steps  8 reward -1 next state  4 agent position  (5, 7)
agent epsilon  0.6412518701055556 agent memory len 512 steps  8 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 683 steps  9 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.6412518701055556 agent memory len 497 steps  9 reward -1 next state  4 agent position  (6, 7)
agent epsilon  0.6412518701055556 agent memory len 513 steps  9 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  0.6412518701055556 agent memory len 684 steps  10 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.6412518701055556 agent memory len 498 steps  10 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 514 steps  10 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.6412518701055556 agent memory len 685 steps  11 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.6412518701055556 agent memory len 499 steps  11 reward -1 next state  4 agent position  (5, 6)
agent epsilon  0.6412518701055556 agent memory len 515 steps  11 reward -1 next state  5 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 686 steps  12 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 500 steps  12 reward -1 next state  3 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 516 steps  12 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.6412518701055556 agent memory len 687 steps  13 reward -2 next state  0 agent position  (0, 3)
agent epsilon  0.6412518701055556 agent memory len 501 steps  13 reward -1 next state  3 agent position  (7, 6)
agent epsilon  0.6412518701055556 agent memory len 517 steps  13 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 688 steps  14 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.6412518701055556 agent memory len 502 steps  14 reward -1 next state  3 agent position  (8, 6)
agent epsilon  0.6412518701055556 agent memory len 518 steps  14 reward -1 next state  8 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 689 steps  15 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.6412518701055556 agent memory len 503 steps  15 reward -1 next state  3 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 519 steps  15 reward -2 next state  9 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.6412518701055556 agent memory len 690 steps  16 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6412518701055556 agent memory len 504 steps  16 reward -2 next state  4 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 520 steps  16 reward 0 next state  9 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'R', 'D']
agent epsilon  0.6412518701055556 agent memory len 691 steps  17 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6412518701055556 agent memory len 505 steps  17 reward 0 next state  4 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 521 steps  17 reward 0 next state  9 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 692 steps  18 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.6412518701055556 agent memory len 506 steps  18 reward 0 next state  4 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 522 steps  18 reward 0 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 693 steps  19 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.6412518701055556 agent memory len 507 steps  19 reward 0 next state  4 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 523 steps  19 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.6412518701055556 agent memory len 694 steps  20 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.6412518701055556 agent memory len 508 steps  20 reward 0 next state  4 agent position  (9, 8)
agent epsilon  0.6412518701055556 agent memory len 524 steps  20 reward 0 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6412518701055556 agent memory len 695 steps  21 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.6412518701055556 agent memory len 509 steps  21 reward -2 next state  5 agent position  (9, 8)
agent epsilon  0.6412518701055556 agent memory len 525 steps  21 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
landmark captured 2
agent reached landmark-------------------------------- 1
actions ['D', 'U', 'S']
agent epsilon  0.6412518701055556 agent memory len 696 steps  22 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  22 reward 10 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 526 steps  22 reward -1 next state  8 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 697 steps  23 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  23 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 527 steps  23 reward -1 next state  8 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 698 steps  24 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  24 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 528 steps  24 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 699 steps  25 reward 0 next state  6 agent position  (6, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  25 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 529 steps  25 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 700 steps  26 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  26 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 530 steps  26 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 701 steps  27 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  27 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 531 steps  27 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 702 steps  28 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  28 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 532 steps  28 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 703 steps  29 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  29 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 533 steps  29 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 704 steps  30 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  30 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 534 steps  30 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 705 steps  31 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  31 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 535 steps  31 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 706 steps  32 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  32 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 536 steps  32 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 707 steps  33 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  33 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 537 steps  33 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 708 steps  34 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  34 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 538 steps  34 reward -1 next state  8 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 709 steps  35 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  35 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 539 steps  35 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 710 steps  36 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  36 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 540 steps  36 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 711 steps  37 reward 0 next state  6 agent position  (6, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  37 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 541 steps  37 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 712 steps  38 reward 0 next state  7 agent position  (7, 5)
agent epsilon  0.6412518701055556 agent memory len 510 steps  38 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 542 steps  38 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 713 steps  39 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  39 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 543 steps  39 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 714 steps  40 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  40 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 544 steps  40 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 715 steps  41 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  41 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 545 steps  41 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 716 steps  42 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  42 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 546 steps  42 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 717 steps  43 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  43 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 547 steps  43 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 718 steps  44 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 510 steps  44 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 548 steps  44 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 719 steps  45 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  45 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 549 steps  45 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 720 steps  46 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  46 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 550 steps  46 reward -2 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 721 steps  47 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  47 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 551 steps  47 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 722 steps  48 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  48 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 552 steps  48 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 723 steps  49 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  49 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 553 steps  49 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 724 steps  50 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  50 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 554 steps  50 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 725 steps  51 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  51 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 555 steps  51 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 726 steps  52 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  52 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 556 steps  52 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 727 steps  53 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  53 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 557 steps  53 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 728 steps  54 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  54 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 558 steps  54 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 729 steps  55 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  55 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 559 steps  55 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 730 steps  56 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  56 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 560 steps  56 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 731 steps  57 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  57 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 561 steps  57 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 732 steps  58 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  58 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 562 steps  58 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 733 steps  59 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  59 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 563 steps  59 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 734 steps  60 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  60 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 564 steps  60 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 735 steps  61 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  61 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 565 steps  61 reward -1 next state  8 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 736 steps  62 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  62 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 566 steps  62 reward -1 next state  8 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 737 steps  63 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  63 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 567 steps  63 reward -1 next state  8 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 738 steps  64 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.6412518701055556 agent memory len 510 steps  64 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 568 steps  64 reward -1 next state  8 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 739 steps  65 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  65 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 569 steps  65 reward -1 next state  8 agent position  (8, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 740 steps  66 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  66 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 570 steps  66 reward -1 next state  8 agent position  (8, 6)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.6412518701055556 agent memory len 741 steps  67 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  67 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 571 steps  67 reward -1 next state  8 agent position  (9, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 742 steps  68 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  68 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 572 steps  68 reward -1 next state  8 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 743 steps  69 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  69 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 573 steps  69 reward -1 next state  8 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 744 steps  70 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  70 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 574 steps  70 reward -1 next state  8 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.6412518701055556 agent memory len 745 steps  71 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  71 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 575 steps  71 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 746 steps  72 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  72 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 576 steps  72 reward -1 next state  8 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.6412518701055556 agent memory len 747 steps  73 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  73 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 577 steps  73 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.6412518701055556 agent memory len 748 steps  74 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  74 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 578 steps  74 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.6412518701055556 agent memory len 749 steps  75 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.6412518701055556 agent memory len 510 steps  75 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.6412518701055556 agent memory len 579 steps  75 reward -1 next state  8 agent position  (6, 7)
max steps reached
total rewards -223
epsilon  0.6104653531155071
epsilon  0.6104653531155071
epsilon  0.6104653531155071
Episode number:  11
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  0.6104653531155071 agent memory len 750 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.6104653531155071 agent memory len 511 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.6104653531155071 agent memory len 580 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.6104653531155071 agent memory len 751 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6104653531155071 agent memory len 512 steps  2 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.6104653531155071 agent memory len 581 steps  2 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 752 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.6104653531155071 agent memory len 513 steps  3 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.6104653531155071 agent memory len 582 steps  3 reward -1 next state  1 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  0.6104653531155071 agent memory len 753 steps  4 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.6104653531155071 agent memory len 514 steps  4 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.6104653531155071 agent memory len 583 steps  4 reward -1 next state  1 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 754 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.6104653531155071 agent memory len 515 steps  5 reward -1 next state  0 agent position  (2, 6)
agent epsilon  0.6104653531155071 agent memory len 584 steps  5 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.6104653531155071 agent memory len 755 steps  6 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.6104653531155071 agent memory len 516 steps  6 reward -1 next state  0 agent position  (3, 6)
agent epsilon  0.6104653531155071 agent memory len 585 steps  6 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.6104653531155071 agent memory len 756 steps  7 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.6104653531155071 agent memory len 517 steps  7 reward -1 next state  0 agent position  (4, 6)
agent epsilon  0.6104653531155071 agent memory len 586 steps  7 reward -1 next state  4 agent position  (4, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.6104653531155071 agent memory len 757 steps  8 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.6104653531155071 agent memory len 518 steps  8 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.6104653531155071 agent memory len 587 steps  8 reward -1 next state  4 agent position  (3, 8)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'D']
agent epsilon  0.6104653531155071 agent memory len 758 steps  9 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.6104653531155071 agent memory len 519 steps  9 reward -1 next state  2 agent position  (4, 4)
agent epsilon  0.6104653531155071 agent memory len 588 steps  9 reward -1 next state  4 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 759 steps  10 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.6104653531155071 agent memory len 520 steps  10 reward -1 next state  2 agent position  (5, 4)
agent epsilon  0.6104653531155071 agent memory len 589 steps  10 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 760 steps  11 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.6104653531155071 agent memory len 521 steps  11 reward -1 next state  3 agent position  (5, 4)
agent epsilon  0.6104653531155071 agent memory len 590 steps  11 reward -2 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 761 steps  12 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.6104653531155071 agent memory len 522 steps  12 reward -1 next state  3 agent position  (5, 4)
agent epsilon  0.6104653531155071 agent memory len 591 steps  12 reward -1 next state  5 agent position  (5, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 762 steps  13 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.6104653531155071 agent memory len 523 steps  13 reward 0 next state  4 agent position  (6, 4)
agent epsilon  0.6104653531155071 agent memory len 592 steps  13 reward -2 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 763 steps  14 reward 10 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 524 steps  14 reward -1 next state  4 agent position  (6, 4)
agent epsilon  0.6104653531155071 agent memory len 593 steps  14 reward -1 next state  6 agent position  (5, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  15 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 525 steps  15 reward -1 next state  4 agent position  (6, 3)
agent epsilon  0.6104653531155071 agent memory len 594 steps  15 reward -1 next state  6 agent position  (5, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  16 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 526 steps  16 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.6104653531155071 agent memory len 595 steps  16 reward -1 next state  7 agent position  (5, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 763 steps  17 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 527 steps  17 reward -1 next state  4 agent position  (7, 3)
agent epsilon  0.6104653531155071 agent memory len 596 steps  17 reward -1 next state  7 agent position  (5, 7)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  18 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 528 steps  18 reward 0 next state  4 agent position  (8, 3)
agent epsilon  0.6104653531155071 agent memory len 597 steps  18 reward -1 next state  8 agent position  (5, 7)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  19 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 529 steps  19 reward 0 next state  4 agent position  (9, 3)
agent epsilon  0.6104653531155071 agent memory len 598 steps  19 reward -1 next state  9 agent position  (5, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'D', 'D']
agent epsilon  0.6104653531155071 agent memory len 763 steps  20 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 530 steps  20 reward -2 next state  4 agent position  (9, 3)
agent epsilon  0.6104653531155071 agent memory len 599 steps  20 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  21 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 531 steps  21 reward -2 next state  4 agent position  (9, 3)
agent epsilon  0.6104653531155071 agent memory len 600 steps  21 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, False, False]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  22 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  22 reward 10 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 601 steps  22 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  23 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  23 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 602 steps  23 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  24 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  24 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 603 steps  24 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  25 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  25 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 604 steps  25 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  26 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  26 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 605 steps  26 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  27 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  27 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 606 steps  27 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  28 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  28 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 607 steps  28 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  29 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  29 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 608 steps  29 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  30 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  30 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 609 steps  30 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  31 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  31 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 610 steps  31 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  32 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  32 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 611 steps  32 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  33 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  33 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 612 steps  33 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  34 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  34 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 613 steps  34 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  35 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  35 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 614 steps  35 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  36 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  36 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 615 steps  36 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  37 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  37 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 616 steps  37 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  38 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  38 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 617 steps  38 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  39 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  39 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 618 steps  39 reward -2 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  40 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  40 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 619 steps  40 reward -2 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  41 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  41 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 620 steps  41 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  42 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 621 steps  42 reward -1 next state  9 agent position  (6, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  43 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 622 steps  43 reward -1 next state  9 agent position  (5, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  44 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  44 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 623 steps  44 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  45 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 624 steps  45 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  46 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  46 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 625 steps  46 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  47 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  47 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 626 steps  47 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 763 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  48 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 627 steps  48 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  49 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  49 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 628 steps  49 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  50 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 629 steps  50 reward -1 next state  9 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 763 steps  51 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  51 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 630 steps  51 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  52 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  52 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 631 steps  52 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  53 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  53 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 632 steps  53 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  54 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  54 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 633 steps  54 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  55 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  55 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 634 steps  55 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  56 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  56 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 635 steps  56 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  57 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  57 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 636 steps  57 reward -1 next state  9 agent position  (4, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 763 steps  58 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  58 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 637 steps  58 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  59 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 638 steps  59 reward -1 next state  9 agent position  (4, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  60 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  60 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 639 steps  60 reward -1 next state  9 agent position  (3, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  61 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  61 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 640 steps  61 reward -1 next state  9 agent position  (2, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  62 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  62 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 641 steps  62 reward -1 next state  9 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  63 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  63 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 642 steps  63 reward -1 next state  9 agent position  (2, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.6104653531155071 agent memory len 763 steps  64 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  64 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 643 steps  64 reward -1 next state  9 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  65 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 644 steps  65 reward -1 next state  9 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  66 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 645 steps  66 reward -1 next state  9 agent position  (1, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  67 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 646 steps  67 reward -1 next state  9 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.6104653531155071 agent memory len 763 steps  68 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  68 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 647 steps  68 reward -1 next state  9 agent position  (0, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 763 steps  69 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  69 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 648 steps  69 reward -1 next state  9 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  70 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 649 steps  70 reward -1 next state  9 agent position  (1, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.6104653531155071 agent memory len 763 steps  71 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  71 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 650 steps  71 reward -1 next state  9 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  72 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  72 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 651 steps  72 reward -1 next state  9 agent position  (2, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.6104653531155071 agent memory len 763 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  73 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 652 steps  73 reward -1 next state  9 agent position  (2, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  74 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  74 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 653 steps  74 reward -1 next state  9 agent position  (2, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.6104653531155071 agent memory len 763 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.6104653531155071 agent memory len 532 steps  75 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.6104653531155071 agent memory len 654 steps  75 reward -1 next state  9 agent position  (2, 8)
max steps reached
total rewards -203
epsilon  0.5811803122766818
epsilon  0.5811803122766818
epsilon  0.5811803122766818
Episode number:  12
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 764 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 533 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.5811803122766818 agent memory len 655 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.5811803122766818 agent memory len 765 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5811803122766818 agent memory len 534 steps  2 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.5811803122766818 agent memory len 656 steps  2 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  0.5811803122766818 agent memory len 766 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 535 steps  3 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.5811803122766818 agent memory len 657 steps  3 reward -1 next state  0 agent position  (1, 7)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 767 steps  4 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5811803122766818 agent memory len 536 steps  4 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.5811803122766818 agent memory len 658 steps  4 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 768 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 537 steps  5 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.5811803122766818 agent memory len 659 steps  5 reward -1 next state  2 agent position  (3, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'D']
agent epsilon  0.5811803122766818 agent memory len 769 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 538 steps  6 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.5811803122766818 agent memory len 660 steps  6 reward -1 next state  1 agent position  (4, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'U']
agent epsilon  0.5811803122766818 agent memory len 770 steps  7 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5811803122766818 agent memory len 539 steps  7 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.5811803122766818 agent memory len 661 steps  7 reward -1 next state  1 agent position  (3, 7)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.5811803122766818 agent memory len 771 steps  8 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5811803122766818 agent memory len 540 steps  8 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5811803122766818 agent memory len 662 steps  8 reward -1 next state  0 agent position  (4, 7)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 772 steps  9 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.5811803122766818 agent memory len 541 steps  9 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.5811803122766818 agent memory len 663 steps  9 reward -1 next state  0 agent position  (5, 7)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 773 steps  10 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.5811803122766818 agent memory len 542 steps  10 reward -1 next state  0 agent position  (1, 3)
agent epsilon  0.5811803122766818 agent memory len 664 steps  10 reward -1 next state  1 agent position  (6, 7)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.5811803122766818 agent memory len 774 steps  11 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.5811803122766818 agent memory len 543 steps  11 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.5811803122766818 agent memory len 665 steps  11 reward -1 next state  2 agent position  (6, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 775 steps  12 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 544 steps  12 reward -1 next state  0 agent position  (2, 2)
agent epsilon  0.5811803122766818 agent memory len 666 steps  12 reward -1 next state  2 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 776 steps  13 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.5811803122766818 agent memory len 545 steps  13 reward -1 next state  1 agent position  (2, 3)
agent epsilon  0.5811803122766818 agent memory len 667 steps  13 reward 0 next state  2 agent position  (7, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 777 steps  14 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.5811803122766818 agent memory len 546 steps  14 reward -1 next state  0 agent position  (3, 3)
agent epsilon  0.5811803122766818 agent memory len 668 steps  14 reward 0 next state  3 agent position  (8, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'L']
agent epsilon  0.5811803122766818 agent memory len 778 steps  15 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 547 steps  15 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.5811803122766818 agent memory len 669 steps  15 reward -1 next state  2 agent position  (8, 6)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 779 steps  16 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 548 steps  16 reward -1 next state  0 agent position  (3, 3)
agent epsilon  0.5811803122766818 agent memory len 670 steps  16 reward -1 next state  3 agent position  (9, 6)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.5811803122766818 agent memory len 780 steps  17 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 549 steps  17 reward -1 next state  0 agent position  (3, 4)
agent epsilon  0.5811803122766818 agent memory len 671 steps  17 reward 0 next state  3 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 781 steps  18 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.5811803122766818 agent memory len 550 steps  18 reward -1 next state  0 agent position  (4, 4)
agent epsilon  0.5811803122766818 agent memory len 672 steps  18 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 782 steps  19 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.5811803122766818 agent memory len 551 steps  19 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.5811803122766818 agent memory len 673 steps  19 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 783 steps  20 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.5811803122766818 agent memory len 552 steps  20 reward -1 next state  1 agent position  (4, 4)
agent epsilon  0.5811803122766818 agent memory len 674 steps  20 reward -2 next state  4 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'R']
agent epsilon  0.5811803122766818 agent memory len 784 steps  21 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.5811803122766818 agent memory len 553 steps  21 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.5811803122766818 agent memory len 675 steps  21 reward 0 next state  4 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 785 steps  22 reward 0 next state  9 agent position  (9, 1)
agent epsilon  0.5811803122766818 agent memory len 554 steps  22 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.5811803122766818 agent memory len 676 steps  22 reward -2 next state  5 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['R', 'R', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  23 reward 10 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 555 steps  23 reward -1 next state  2 agent position  (5, 6)
agent epsilon  0.5811803122766818 agent memory len 677 steps  23 reward -2 next state  5 agent position  (9, 8)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  24 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 556 steps  24 reward -1 next state  2 agent position  (5, 5)
agent epsilon  0.5811803122766818 agent memory len 678 steps  24 reward 0 next state  5 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  25 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 557 steps  25 reward -1 next state  2 agent position  (5, 5)
agent epsilon  0.5811803122766818 agent memory len 679 steps  25 reward -2 next state  5 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  26 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 558 steps  26 reward 0 next state  2 agent position  (6, 5)
agent epsilon  0.5811803122766818 agent memory len 680 steps  26 reward -2 next state  6 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'U']
agent epsilon  0.5811803122766818 agent memory len 786 steps  27 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 559 steps  27 reward -1 next state  2 agent position  (6, 6)
agent epsilon  0.5811803122766818 agent memory len 681 steps  27 reward 0 next state  6 agent position  (8, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  28 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 560 steps  28 reward -1 next state  2 agent position  (7, 6)
agent epsilon  0.5811803122766818 agent memory len 682 steps  28 reward -2 next state  7 agent position  (8, 9)
 is_terminal [True, False, False]
actions ['S', 'L', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  29 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 561 steps  29 reward 0 next state  2 agent position  (7, 5)
agent epsilon  0.5811803122766818 agent memory len 683 steps  29 reward 0 next state  7 agent position  (9, 9)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'R', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  30 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 562 steps  30 reward -1 next state  2 agent position  (7, 6)
agent epsilon  0.5811803122766818 agent memory len 684 steps  30 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  31 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 563 steps  31 reward -1 next state  2 agent position  (7, 6)
agent epsilon  0.5811803122766818 agent memory len 685 steps  31 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  32 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 564 steps  32 reward 0 next state  2 agent position  (7, 5)
agent epsilon  0.5811803122766818 agent memory len 686 steps  32 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, False, False]
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'S']
agent epsilon  0.5811803122766818 agent memory len 786 steps  33 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  33 reward 10 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 687 steps  33 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  34 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  34 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 688 steps  34 reward -2 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  35 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  35 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 689 steps  35 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  36 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  36 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 690 steps  36 reward -1 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5811803122766818 agent memory len 786 steps  37 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  37 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 691 steps  37 reward -1 next state  7 agent position  (8, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  38 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  38 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 692 steps  38 reward -1 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  39 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  39 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 693 steps  39 reward -2 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  40 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  40 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 694 steps  40 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  41 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  41 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 695 steps  41 reward -2 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 786 steps  42 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  42 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 696 steps  42 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  43 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  43 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 697 steps  43 reward -1 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  44 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  44 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 698 steps  44 reward -2 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  45 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  45 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 699 steps  45 reward -2 next state  7 agent position  (9, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  46 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  46 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 700 steps  46 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  47 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  47 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 701 steps  47 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  48 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  48 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 702 steps  48 reward 0 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  49 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  49 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 703 steps  49 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5811803122766818 agent memory len 786 steps  50 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  50 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 704 steps  50 reward 0 next state  7 agent position  (8, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  51 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  51 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 705 steps  51 reward 0 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  52 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  52 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 706 steps  52 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  53 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  53 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 707 steps  53 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  54 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  54 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 708 steps  54 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  55 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  55 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 709 steps  55 reward -2 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.5811803122766818 agent memory len 786 steps  56 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  56 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 710 steps  56 reward 0 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  57 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  57 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 711 steps  57 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  58 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  58 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 712 steps  58 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  59 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  59 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 713 steps  59 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  60 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  60 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 714 steps  60 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 786 steps  61 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  61 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 715 steps  61 reward 0 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  62 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  62 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 716 steps  62 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  63 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  63 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 717 steps  63 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  64 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  64 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 718 steps  64 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  65 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  65 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 719 steps  65 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  66 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  66 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 720 steps  66 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  67 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  67 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 721 steps  67 reward -2 next state  7 agent position  (9, 9)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  68 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  68 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 722 steps  68 reward 0 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  69 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  69 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 723 steps  69 reward -2 next state  7 agent position  (9, 8)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.5811803122766818 agent memory len 786 steps  70 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  70 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 724 steps  70 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.5811803122766818 agent memory len 786 steps  71 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  71 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 725 steps  71 reward 0 next state  7 agent position  (8, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  72 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  72 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 726 steps  72 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  73 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  73 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 727 steps  73 reward -2 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.5811803122766818 agent memory len 786 steps  74 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  74 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 728 steps  74 reward 0 next state  7 agent position  (9, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.5811803122766818 agent memory len 786 steps  75 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5811803122766818 agent memory len 565 steps  75 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.5811803122766818 agent memory len 729 steps  75 reward -2 next state  7 agent position  (9, 7)
max steps reached
total rewards -198
epsilon  0.5533235197330861
epsilon  0.5533235197330861
epsilon  0.5533235197330861
Episode number:  13
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.5533235197330861 agent memory len 787 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5533235197330861 agent memory len 566 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5533235197330861 agent memory len 730 steps  1 reward -2 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.5533235197330861 agent memory len 788 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.5533235197330861 agent memory len 567 steps  2 reward -1 next state  1 agent position  (0, 3)
agent epsilon  0.5533235197330861 agent memory len 731 steps  2 reward -1 next state  0 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.5533235197330861 agent memory len 789 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 568 steps  3 reward -2 next state  2 agent position  (0, 3)
agent epsilon  0.5533235197330861 agent memory len 732 steps  3 reward -1 next state  0 agent position  (0, 7)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 790 steps  4 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5533235197330861 agent memory len 569 steps  4 reward -1 next state  2 agent position  (1, 3)
agent epsilon  0.5533235197330861 agent memory len 733 steps  4 reward -1 next state  1 agent position  (1, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.5533235197330861 agent memory len 791 steps  5 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5533235197330861 agent memory len 570 steps  5 reward -1 next state  2 agent position  (1, 2)
agent epsilon  0.5533235197330861 agent memory len 734 steps  5 reward -1 next state  1 agent position  (2, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.5533235197330861 agent memory len 792 steps  6 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5533235197330861 agent memory len 571 steps  6 reward -1 next state  2 agent position  (1, 2)
agent epsilon  0.5533235197330861 agent memory len 735 steps  6 reward -1 next state  1 agent position  (1, 7)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 793 steps  7 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5533235197330861 agent memory len 572 steps  7 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5533235197330861 agent memory len 736 steps  7 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 794 steps  8 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5533235197330861 agent memory len 573 steps  8 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5533235197330861 agent memory len 737 steps  8 reward -1 next state  3 agent position  (2, 6)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 795 steps  9 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.5533235197330861 agent memory len 574 steps  9 reward -1 next state  3 agent position  (4, 2)
agent epsilon  0.5533235197330861 agent memory len 738 steps  9 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, False, False]
random action 1
actions ['S', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 796 steps  10 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.5533235197330861 agent memory len 575 steps  10 reward -1 next state  3 agent position  (4, 2)
agent epsilon  0.5533235197330861 agent memory len 739 steps  10 reward -1 next state  4 agent position  (3, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.5533235197330861 agent memory len 797 steps  11 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.5533235197330861 agent memory len 576 steps  11 reward -1 next state  3 agent position  (4, 3)
agent epsilon  0.5533235197330861 agent memory len 740 steps  11 reward -1 next state  4 agent position  (2, 5)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.5533235197330861 agent memory len 798 steps  12 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.5533235197330861 agent memory len 577 steps  12 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5533235197330861 agent memory len 741 steps  12 reward -1 next state  3 agent position  (3, 5)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 799 steps  13 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.5533235197330861 agent memory len 578 steps  13 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5533235197330861 agent memory len 742 steps  13 reward -1 next state  3 agent position  (3, 5)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.5533235197330861 agent memory len 800 steps  14 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.5533235197330861 agent memory len 579 steps  14 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5533235197330861 agent memory len 743 steps  14 reward -1 next state  3 agent position  (4, 5)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 801 steps  15 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.5533235197330861 agent memory len 580 steps  15 reward -1 next state  3 agent position  (4, 2)
agent epsilon  0.5533235197330861 agent memory len 744 steps  15 reward -1 next state  4 agent position  (5, 5)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 802 steps  16 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.5533235197330861 agent memory len 581 steps  16 reward -1 next state  3 agent position  (5, 2)
agent epsilon  0.5533235197330861 agent memory len 745 steps  16 reward -1 next state  5 agent position  (5, 4)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.5533235197330861 agent memory len 803 steps  17 reward 0 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 582 steps  17 reward -1 next state  3 agent position  (5, 3)
agent epsilon  0.5533235197330861 agent memory len 746 steps  17 reward -1 next state  5 agent position  (4, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 804 steps  18 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 583 steps  18 reward -1 next state  3 agent position  (5, 2)
agent epsilon  0.5533235197330861 agent memory len 747 steps  18 reward -1 next state  5 agent position  (4, 4)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.5533235197330861 agent memory len 805 steps  19 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 584 steps  19 reward -1 next state  3 agent position  (5, 1)
agent epsilon  0.5533235197330861 agent memory len 748 steps  19 reward -1 next state  5 agent position  (4, 5)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 806 steps  20 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 585 steps  20 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.5533235197330861 agent memory len 749 steps  20 reward -1 next state  6 agent position  (5, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'L']
agent epsilon  0.5533235197330861 agent memory len 807 steps  21 reward 0 next state  8 agent position  (8, 4)
agent epsilon  0.5533235197330861 agent memory len 586 steps  21 reward -1 next state  4 agent position  (5, 1)
agent epsilon  0.5533235197330861 agent memory len 750 steps  21 reward -1 next state  5 agent position  (5, 4)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 808 steps  22 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 587 steps  22 reward -1 next state  4 agent position  (6, 1)
agent epsilon  0.5533235197330861 agent memory len 751 steps  22 reward 0 next state  6 agent position  (6, 4)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 809 steps  23 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 588 steps  23 reward -1 next state  4 agent position  (6, 0)
agent epsilon  0.5533235197330861 agent memory len 752 steps  23 reward 0 next state  6 agent position  (6, 4)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 810 steps  24 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 589 steps  24 reward -1 next state  4 agent position  (7, 0)
agent epsilon  0.5533235197330861 agent memory len 753 steps  24 reward 0 next state  7 agent position  (6, 3)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 811 steps  25 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 590 steps  25 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.5533235197330861 agent memory len 754 steps  25 reward 0 next state  8 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'L']
agent epsilon  0.5533235197330861 agent memory len 812 steps  26 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 591 steps  26 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.5533235197330861 agent memory len 755 steps  26 reward -1 next state  8 agent position  (7, 2)
 is_terminal [False, False, False]
actions ['D', 'D', 'R']
agent epsilon  0.5533235197330861 agent memory len 813 steps  27 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 592 steps  27 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 756 steps  27 reward 0 next state  9 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 814 steps  28 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 593 steps  28 reward -2 next state  4 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 757 steps  28 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.5533235197330861 agent memory len 815 steps  29 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 594 steps  29 reward -2 next state  4 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 758 steps  29 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.5533235197330861 agent memory len 816 steps  30 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 595 steps  30 reward -2 next state  4 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 759 steps  30 reward 0 next state  9 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 817 steps  31 reward 0 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 596 steps  31 reward -2 next state  3 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 760 steps  31 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 818 steps  32 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 597 steps  32 reward -1 next state  3 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 761 steps  32 reward 0 next state  9 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'L']
agent epsilon  0.5533235197330861 agent memory len 819 steps  33 reward 0 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 598 steps  33 reward -2 next state  3 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 762 steps  33 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.5533235197330861 agent memory len 820 steps  34 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 599 steps  34 reward -2 next state  3 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 763 steps  34 reward -1 next state  9 agent position  (6, 2)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 821 steps  35 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 600 steps  35 reward -2 next state  4 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 764 steps  35 reward -1 next state  9 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.5533235197330861 agent memory len 822 steps  36 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 601 steps  36 reward -2 next state  5 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 765 steps  36 reward -1 next state  9 agent position  (6, 0)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 823 steps  37 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 602 steps  37 reward -2 next state  5 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 766 steps  37 reward -2 next state  9 agent position  (6, 0)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.5533235197330861 agent memory len 824 steps  38 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 603 steps  38 reward -1 next state  5 agent position  (8, 0)
agent epsilon  0.5533235197330861 agent memory len 767 steps  38 reward -1 next state  8 agent position  (6, 1)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.5533235197330861 agent memory len 825 steps  39 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 604 steps  39 reward -1 next state  5 agent position  (9, 0)
agent epsilon  0.5533235197330861 agent memory len 768 steps  39 reward -1 next state  9 agent position  (7, 1)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'R']
agent epsilon  0.5533235197330861 agent memory len 826 steps  40 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 605 steps  40 reward 0 next state  5 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 769 steps  40 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.5533235197330861 agent memory len 827 steps  41 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 606 steps  41 reward -2 next state  5 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 770 steps  41 reward -1 next state  9 agent position  (7, 1)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 828 steps  42 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 607 steps  42 reward 0 next state  6 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 771 steps  42 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.5533235197330861 agent memory len 829 steps  43 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 608 steps  43 reward -2 next state  6 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 772 steps  43 reward 0 next state  9 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['R', 'R', 'L']
agent epsilon  0.5533235197330861 agent memory len 830 steps  44 reward 0 next state  9 agent position  (9, 7)
agent epsilon  0.5533235197330861 agent memory len 609 steps  44 reward 10 next state  7 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 773 steps  44 reward -1 next state  9 agent position  (7, 2)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 831 steps  45 reward 0 next state  9 agent position  (9, 7)
agent epsilon  0.5533235197330861 agent memory len 609 steps  45 reward -1 next state  7 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 774 steps  45 reward 0 next state  9 agent position  (7, 3)
 is_terminal [False, True, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['D', 'S', 'R']
agent epsilon  0.5533235197330861 agent memory len 832 steps  46 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.5533235197330861 agent memory len 609 steps  46 reward -1 next state  7 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  46 reward 10 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 833 steps  47 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  47 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  47 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 834 steps  48 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  48 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  48 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 835 steps  49 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  49 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  49 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 836 steps  50 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  50 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  50 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 837 steps  51 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  51 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  51 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 838 steps  52 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  52 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  52 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 839 steps  53 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  53 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  53 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 840 steps  54 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.5533235197330861 agent memory len 609 steps  54 reward -1 next state  6 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  54 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 841 steps  55 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.5533235197330861 agent memory len 609 steps  55 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  55 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 842 steps  56 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.5533235197330861 agent memory len 609 steps  56 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  56 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 843 steps  57 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.5533235197330861 agent memory len 609 steps  57 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  57 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 844 steps  58 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.5533235197330861 agent memory len 609 steps  58 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  58 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 845 steps  59 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 609 steps  59 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  59 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 846 steps  60 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 609 steps  60 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  60 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 847 steps  61 reward -2 next state  9 agent position  (9, 4)
agent epsilon  0.5533235197330861 agent memory len 609 steps  61 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  61 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 848 steps  62 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  62 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  62 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 849 steps  63 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  63 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  63 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 850 steps  64 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  64 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  64 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 851 steps  65 reward -2 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  65 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  65 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 852 steps  66 reward -1 next state  8 agent position  (8, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  66 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  66 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 853 steps  67 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  67 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  67 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 854 steps  68 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 609 steps  68 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  68 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 855 steps  69 reward -2 next state  9 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 609 steps  69 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  69 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 856 steps  70 reward -1 next state  9 agent position  (9, 3)
agent epsilon  0.5533235197330861 agent memory len 609 steps  70 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  70 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 857 steps  71 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 609 steps  71 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  71 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 858 steps  72 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 609 steps  72 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  72 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 859 steps  73 reward -1 next state  9 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 609 steps  73 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  73 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 860 steps  74 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 609 steps  74 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  74 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.5533235197330861 agent memory len 861 steps  75 reward -2 next state  9 agent position  (9, 1)
agent epsilon  0.5533235197330861 agent memory len 609 steps  75 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.5533235197330861 agent memory len 775 steps  75 reward -1 next state  9 agent position  (7, 4)
max steps reached
total rewards -223
epsilon  0.5268253189934059
epsilon  0.5268253189934059
epsilon  0.5268253189934059
Episode number:  14
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'R']
agent epsilon  0.5268253189934059 agent memory len 862 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5268253189934059 agent memory len 610 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.5268253189934059 agent memory len 776 steps  1 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 863 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.5268253189934059 agent memory len 611 steps  2 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.5268253189934059 agent memory len 777 steps  2 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'L', 'D']
agent epsilon  0.5268253189934059 agent memory len 864 steps  3 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.5268253189934059 agent memory len 612 steps  3 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.5268253189934059 agent memory len 778 steps  3 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 865 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5268253189934059 agent memory len 613 steps  4 reward -1 next state  0 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 779 steps  4 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 866 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5268253189934059 agent memory len 614 steps  5 reward -1 next state  0 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 780 steps  5 reward -1 next state  4 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 867 steps  6 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.5268253189934059 agent memory len 615 steps  6 reward -1 next state  0 agent position  (5, 4)
agent epsilon  0.5268253189934059 agent memory len 781 steps  6 reward -1 next state  5 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.5268253189934059 agent memory len 868 steps  7 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.5268253189934059 agent memory len 616 steps  7 reward -1 next state  0 agent position  (5, 3)
agent epsilon  0.5268253189934059 agent memory len 782 steps  7 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 869 steps  8 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.5268253189934059 agent memory len 617 steps  8 reward 0 next state  1 agent position  (6, 3)
agent epsilon  0.5268253189934059 agent memory len 783 steps  8 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'U', 'D']
agent epsilon  0.5268253189934059 agent memory len 870 steps  9 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.5268253189934059 agent memory len 618 steps  9 reward -1 next state  1 agent position  (5, 3)
agent epsilon  0.5268253189934059 agent memory len 784 steps  9 reward 0 next state  5 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 871 steps  10 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.5268253189934059 agent memory len 619 steps  10 reward 0 next state  1 agent position  (6, 3)
agent epsilon  0.5268253189934059 agent memory len 785 steps  10 reward 0 next state  6 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'U']
agent epsilon  0.5268253189934059 agent memory len 872 steps  11 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.5268253189934059 agent memory len 620 steps  11 reward 0 next state  2 agent position  (6, 4)
agent epsilon  0.5268253189934059 agent memory len 786 steps  11 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 873 steps  12 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.5268253189934059 agent memory len 621 steps  12 reward 0 next state  3 agent position  (6, 4)
agent epsilon  0.5268253189934059 agent memory len 787 steps  12 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'R', 'U']
agent epsilon  0.5268253189934059 agent memory len 874 steps  13 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 622 steps  13 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.5268253189934059 agent memory len 788 steps  13 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 875 steps  14 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 623 steps  14 reward -1 next state  5 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 789 steps  14 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['L', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 876 steps  15 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 624 steps  15 reward -1 next state  4 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 790 steps  15 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.5268253189934059 agent memory len 877 steps  16 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.5268253189934059 agent memory len 625 steps  16 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 791 steps  16 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 878 steps  17 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 626 steps  17 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 792 steps  17 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 879 steps  18 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.5268253189934059 agent memory len 627 steps  18 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.5268253189934059 agent memory len 793 steps  18 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'R', 'D']
agent epsilon  0.5268253189934059 agent memory len 880 steps  19 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 628 steps  19 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 794 steps  19 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 881 steps  20 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.5268253189934059 agent memory len 629 steps  20 reward -1 next state  5 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 795 steps  20 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.5268253189934059 agent memory len 882 steps  21 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 630 steps  21 reward -1 next state  4 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 796 steps  21 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 883 steps  22 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 631 steps  22 reward 0 next state  4 agent position  (7, 7)
agent epsilon  0.5268253189934059 agent memory len 797 steps  22 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  0.5268253189934059 agent memory len 884 steps  23 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.5268253189934059 agent memory len 632 steps  23 reward -1 next state  3 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 798 steps  23 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 885 steps  24 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.5268253189934059 agent memory len 633 steps  24 reward 0 next state  3 agent position  (7, 7)
agent epsilon  0.5268253189934059 agent memory len 799 steps  24 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'U', 'R']
agent epsilon  0.5268253189934059 agent memory len 886 steps  25 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 634 steps  25 reward -1 next state  3 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 800 steps  25 reward -2 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'U', 'L']
agent epsilon  0.5268253189934059 agent memory len 887 steps  26 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 635 steps  26 reward -1 next state  4 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 801 steps  26 reward -1 next state  5 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 888 steps  27 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.5268253189934059 agent memory len 636 steps  27 reward -1 next state  4 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 802 steps  27 reward -1 next state  6 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 889 steps  28 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 637 steps  28 reward 0 next state  4 agent position  (7, 7)
agent epsilon  0.5268253189934059 agent memory len 803 steps  28 reward -1 next state  7 agent position  (5, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'D', 'D']
agent epsilon  0.5268253189934059 agent memory len 890 steps  29 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 638 steps  29 reward 0 next state  3 agent position  (8, 7)
agent epsilon  0.5268253189934059 agent memory len 804 steps  29 reward -1 next state  8 agent position  (6, 8)
 is_terminal [False, False, False]
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 891 steps  30 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5268253189934059 agent memory len 639 steps  30 reward 0 next state  2 agent position  (7, 7)
agent epsilon  0.5268253189934059 agent memory len 805 steps  30 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, False, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 892 steps  31 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 640 steps  31 reward 0 next state  3 agent position  (7, 7)
agent epsilon  0.5268253189934059 agent memory len 806 steps  31 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'U', 'R']
agent epsilon  0.5268253189934059 agent memory len 893 steps  32 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5268253189934059 agent memory len 641 steps  32 reward -1 next state  2 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 807 steps  32 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'R', 'D']
agent epsilon  0.5268253189934059 agent memory len 894 steps  33 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.5268253189934059 agent memory len 642 steps  33 reward -1 next state  1 agent position  (6, 8)
agent epsilon  0.5268253189934059 agent memory len 808 steps  33 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.5268253189934059 agent memory len 895 steps  34 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.5268253189934059 agent memory len 643 steps  34 reward -1 next state  1 agent position  (6, 8)
agent epsilon  0.5268253189934059 agent memory len 809 steps  34 reward 0 next state  6 agent position  (7, 8)
 is_terminal [False, False, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.5268253189934059 agent memory len 896 steps  35 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 644 steps  35 reward -1 next state  2 agent position  (6, 8)
agent epsilon  0.5268253189934059 agent memory len 810 steps  35 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.5268253189934059 agent memory len 897 steps  36 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 645 steps  36 reward -1 next state  3 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 811 steps  36 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
actions ['D', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 898 steps  37 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 646 steps  37 reward -1 next state  3 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 812 steps  37 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.5268253189934059 agent memory len 899 steps  38 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 647 steps  38 reward -1 next state  4 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 813 steps  38 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.5268253189934059 agent memory len 900 steps  39 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.5268253189934059 agent memory len 648 steps  39 reward -1 next state  5 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 814 steps  39 reward -2 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'L', 'D']
agent epsilon  0.5268253189934059 agent memory len 901 steps  40 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.5268253189934059 agent memory len 649 steps  40 reward 0 next state  5 agent position  (6, 5)
agent epsilon  0.5268253189934059 agent memory len 815 steps  40 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
actions ['D', 'R', 'D']
agent epsilon  0.5268253189934059 agent memory len 902 steps  41 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.5268253189934059 agent memory len 650 steps  41 reward -1 next state  5 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 816 steps  41 reward 0 next state  6 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 903 steps  42 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.5268253189934059 agent memory len 651 steps  42 reward -1 next state  6 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 817 steps  42 reward 0 next state  5 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 904 steps  43 reward -1 next state  3 agent position  (3, 7)
agent epsilon  0.5268253189934059 agent memory len 652 steps  43 reward -1 next state  7 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 818 steps  43 reward 0 next state  5 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 905 steps  44 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.5268253189934059 agent memory len 653 steps  44 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 819 steps  44 reward 0 next state  6 agent position  (9, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'U', 'R']
agent epsilon  0.5268253189934059 agent memory len 906 steps  45 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.5268253189934059 agent memory len 654 steps  45 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 820 steps  45 reward -2 next state  5 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
actions ['L', 'U', 'U']
agent epsilon  0.5268253189934059 agent memory len 907 steps  46 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 655 steps  46 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.5268253189934059 agent memory len 821 steps  46 reward 0 next state  4 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'D']
agent epsilon  0.5268253189934059 agent memory len 908 steps  47 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.5268253189934059 agent memory len 656 steps  47 reward -1 next state  5 agent position  (4, 7)
agent epsilon  0.5268253189934059 agent memory len 822 steps  47 reward 0 next state  4 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'U']
agent epsilon  0.5268253189934059 agent memory len 909 steps  48 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.5268253189934059 agent memory len 657 steps  48 reward -1 next state  4 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 823 steps  48 reward 0 next state  5 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['L', 'L', 'L']
agent epsilon  0.5268253189934059 agent memory len 910 steps  49 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 658 steps  49 reward -1 next state  3 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  49 reward 10 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 911 steps  50 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 659 steps  50 reward -1 next state  3 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  50 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 912 steps  51 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 660 steps  51 reward -1 next state  2 agent position  (7, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 913 steps  52 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 661 steps  52 reward -1 next state  3 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  52 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 914 steps  53 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 662 steps  53 reward 0 next state  2 agent position  (6, 5)
agent epsilon  0.5268253189934059 agent memory len 824 steps  53 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 915 steps  54 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5268253189934059 agent memory len 663 steps  54 reward -1 next state  2 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  54 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 916 steps  55 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 664 steps  55 reward -1 next state  3 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 824 steps  55 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 917 steps  56 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5268253189934059 agent memory len 665 steps  56 reward -1 next state  2 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 824 steps  56 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 918 steps  57 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5268253189934059 agent memory len 666 steps  57 reward -1 next state  2 agent position  (6, 7)
agent epsilon  0.5268253189934059 agent memory len 824 steps  57 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 919 steps  58 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.5268253189934059 agent memory len 667 steps  58 reward -1 next state  3 agent position  (6, 8)
agent epsilon  0.5268253189934059 agent memory len 824 steps  58 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 920 steps  59 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 668 steps  59 reward -1 next state  3 agent position  (6, 8)
agent epsilon  0.5268253189934059 agent memory len 824 steps  59 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['L', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 921 steps  60 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 669 steps  60 reward -1 next state  2 agent position  (5, 8)
agent epsilon  0.5268253189934059 agent memory len 824 steps  60 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 922 steps  61 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 670 steps  61 reward -1 next state  2 agent position  (5, 8)
agent epsilon  0.5268253189934059 agent memory len 824 steps  61 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 923 steps  62 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.5268253189934059 agent memory len 671 steps  62 reward -1 next state  1 agent position  (5, 7)
agent epsilon  0.5268253189934059 agent memory len 824 steps  62 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 924 steps  63 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5268253189934059 agent memory len 672 steps  63 reward -1 next state  2 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  63 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 925 steps  64 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 673 steps  64 reward -1 next state  3 agent position  (5, 5)
agent epsilon  0.5268253189934059 agent memory len 824 steps  64 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 926 steps  65 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.5268253189934059 agent memory len 674 steps  65 reward -1 next state  4 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  65 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 927 steps  66 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.5268253189934059 agent memory len 675 steps  66 reward -1 next state  3 agent position  (4, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  66 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 928 steps  67 reward -1 next state  1 agent position  (1, 3)
agent epsilon  0.5268253189934059 agent memory len 676 steps  67 reward -1 next state  3 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  67 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.5268253189934059 agent memory len 929 steps  68 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5268253189934059 agent memory len 677 steps  68 reward -1 next state  4 agent position  (5, 5)
agent epsilon  0.5268253189934059 agent memory len 824 steps  68 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'R', 'S']
agent epsilon  0.5268253189934059 agent memory len 930 steps  69 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.5268253189934059 agent memory len 678 steps  69 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  69 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 931 steps  70 reward -1 next state  1 agent position  (1, 4)
agent epsilon  0.5268253189934059 agent memory len 679 steps  70 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  70 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 932 steps  71 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.5268253189934059 agent memory len 680 steps  71 reward -1 next state  4 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  71 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.5268253189934059 agent memory len 933 steps  72 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.5268253189934059 agent memory len 681 steps  72 reward -1 next state  3 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  72 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.5268253189934059 agent memory len 934 steps  73 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.5268253189934059 agent memory len 682 steps  73 reward -1 next state  2 agent position  (4, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  73 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['D', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 935 steps  74 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5268253189934059 agent memory len 683 steps  74 reward -1 next state  2 agent position  (5, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  74 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'D', 'S']
agent epsilon  0.5268253189934059 agent memory len 936 steps  75 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5268253189934059 agent memory len 684 steps  75 reward -1 next state  2 agent position  (6, 6)
agent epsilon  0.5268253189934059 agent memory len 824 steps  75 reward -1 next state  6 agent position  (8, 8)
max steps reached
total rewards -180
epsilon  0.5016194507534953
epsilon  0.5016194507534953
epsilon  0.5016194507534953
Episode number:  15
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.5016194507534953 agent memory len 937 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 685 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.5016194507534953 agent memory len 825 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'D']
agent epsilon  0.5016194507534953 agent memory len 938 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 686 steps  2 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5016194507534953 agent memory len 826 steps  2 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.5016194507534953 agent memory len 939 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 687 steps  3 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.5016194507534953 agent memory len 827 steps  3 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 940 steps  4 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.5016194507534953 agent memory len 688 steps  4 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.5016194507534953 agent memory len 828 steps  4 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 941 steps  5 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.5016194507534953 agent memory len 689 steps  5 reward -1 next state  1 agent position  (0, 5)
agent epsilon  0.5016194507534953 agent memory len 829 steps  5 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 942 steps  6 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.5016194507534953 agent memory len 690 steps  6 reward -1 next state  1 agent position  (0, 4)
agent epsilon  0.5016194507534953 agent memory len 830 steps  6 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 943 steps  7 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5016194507534953 agent memory len 691 steps  7 reward -1 next state  2 agent position  (0, 3)
agent epsilon  0.5016194507534953 agent memory len 831 steps  7 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.5016194507534953 agent memory len 944 steps  8 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5016194507534953 agent memory len 692 steps  8 reward -1 next state  2 agent position  (0, 2)
agent epsilon  0.5016194507534953 agent memory len 832 steps  8 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 945 steps  9 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 693 steps  9 reward -2 next state  1 agent position  (0, 2)
agent epsilon  0.5016194507534953 agent memory len 833 steps  9 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 946 steps  10 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5016194507534953 agent memory len 694 steps  10 reward -1 next state  2 agent position  (0, 1)
agent epsilon  0.5016194507534953 agent memory len 834 steps  10 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 947 steps  11 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.5016194507534953 agent memory len 695 steps  11 reward -1 next state  2 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 835 steps  11 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 948 steps  12 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.5016194507534953 agent memory len 696 steps  12 reward -1 next state  2 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 836 steps  12 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'S']
agent epsilon  0.5016194507534953 agent memory len 949 steps  13 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.5016194507534953 agent memory len 697 steps  13 reward -2 next state  2 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 837 steps  13 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'D']
agent epsilon  0.5016194507534953 agent memory len 950 steps  14 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.5016194507534953 agent memory len 698 steps  14 reward -1 next state  2 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 838 steps  14 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 951 steps  15 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.5016194507534953 agent memory len 699 steps  15 reward -2 next state  2 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 839 steps  15 reward -2 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 952 steps  16 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.5016194507534953 agent memory len 700 steps  16 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 840 steps  16 reward -2 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 953 steps  17 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.5016194507534953 agent memory len 701 steps  17 reward -2 next state  2 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 841 steps  17 reward -2 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 954 steps  18 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.5016194507534953 agent memory len 702 steps  18 reward -1 next state  3 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 842 steps  18 reward -2 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.5016194507534953 agent memory len 955 steps  19 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.5016194507534953 agent memory len 703 steps  19 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 843 steps  19 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 956 steps  20 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.5016194507534953 agent memory len 704 steps  20 reward -2 next state  2 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 844 steps  20 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'D']
agent epsilon  0.5016194507534953 agent memory len 957 steps  21 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.5016194507534953 agent memory len 705 steps  21 reward -2 next state  3 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 845 steps  21 reward -1 next state  0 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 958 steps  22 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.5016194507534953 agent memory len 706 steps  22 reward -2 next state  4 agent position  (0, 0)
agent epsilon  0.5016194507534953 agent memory len 846 steps  22 reward -2 next state  0 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.5016194507534953 agent memory len 959 steps  23 reward 0 next state  6 agent position  (6, 5)
agent epsilon  0.5016194507534953 agent memory len 707 steps  23 reward -1 next state  5 agent position  (0, 1)
agent epsilon  0.5016194507534953 agent memory len 847 steps  23 reward -2 next state  0 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.5016194507534953 agent memory len 960 steps  24 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.5016194507534953 agent memory len 708 steps  24 reward -1 next state  6 agent position  (0, 2)
agent epsilon  0.5016194507534953 agent memory len 848 steps  24 reward -1 next state  0 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.5016194507534953 agent memory len 961 steps  25 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.5016194507534953 agent memory len 709 steps  25 reward -1 next state  7 agent position  (1, 2)
agent epsilon  0.5016194507534953 agent memory len 849 steps  25 reward -1 next state  1 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 962 steps  26 reward 0 next state  7 agent position  (7, 7)
agent epsilon  0.5016194507534953 agent memory len 710 steps  26 reward -1 next state  7 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 850 steps  26 reward -1 next state  1 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 963 steps  27 reward 0 next state  7 agent position  (7, 8)
agent epsilon  0.5016194507534953 agent memory len 711 steps  27 reward -1 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 851 steps  27 reward -2 next state  1 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.5016194507534953 agent memory len 964 steps  28 reward 0 next state  7 agent position  (7, 9)
agent epsilon  0.5016194507534953 agent memory len 712 steps  28 reward -1 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 852 steps  28 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['U', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 965 steps  29 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.5016194507534953 agent memory len 713 steps  29 reward -2 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 853 steps  29 reward -2 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 966 steps  30 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.5016194507534953 agent memory len 714 steps  30 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 854 steps  30 reward -2 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'R', 'L']
agent epsilon  0.5016194507534953 agent memory len 967 steps  31 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 715 steps  31 reward -1 next state  8 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 855 steps  31 reward -1 next state  2 agent position  (3, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.5016194507534953 agent memory len 968 steps  32 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 716 steps  32 reward -1 next state  8 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 856 steps  32 reward -1 next state  1 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 969 steps  33 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 717 steps  33 reward -1 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 857 steps  33 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 970 steps  34 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.5016194507534953 agent memory len 718 steps  34 reward -2 next state  9 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 858 steps  34 reward -2 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 971 steps  35 reward -2 next state  5 agent position  (5, 9)
agent epsilon  0.5016194507534953 agent memory len 719 steps  35 reward -2 next state  9 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 859 steps  35 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 972 steps  36 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.5016194507534953 agent memory len 720 steps  36 reward -1 next state  9 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 860 steps  36 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  0.5016194507534953 agent memory len 973 steps  37 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 721 steps  37 reward -1 next state  8 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 861 steps  37 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 974 steps  38 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 722 steps  38 reward -1 next state  8 agent position  (0, 1)
agent epsilon  0.5016194507534953 agent memory len 862 steps  38 reward -2 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 975 steps  39 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 723 steps  39 reward -1 next state  8 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 863 steps  39 reward -2 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 976 steps  40 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.5016194507534953 agent memory len 724 steps  40 reward -1 next state  7 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 864 steps  40 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.5016194507534953 agent memory len 977 steps  41 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 725 steps  41 reward -2 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 865 steps  41 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 978 steps  42 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.5016194507534953 agent memory len 726 steps  42 reward -1 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 866 steps  42 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 979 steps  43 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.5016194507534953 agent memory len 727 steps  43 reward -2 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 867 steps  43 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.5016194507534953 agent memory len 980 steps  44 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 728 steps  44 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 868 steps  44 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 981 steps  45 reward -1 next state  4 agent position  (4, 9)
agent epsilon  0.5016194507534953 agent memory len 729 steps  45 reward -2 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 869 steps  45 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.5016194507534953 agent memory len 982 steps  46 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 730 steps  46 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 870 steps  46 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 983 steps  47 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 731 steps  47 reward -1 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 871 steps  47 reward -2 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 984 steps  48 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 732 steps  48 reward -1 next state  8 agent position  (3, 0)
agent epsilon  0.5016194507534953 agent memory len 872 steps  48 reward -2 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'D', 'L']
agent epsilon  0.5016194507534953 agent memory len 985 steps  49 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.5016194507534953 agent memory len 733 steps  49 reward -1 next state  8 agent position  (4, 0)
agent epsilon  0.5016194507534953 agent memory len 873 steps  49 reward -1 next state  4 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 986 steps  50 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 734 steps  50 reward -2 next state  8 agent position  (4, 0)
agent epsilon  0.5016194507534953 agent memory len 874 steps  50 reward -1 next state  4 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 987 steps  51 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.5016194507534953 agent memory len 735 steps  51 reward -1 next state  8 agent position  (4, 0)
agent epsilon  0.5016194507534953 agent memory len 875 steps  51 reward -1 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['U', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 988 steps  52 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.5016194507534953 agent memory len 736 steps  52 reward -2 next state  8 agent position  (4, 0)
agent epsilon  0.5016194507534953 agent memory len 876 steps  52 reward -2 next state  4 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 989 steps  53 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.5016194507534953 agent memory len 737 steps  53 reward -1 next state  8 agent position  (3, 0)
agent epsilon  0.5016194507534953 agent memory len 877 steps  53 reward -2 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 990 steps  54 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 738 steps  54 reward -1 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 878 steps  54 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 991 steps  55 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.5016194507534953 agent memory len 739 steps  55 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 879 steps  55 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.5016194507534953 agent memory len 992 steps  56 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.5016194507534953 agent memory len 740 steps  56 reward -1 next state  9 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 880 steps  56 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'R', 'R']
agent epsilon  0.5016194507534953 agent memory len 993 steps  57 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.5016194507534953 agent memory len 741 steps  57 reward -1 next state  8 agent position  (2, 2)
agent epsilon  0.5016194507534953 agent memory len 881 steps  57 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 994 steps  58 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.5016194507534953 agent memory len 742 steps  58 reward -1 next state  9 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 882 steps  58 reward -2 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 995 steps  59 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 743 steps  59 reward -1 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 883 steps  59 reward -1 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 996 steps  60 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 744 steps  60 reward -2 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 884 steps  60 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 997 steps  61 reward -1 next state  3 agent position  (3, 9)
agent epsilon  0.5016194507534953 agent memory len 745 steps  61 reward -1 next state  9 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 885 steps  61 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['L', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 998 steps  62 reward -1 next state  3 agent position  (3, 8)
agent epsilon  0.5016194507534953 agent memory len 746 steps  62 reward -2 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 886 steps  62 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.5016194507534953 agent memory len 999 steps  63 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 747 steps  63 reward -1 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 887 steps  63 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1000 steps  64 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.5016194507534953 agent memory len 748 steps  64 reward -2 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 888 steps  64 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['D', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1001 steps  65 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 749 steps  65 reward -2 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 889 steps  65 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1002 steps  66 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 750 steps  66 reward -2 next state  9 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 890 steps  66 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.5016194507534953 agent memory len 1003 steps  67 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 751 steps  67 reward -1 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 891 steps  67 reward -1 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1004 steps  68 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 752 steps  68 reward -2 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 892 steps  68 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'U', 'R']
agent epsilon  0.5016194507534953 agent memory len 1005 steps  69 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 753 steps  69 reward -1 next state  8 agent position  (1, 0)
agent epsilon  0.5016194507534953 agent memory len 893 steps  69 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'R', 'R']
agent epsilon  0.5016194507534953 agent memory len 1006 steps  70 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 754 steps  70 reward -1 next state  9 agent position  (1, 1)
agent epsilon  0.5016194507534953 agent memory len 894 steps  70 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.5016194507534953 agent memory len 1007 steps  71 reward -2 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 755 steps  71 reward -1 next state  9 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 895 steps  71 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['L', 'S', 'R']
agent epsilon  0.5016194507534953 agent memory len 1008 steps  72 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 756 steps  72 reward -1 next state  8 agent position  (2, 1)
agent epsilon  0.5016194507534953 agent memory len 896 steps  72 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1009 steps  73 reward -1 next state  2 agent position  (2, 9)
agent epsilon  0.5016194507534953 agent memory len 757 steps  73 reward -1 next state  9 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 897 steps  73 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'L', 'R']
agent epsilon  0.5016194507534953 agent memory len 1010 steps  74 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.5016194507534953 agent memory len 758 steps  74 reward -2 next state  8 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 898 steps  74 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'U']
agent epsilon  0.5016194507534953 agent memory len 1011 steps  75 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.5016194507534953 agent memory len 759 steps  75 reward -2 next state  7 agent position  (2, 0)
agent epsilon  0.5016194507534953 agent memory len 899 steps  75 reward -2 next state  2 agent position  (0, 9)
max steps reached
total rewards -291
epsilon  0.47764288721360454
epsilon  0.47764288721360454
epsilon  0.47764288721360454
Episode number:  16
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.47764288721360454 agent memory len 1012 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.47764288721360454 agent memory len 760 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.47764288721360454 agent memory len 900 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1013 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.47764288721360454 agent memory len 761 steps  2 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.47764288721360454 agent memory len 901 steps  2 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1014 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.47764288721360454 agent memory len 762 steps  3 reward -1 next state  1 agent position  (2, 6)
agent epsilon  0.47764288721360454 agent memory len 902 steps  3 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.47764288721360454 agent memory len 1015 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.47764288721360454 agent memory len 763 steps  4 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.47764288721360454 agent memory len 903 steps  4 reward -1 next state  1 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1016 steps  5 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.47764288721360454 agent memory len 764 steps  5 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.47764288721360454 agent memory len 904 steps  5 reward -1 next state  1 agent position  (5, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1017 steps  6 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.47764288721360454 agent memory len 765 steps  6 reward -1 next state  0 agent position  (2, 6)
agent epsilon  0.47764288721360454 agent memory len 905 steps  6 reward -1 next state  2 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'R']
agent epsilon  0.47764288721360454 agent memory len 1018 steps  7 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.47764288721360454 agent memory len 766 steps  7 reward -1 next state  1 agent position  (2, 7)
agent epsilon  0.47764288721360454 agent memory len 906 steps  7 reward -2 next state  2 agent position  (6, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1019 steps  8 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.47764288721360454 agent memory len 767 steps  8 reward -1 next state  2 agent position  (3, 7)
agent epsilon  0.47764288721360454 agent memory len 907 steps  8 reward 0 next state  3 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1020 steps  9 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.47764288721360454 agent memory len 768 steps  9 reward -1 next state  2 agent position  (4, 7)
agent epsilon  0.47764288721360454 agent memory len 908 steps  9 reward 0 next state  4 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1021 steps  10 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 769 steps  10 reward -1 next state  3 agent position  (5, 7)
agent epsilon  0.47764288721360454 agent memory len 909 steps  10 reward 0 next state  5 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1022 steps  11 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 770 steps  11 reward -1 next state  3 agent position  (6, 7)
agent epsilon  0.47764288721360454 agent memory len 910 steps  11 reward -2 next state  6 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1023 steps  12 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 771 steps  12 reward 0 next state  4 agent position  (7, 7)
agent epsilon  0.47764288721360454 agent memory len 911 steps  12 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'R', 'U']
agent epsilon  0.47764288721360454 agent memory len 1024 steps  13 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 772 steps  13 reward 0 next state  3 agent position  (7, 8)
agent epsilon  0.47764288721360454 agent memory len 912 steps  13 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 1
actions ['R', 'D', 'D']
agent epsilon  0.47764288721360454 agent memory len 1025 steps  14 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  14 reward 10 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 913 steps  14 reward -1 next state  8 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1026 steps  15 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  15 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 914 steps  15 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1027 steps  16 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  16 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 915 steps  16 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 1028 steps  17 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  17 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 916 steps  17 reward -2 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1029 steps  18 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  18 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 917 steps  18 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1030 steps  19 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  19 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 918 steps  19 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1031 steps  20 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  20 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 919 steps  20 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 1032 steps  21 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  21 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 920 steps  21 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1033 steps  22 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  22 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 921 steps  22 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1034 steps  23 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  23 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 922 steps  23 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1035 steps  24 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  24 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 923 steps  24 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1036 steps  25 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  25 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 924 steps  25 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1037 steps  26 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  26 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 925 steps  26 reward -1 next state  8 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1038 steps  27 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  27 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 926 steps  27 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1039 steps  28 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  28 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 927 steps  28 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 1040 steps  29 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  29 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 928 steps  29 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1041 steps  30 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  30 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 929 steps  30 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1042 steps  31 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  31 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 930 steps  31 reward -1 next state  8 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1043 steps  32 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  32 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 931 steps  32 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1044 steps  33 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  33 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 932 steps  33 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1045 steps  34 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  34 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 933 steps  34 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1046 steps  35 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  35 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 934 steps  35 reward -1 next state  8 agent position  (7, 9)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1047 steps  36 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  36 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 935 steps  36 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1048 steps  37 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  37 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 936 steps  37 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1049 steps  38 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  38 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 937 steps  38 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1050 steps  39 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  39 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 938 steps  39 reward -1 next state  8 agent position  (6, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1051 steps  40 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  40 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 939 steps  40 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1052 steps  41 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  41 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 940 steps  41 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1053 steps  42 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  42 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 941 steps  42 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1054 steps  43 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  43 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 942 steps  43 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1055 steps  44 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  44 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 943 steps  44 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1056 steps  45 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  45 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 944 steps  45 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1057 steps  46 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  46 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 945 steps  46 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1058 steps  47 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  47 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 946 steps  47 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.47764288721360454 agent memory len 1059 steps  48 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  48 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 947 steps  48 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1060 steps  49 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  49 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 948 steps  49 reward -1 next state  8 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1061 steps  50 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  50 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 949 steps  50 reward -1 next state  8 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1062 steps  51 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  51 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 950 steps  51 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1063 steps  52 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.47764288721360454 agent memory len 773 steps  52 reward -1 next state  3 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 951 steps  52 reward -1 next state  8 agent position  (7, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1064 steps  53 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  53 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 952 steps  53 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1065 steps  54 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.47764288721360454 agent memory len 773 steps  54 reward -1 next state  4 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 953 steps  54 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1066 steps  55 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  55 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 954 steps  55 reward -1 next state  8 agent position  (8, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1067 steps  56 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  56 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 955 steps  56 reward -1 next state  8 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1068 steps  57 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  57 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 956 steps  57 reward -1 next state  8 agent position  (6, 7)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.47764288721360454 agent memory len 1069 steps  58 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  58 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 957 steps  58 reward -1 next state  8 agent position  (5, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1070 steps  59 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.47764288721360454 agent memory len 773 steps  59 reward -1 next state  5 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 958 steps  59 reward -1 next state  8 agent position  (5, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1071 steps  60 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  60 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 959 steps  60 reward -1 next state  8 agent position  (5, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1072 steps  61 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  61 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 960 steps  61 reward -1 next state  8 agent position  (5, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1073 steps  62 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  62 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 961 steps  62 reward -1 next state  8 agent position  (5, 6)
 is_terminal [False, True, False]
actions ['S', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1074 steps  63 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  63 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 962 steps  63 reward -1 next state  8 agent position  (6, 6)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1075 steps  64 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  64 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 963 steps  64 reward -1 next state  8 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1076 steps  65 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  65 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 964 steps  65 reward -1 next state  8 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1077 steps  66 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  66 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 965 steps  66 reward -1 next state  8 agent position  (6, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1078 steps  67 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  67 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 966 steps  67 reward 0 next state  8 agent position  (6, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.47764288721360454 agent memory len 1079 steps  68 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.47764288721360454 agent memory len 773 steps  68 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 967 steps  68 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1080 steps  69 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  69 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 968 steps  69 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1081 steps  70 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.47764288721360454 agent memory len 773 steps  70 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 969 steps  70 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1082 steps  71 reward -1 next state  7 agent position  (7, 7)
agent epsilon  0.47764288721360454 agent memory len 773 steps  71 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 970 steps  71 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, True, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['R', 'S', 'L']
agent epsilon  0.47764288721360454 agent memory len 1083 steps  72 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.47764288721360454 agent memory len 773 steps  72 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 971 steps  72 reward 10 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1084 steps  73 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.47764288721360454 agent memory len 773 steps  73 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 971 steps  73 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1085 steps  74 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.47764288721360454 agent memory len 773 steps  74 reward -1 next state  9 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 971 steps  74 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.47764288721360454 agent memory len 1086 steps  75 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.47764288721360454 agent memory len 773 steps  75 reward -1 next state  9 agent position  (8, 8)
agent epsilon  0.47764288721360454 agent memory len 971 steps  75 reward -1 next state  8 agent position  (7, 4)
max steps reached
total rewards -193
epsilon  0.45483567447604933
epsilon  0.45483567447604933
epsilon  0.45483567447604933
Episode number:  17
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1087 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.45483567447604933 agent memory len 774 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.45483567447604933 agent memory len 972 steps  1 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.45483567447604933 agent memory len 1088 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.45483567447604933 agent memory len 775 steps  2 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 973 steps  2 reward -2 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.45483567447604933 agent memory len 1089 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.45483567447604933 agent memory len 776 steps  3 reward -1 next state  1 agent position  (3, 5)
agent epsilon  0.45483567447604933 agent memory len 974 steps  3 reward -2 next state  3 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.45483567447604933 agent memory len 1090 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.45483567447604933 agent memory len 777 steps  4 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.45483567447604933 agent memory len 975 steps  4 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.45483567447604933 agent memory len 1091 steps  5 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.45483567447604933 agent memory len 778 steps  5 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.45483567447604933 agent memory len 976 steps  5 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.45483567447604933 agent memory len 1092 steps  6 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.45483567447604933 agent memory len 779 steps  6 reward -1 next state  3 agent position  (4, 5)
agent epsilon  0.45483567447604933 agent memory len 977 steps  6 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.45483567447604933 agent memory len 1093 steps  7 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.45483567447604933 agent memory len 780 steps  7 reward -1 next state  4 agent position  (5, 5)
agent epsilon  0.45483567447604933 agent memory len 978 steps  7 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1094 steps  8 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.45483567447604933 agent memory len 781 steps  8 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.45483567447604933 agent memory len 979 steps  8 reward -1 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1095 steps  9 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.45483567447604933 agent memory len 782 steps  9 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.45483567447604933 agent memory len 980 steps  9 reward -1 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.45483567447604933 agent memory len 1096 steps  10 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.45483567447604933 agent memory len 783 steps  10 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.45483567447604933 agent memory len 981 steps  10 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'L', 'L']
agent epsilon  0.45483567447604933 agent memory len 1097 steps  11 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.45483567447604933 agent memory len 784 steps  11 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.45483567447604933 agent memory len 982 steps  11 reward -1 next state  6 agent position  (5, 8)
 is_terminal [False, False, False]
random action 1
actions ['U', 'R', 'L']
agent epsilon  0.45483567447604933 agent memory len 1098 steps  12 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.45483567447604933 agent memory len 785 steps  12 reward -1 next state  4 agent position  (6, 6)
agent epsilon  0.45483567447604933 agent memory len 983 steps  12 reward -1 next state  6 agent position  (5, 7)
 is_terminal [False, False, False]
actions ['L', 'D', 'L']
agent epsilon  0.45483567447604933 agent memory len 1099 steps  13 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.45483567447604933 agent memory len 786 steps  13 reward -1 next state  3 agent position  (7, 6)
agent epsilon  0.45483567447604933 agent memory len 984 steps  13 reward -1 next state  7 agent position  (5, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'L']
agent epsilon  0.45483567447604933 agent memory len 1100 steps  14 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.45483567447604933 agent memory len 787 steps  14 reward -1 next state  2 agent position  (7, 6)
agent epsilon  0.45483567447604933 agent memory len 985 steps  14 reward -1 next state  7 agent position  (5, 5)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.45483567447604933 agent memory len 1101 steps  15 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.45483567447604933 agent memory len 788 steps  15 reward -1 next state  3 agent position  (8, 6)
agent epsilon  0.45483567447604933 agent memory len 986 steps  15 reward 0 next state  8 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['R', 'R', 'D']
agent epsilon  0.45483567447604933 agent memory len 1102 steps  16 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.45483567447604933 agent memory len 789 steps  16 reward 0 next state  4 agent position  (8, 7)
agent epsilon  0.45483567447604933 agent memory len 987 steps  16 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['L', 'U', 'L']
agent epsilon  0.45483567447604933 agent memory len 1103 steps  17 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.45483567447604933 agent memory len 790 steps  17 reward 0 next state  3 agent position  (7, 7)
agent epsilon  0.45483567447604933 agent memory len 988 steps  17 reward 10 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1104 steps  18 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.45483567447604933 agent memory len 791 steps  18 reward 0 next state  4 agent position  (7, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  18 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1105 steps  19 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.45483567447604933 agent memory len 792 steps  19 reward 0 next state  5 agent position  (7, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  19 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['S', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1106 steps  20 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.45483567447604933 agent memory len 793 steps  20 reward 0 next state  5 agent position  (8, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  20 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1107 steps  21 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.45483567447604933 agent memory len 794 steps  21 reward 0 next state  6 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  21 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1108 steps  22 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 795 steps  22 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  22 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1109 steps  23 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 796 steps  23 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  23 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1110 steps  24 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 797 steps  24 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  24 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1111 steps  25 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 798 steps  25 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  25 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1112 steps  26 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 799 steps  26 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  26 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1113 steps  27 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 800 steps  27 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  27 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1114 steps  28 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.45483567447604933 agent memory len 801 steps  28 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  28 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1115 steps  29 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.45483567447604933 agent memory len 802 steps  29 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  29 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1116 steps  30 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.45483567447604933 agent memory len 803 steps  30 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  30 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1117 steps  31 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.45483567447604933 agent memory len 804 steps  31 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  31 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1118 steps  32 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.45483567447604933 agent memory len 805 steps  32 reward -2 next state  9 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  32 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.45483567447604933 agent memory len 1119 steps  33 reward 0 next state  7 agent position  (7, 9)
agent epsilon  0.45483567447604933 agent memory len 806 steps  33 reward 0 next state  9 agent position  (9, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  33 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1120 steps  34 reward 0 next state  7 agent position  (7, 8)
agent epsilon  0.45483567447604933 agent memory len 807 steps  34 reward -2 next state  8 agent position  (9, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  34 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1121 steps  35 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.45483567447604933 agent memory len 808 steps  35 reward 0 next state  8 agent position  (9, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  35 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['U', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1122 steps  36 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 809 steps  36 reward 0 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  36 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1123 steps  37 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 810 steps  37 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  37 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1124 steps  38 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 811 steps  38 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  38 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1125 steps  39 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 812 steps  39 reward 0 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  39 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1126 steps  40 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 813 steps  40 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  40 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1127 steps  41 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 814 steps  41 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  41 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1128 steps  42 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 815 steps  42 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  42 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1129 steps  43 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.45483567447604933 agent memory len 816 steps  43 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  43 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['D', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1130 steps  44 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.45483567447604933 agent memory len 817 steps  44 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  44 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['D', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1131 steps  45 reward 0 next state  7 agent position  (7, 7)
agent epsilon  0.45483567447604933 agent memory len 818 steps  45 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  45 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.45483567447604933 agent memory len 1132 steps  46 reward 0 next state  7 agent position  (7, 8)
agent epsilon  0.45483567447604933 agent memory len 819 steps  46 reward 0 next state  8 agent position  (9, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  46 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1133 steps  47 reward 0 next state  7 agent position  (7, 7)
agent epsilon  0.45483567447604933 agent memory len 820 steps  47 reward 0 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  47 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1134 steps  48 reward 0 next state  7 agent position  (7, 8)
agent epsilon  0.45483567447604933 agent memory len 821 steps  48 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  48 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1135 steps  49 reward 0 next state  7 agent position  (7, 8)
agent epsilon  0.45483567447604933 agent memory len 822 steps  49 reward 0 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  49 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['U', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1136 steps  50 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.45483567447604933 agent memory len 823 steps  50 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  50 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.45483567447604933 agent memory len 1137 steps  51 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.45483567447604933 agent memory len 824 steps  51 reward 0 next state  9 agent position  (8, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  51 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['U', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1138 steps  52 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.45483567447604933 agent memory len 825 steps  52 reward -2 next state  9 agent position  (8, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  52 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1139 steps  53 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.45483567447604933 agent memory len 826 steps  53 reward -2 next state  8 agent position  (8, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  53 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1140 steps  54 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 827 steps  54 reward 0 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  54 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1141 steps  55 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 828 steps  55 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  55 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.45483567447604933 agent memory len 1142 steps  56 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 829 steps  56 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  56 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1143 steps  57 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 830 steps  57 reward -2 next state  7 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  57 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.45483567447604933 agent memory len 1144 steps  58 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 831 steps  58 reward -2 next state  8 agent position  (9, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  58 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.45483567447604933 agent memory len 1145 steps  59 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 832 steps  59 reward 0 next state  7 agent position  (8, 9)
agent epsilon  0.45483567447604933 agent memory len 988 steps  59 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
landmark captured 2
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'S']
agent epsilon  0.45483567447604933 agent memory len 1146 steps  60 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  60 reward 10 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  60 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1147 steps  61 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  61 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  61 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1148 steps  62 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  62 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  62 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1149 steps  63 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  63 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  63 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1150 steps  64 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  64 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  64 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1151 steps  65 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  65 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  65 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1152 steps  66 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  66 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  66 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1153 steps  67 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  67 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  67 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1154 steps  68 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  68 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  68 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1155 steps  69 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.45483567447604933 agent memory len 833 steps  69 reward -1 next state  6 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  69 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1156 steps  70 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  70 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  70 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1157 steps  71 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  71 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  71 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1158 steps  72 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  72 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  72 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1159 steps  73 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.45483567447604933 agent memory len 833 steps  73 reward -1 next state  7 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  73 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1160 steps  74 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  74 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  74 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.45483567447604933 agent memory len 1161 steps  75 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.45483567447604933 agent memory len 833 steps  75 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.45483567447604933 agent memory len 988 steps  75 reward -1 next state  8 agent position  (7, 4)
max steps reached
total rewards -202
epsilon  0.4331407826292394
epsilon  0.4331407826292394
epsilon  0.4331407826292394
Episode number:  18
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1162 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.4331407826292394 agent memory len 834 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.4331407826292394 agent memory len 989 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1163 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 835 steps  2 reward -1 next state  0 agent position  (0, 3)
agent epsilon  0.4331407826292394 agent memory len 990 steps  2 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1164 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 836 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.4331407826292394 agent memory len 991 steps  3 reward -1 next state  0 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1165 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 837 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.4331407826292394 agent memory len 992 steps  4 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'U']
agent epsilon  0.4331407826292394 agent memory len 1166 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 838 steps  5 reward -2 next state  0 agent position  (0, 1)
agent epsilon  0.4331407826292394 agent memory len 993 steps  5 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1167 steps  6 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.4331407826292394 agent memory len 839 steps  6 reward -1 next state  1 agent position  (0, 0)
agent epsilon  0.4331407826292394 agent memory len 994 steps  6 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1168 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.4331407826292394 agent memory len 840 steps  7 reward -2 next state  1 agent position  (0, 0)
agent epsilon  0.4331407826292394 agent memory len 995 steps  7 reward -1 next state  0 agent position  (3, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.4331407826292394 agent memory len 1169 steps  8 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 841 steps  8 reward -1 next state  0 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 996 steps  8 reward -1 next state  1 agent position  (2, 8)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.4331407826292394 agent memory len 1170 steps  9 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 842 steps  9 reward -1 next state  0 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 997 steps  9 reward -1 next state  2 agent position  (2, 7)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.4331407826292394 agent memory len 1171 steps  10 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 843 steps  10 reward -1 next state  0 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 998 steps  10 reward -1 next state  3 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.4331407826292394 agent memory len 1172 steps  11 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 844 steps  11 reward -1 next state  0 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 999 steps  11 reward -1 next state  2 agent position  (3, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'D']
agent epsilon  0.4331407826292394 agent memory len 1173 steps  12 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 845 steps  12 reward -1 next state  0 agent position  (1, 0)
agent epsilon  0.4331407826292394 agent memory len 1000 steps  12 reward -1 next state  1 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.4331407826292394 agent memory len 1174 steps  13 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4331407826292394 agent memory len 846 steps  13 reward -1 next state  1 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1001 steps  13 reward -1 next state  2 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.4331407826292394 agent memory len 1175 steps  14 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 847 steps  14 reward -1 next state  1 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1002 steps  14 reward -1 next state  3 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'R']
agent epsilon  0.4331407826292394 agent memory len 1176 steps  15 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 848 steps  15 reward -1 next state  1 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1003 steps  15 reward -1 next state  2 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.4331407826292394 agent memory len 1177 steps  16 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4331407826292394 agent memory len 849 steps  16 reward -1 next state  1 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1004 steps  16 reward 0 next state  2 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1178 steps  17 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4331407826292394 agent memory len 850 steps  17 reward -2 next state  1 agent position  (2, 0)
agent epsilon  0.4331407826292394 agent memory len 1005 steps  17 reward 0 next state  2 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'D']
agent epsilon  0.4331407826292394 agent memory len 1179 steps  18 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4331407826292394 agent memory len 851 steps  18 reward -1 next state  1 agent position  (2, 1)
agent epsilon  0.4331407826292394 agent memory len 1006 steps  18 reward 0 next state  2 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1180 steps  19 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4331407826292394 agent memory len 852 steps  19 reward -1 next state  1 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 1007 steps  19 reward 0 next state  3 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1181 steps  20 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4331407826292394 agent memory len 853 steps  20 reward -1 next state  2 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1008 steps  20 reward 0 next state  3 agent position  (9, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1182 steps  21 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4331407826292394 agent memory len 854 steps  21 reward -2 next state  2 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1009 steps  21 reward -2 next state  3 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1183 steps  22 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.4331407826292394 agent memory len 855 steps  22 reward -2 next state  2 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1010 steps  22 reward 0 next state  3 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1184 steps  23 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.4331407826292394 agent memory len 856 steps  23 reward -1 next state  3 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1011 steps  23 reward 0 next state  4 agent position  (9, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.4331407826292394 agent memory len 1185 steps  24 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.4331407826292394 agent memory len 857 steps  24 reward -2 next state  3 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1012 steps  24 reward 0 next state  4 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'R', 'S']
agent epsilon  0.4331407826292394 agent memory len 1186 steps  25 reward 0 next state  8 agent position  (8, 4)
agent epsilon  0.4331407826292394 agent memory len 858 steps  25 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4331407826292394 agent memory len 1013 steps  25 reward 0 next state  4 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'U', 'U']
agent epsilon  0.4331407826292394 agent memory len 1187 steps  26 reward 0 next state  8 agent position  (8, 5)
agent epsilon  0.4331407826292394 agent memory len 859 steps  26 reward -1 next state  5 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 1014 steps  26 reward 0 next state  3 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'L']
agent epsilon  0.4331407826292394 agent memory len 1188 steps  27 reward 0 next state  8 agent position  (8, 5)
agent epsilon  0.4331407826292394 agent memory len 860 steps  27 reward -1 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1015 steps  27 reward 0 next state  3 agent position  (7, 8)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'L', 'D']
agent epsilon  0.4331407826292394 agent memory len 1189 steps  28 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 861 steps  28 reward -2 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  28 reward 10 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1190 steps  29 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 862 steps  29 reward -2 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  29 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1191 steps  30 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 863 steps  30 reward -1 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  30 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1192 steps  31 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 864 steps  31 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  31 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1193 steps  32 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 865 steps  32 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  32 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1194 steps  33 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 866 steps  33 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  33 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1195 steps  34 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 867 steps  34 reward -1 next state  6 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  34 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'U', 'S']
agent epsilon  0.4331407826292394 agent memory len 1196 steps  35 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 868 steps  35 reward -1 next state  5 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  35 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1197 steps  36 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 869 steps  36 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  36 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1198 steps  37 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 870 steps  37 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  37 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1199 steps  38 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 871 steps  38 reward -2 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  38 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1200 steps  39 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 872 steps  39 reward -1 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  39 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1201 steps  40 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.4331407826292394 agent memory len 873 steps  40 reward -1 next state  7 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  40 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.4331407826292394 agent memory len 1202 steps  41 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 874 steps  41 reward -1 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  41 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1203 steps  42 reward -2 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 875 steps  42 reward -1 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  42 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1204 steps  43 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.4331407826292394 agent memory len 876 steps  43 reward -2 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  43 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1205 steps  44 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 877 steps  44 reward -2 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  44 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1206 steps  45 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.4331407826292394 agent memory len 878 steps  45 reward -2 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  45 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1207 steps  46 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 879 steps  46 reward -2 next state  7 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  46 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.4331407826292394 agent memory len 1208 steps  47 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 880 steps  47 reward -1 next state  7 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  47 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'R', 'S']
agent epsilon  0.4331407826292394 agent memory len 1209 steps  48 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 881 steps  48 reward -1 next state  6 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  48 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1210 steps  49 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 882 steps  49 reward -1 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  49 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1211 steps  50 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 883 steps  50 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  50 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1212 steps  51 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 884 steps  51 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  51 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1213 steps  52 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 885 steps  52 reward -1 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  52 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'R', 'S']
agent epsilon  0.4331407826292394 agent memory len 1214 steps  53 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 886 steps  53 reward -1 next state  5 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  53 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1215 steps  54 reward 0 next state  8 agent position  (8, 5)
agent epsilon  0.4331407826292394 agent memory len 887 steps  54 reward -1 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  54 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1216 steps  55 reward 0 next state  8 agent position  (8, 5)
agent epsilon  0.4331407826292394 agent memory len 888 steps  55 reward -2 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  55 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1217 steps  56 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 889 steps  56 reward -1 next state  5 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  56 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1218 steps  57 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 890 steps  57 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  57 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1219 steps  58 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 891 steps  58 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  58 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1220 steps  59 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 892 steps  59 reward -1 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  59 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1221 steps  60 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.4331407826292394 agent memory len 893 steps  60 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  60 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1222 steps  61 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 894 steps  61 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  61 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'S', 'S']
agent epsilon  0.4331407826292394 agent memory len 1223 steps  62 reward -2 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 895 steps  62 reward -1 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  62 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1224 steps  63 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 896 steps  63 reward -2 next state  7 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  63 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1225 steps  64 reward -1 next state  9 agent position  (9, 7)
agent epsilon  0.4331407826292394 agent memory len 897 steps  64 reward -2 next state  7 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  64 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['L', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1226 steps  65 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 898 steps  65 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  65 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1227 steps  66 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 899 steps  66 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  66 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1228 steps  67 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 900 steps  67 reward -2 next state  6 agent position  (3, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  67 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.4331407826292394 agent memory len 1229 steps  68 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 901 steps  68 reward -1 next state  6 agent position  (3, 1)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  68 reward -1 next state  3 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1230 steps  69 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 902 steps  69 reward -1 next state  6 agent position  (4, 1)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  69 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1231 steps  70 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.4331407826292394 agent memory len 903 steps  70 reward -1 next state  6 agent position  (4, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  70 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.4331407826292394 agent memory len 1232 steps  71 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 904 steps  71 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  71 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1233 steps  72 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 905 steps  72 reward -2 next state  5 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  72 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1234 steps  73 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 906 steps  73 reward -2 next state  5 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  73 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1235 steps  74 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 907 steps  74 reward -2 next state  5 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  74 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.4331407826292394 agent memory len 1236 steps  75 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.4331407826292394 agent memory len 908 steps  75 reward -2 next state  5 agent position  (5, 0)
agent epsilon  0.4331407826292394 agent memory len 1016 steps  75 reward -1 next state  5 agent position  (8, 8)
max steps reached
total rewards -235
epsilon  0.4125039631431931
epsilon  0.4125039631431931
epsilon  0.4125039631431931
Episode number:  19
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1237 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.4125039631431931 agent memory len 909 steps  1 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.4125039631431931 agent memory len 1017 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'L']
agent epsilon  0.4125039631431931 agent memory len 1238 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.4125039631431931 agent memory len 910 steps  2 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.4125039631431931 agent memory len 1018 steps  2 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.4125039631431931 agent memory len 1239 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.4125039631431931 agent memory len 911 steps  3 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.4125039631431931 agent memory len 1019 steps  3 reward -1 next state  0 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.4125039631431931 agent memory len 1240 steps  4 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.4125039631431931 agent memory len 912 steps  4 reward -2 next state  0 agent position  (0, 4)
agent epsilon  0.4125039631431931 agent memory len 1020 steps  4 reward -1 next state  0 agent position  (3, 8)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1241 steps  5 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.4125039631431931 agent memory len 913 steps  5 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.4125039631431931 agent memory len 1021 steps  5 reward -1 next state  1 agent position  (4, 8)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1242 steps  6 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.4125039631431931 agent memory len 914 steps  6 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.4125039631431931 agent memory len 1022 steps  6 reward -1 next state  2 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1243 steps  7 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.4125039631431931 agent memory len 915 steps  7 reward -1 next state  0 agent position  (3, 4)
agent epsilon  0.4125039631431931 agent memory len 1023 steps  7 reward -1 next state  3 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.4125039631431931 agent memory len 1244 steps  8 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.4125039631431931 agent memory len 916 steps  8 reward -1 next state  0 agent position  (4, 4)
agent epsilon  0.4125039631431931 agent memory len 1024 steps  8 reward -1 next state  4 agent position  (5, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1245 steps  9 reward -2 next state  6 agent position  (6, 0)
agent epsilon  0.4125039631431931 agent memory len 917 steps  9 reward -1 next state  0 agent position  (4, 4)
agent epsilon  0.4125039631431931 agent memory len 1025 steps  9 reward -1 next state  4 agent position  (5, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'U']
agent epsilon  0.4125039631431931 agent memory len 1246 steps  10 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4125039631431931 agent memory len 918 steps  10 reward -1 next state  1 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 1026 steps  10 reward -1 next state  5 agent position  (4, 7)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.4125039631431931 agent memory len 1247 steps  11 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 919 steps  11 reward 0 next state  2 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 1027 steps  11 reward -1 next state  6 agent position  (5, 7)
 is_terminal [False, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'D', 'L']
agent epsilon  0.4125039631431931 agent memory len 1248 steps  12 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  12 reward 10 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1028 steps  12 reward -1 next state  7 agent position  (5, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1249 steps  13 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  13 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1029 steps  13 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1250 steps  14 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  14 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1030 steps  14 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1251 steps  15 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  15 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1031 steps  15 reward -1 next state  7 agent position  (6, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1252 steps  16 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  16 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1032 steps  16 reward -1 next state  7 agent position  (6, 4)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1253 steps  17 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  17 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1033 steps  17 reward -1 next state  7 agent position  (6, 3)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1254 steps  18 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  18 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1034 steps  18 reward -1 next state  7 agent position  (6, 2)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1255 steps  19 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  19 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1035 steps  19 reward -1 next state  7 agent position  (6, 1)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1256 steps  20 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  20 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1036 steps  20 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1257 steps  21 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  21 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1037 steps  21 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1258 steps  22 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  22 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1038 steps  22 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1259 steps  23 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  23 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1039 steps  23 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1260 steps  24 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  24 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1040 steps  24 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1261 steps  25 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  25 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1041 steps  25 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1262 steps  26 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  26 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1042 steps  26 reward -1 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1263 steps  27 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  27 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1043 steps  27 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1264 steps  28 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  28 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1044 steps  28 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1265 steps  29 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  29 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1045 steps  29 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1266 steps  30 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  30 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1046 steps  30 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1267 steps  31 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  31 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1047 steps  31 reward -1 next state  7 agent position  (6, 1)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1268 steps  32 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  32 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1048 steps  32 reward -1 next state  7 agent position  (7, 1)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1269 steps  33 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  33 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1049 steps  33 reward -1 next state  7 agent position  (6, 1)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1270 steps  34 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  34 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1050 steps  34 reward -1 next state  7 agent position  (6, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1271 steps  35 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  35 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1051 steps  35 reward -1 next state  7 agent position  (6, 1)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1272 steps  36 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  36 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1052 steps  36 reward -1 next state  7 agent position  (6, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1273 steps  37 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  37 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1053 steps  37 reward -1 next state  7 agent position  (5, 2)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1274 steps  38 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  38 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1054 steps  38 reward -1 next state  7 agent position  (4, 2)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1275 steps  39 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  39 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1055 steps  39 reward -1 next state  7 agent position  (5, 2)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1276 steps  40 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  40 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1056 steps  40 reward -1 next state  7 agent position  (5, 1)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1277 steps  41 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  41 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1057 steps  41 reward -1 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1278 steps  42 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  42 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1058 steps  42 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1279 steps  43 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  43 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1059 steps  43 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1280 steps  44 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  44 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1060 steps  44 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1281 steps  45 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  45 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1061 steps  45 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1282 steps  46 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  46 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1062 steps  46 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1283 steps  47 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  47 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1063 steps  47 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1284 steps  48 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  48 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1064 steps  48 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1285 steps  49 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  49 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1065 steps  49 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1286 steps  50 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.4125039631431931 agent memory len 920 steps  50 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1066 steps  50 reward -1 next state  7 agent position  (5, 1)
 is_terminal [False, True, False]
actions ['U', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1287 steps  51 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.4125039631431931 agent memory len 920 steps  51 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1067 steps  51 reward -1 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1288 steps  52 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.4125039631431931 agent memory len 920 steps  52 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1068 steps  52 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1289 steps  53 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  53 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1069 steps  53 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1290 steps  54 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  54 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1070 steps  54 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1291 steps  55 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  55 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1071 steps  55 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1292 steps  56 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  56 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1072 steps  56 reward -1 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1293 steps  57 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  57 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1073 steps  57 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1294 steps  58 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  58 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1074 steps  58 reward -2 next state  7 agent position  (5, 0)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1295 steps  59 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  59 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1075 steps  59 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1296 steps  60 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  60 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1076 steps  60 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1297 steps  61 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  61 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1077 steps  61 reward -1 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1298 steps  62 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  62 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1078 steps  62 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1299 steps  63 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  63 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1079 steps  63 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1300 steps  64 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  64 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1080 steps  64 reward -2 next state  7 agent position  (6, 0)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.4125039631431931 agent memory len 1301 steps  65 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.4125039631431931 agent memory len 920 steps  65 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1081 steps  65 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1302 steps  66 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  66 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1082 steps  66 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1303 steps  67 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  67 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1083 steps  67 reward -2 next state  7 agent position  (7, 0)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1304 steps  68 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  68 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1084 steps  68 reward -1 next state  7 agent position  (7, 1)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1305 steps  69 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  69 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1085 steps  69 reward -1 next state  7 agent position  (7, 0)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1306 steps  70 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.4125039631431931 agent memory len 920 steps  70 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1086 steps  70 reward -1 next state  7 agent position  (7, 1)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1307 steps  71 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  71 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1087 steps  71 reward -1 next state  7 agent position  (7, 1)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.4125039631431931 agent memory len 1308 steps  72 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  72 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1088 steps  72 reward -1 next state  7 agent position  (7, 2)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.4125039631431931 agent memory len 1309 steps  73 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  73 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1089 steps  73 reward -1 next state  7 agent position  (6, 2)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.4125039631431931 agent memory len 1310 steps  74 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.4125039631431931 agent memory len 920 steps  74 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1090 steps  74 reward -1 next state  7 agent position  (6, 2)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.4125039631431931 agent memory len 1311 steps  75 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.4125039631431931 agent memory len 920 steps  75 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.4125039631431931 agent memory len 1091 steps  75 reward -1 next state  7 agent position  (6, 1)
max steps reached
total rewards -236
epsilon  0.3928736132199562
epsilon  0.3928736132199562
epsilon  0.3928736132199562
Episode number:  20
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'D']
agent epsilon  0.3928736132199562 agent memory len 1312 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3928736132199562 agent memory len 921 steps  1 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.3928736132199562 agent memory len 1092 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'L']
agent epsilon  0.3928736132199562 agent memory len 1313 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3928736132199562 agent memory len 922 steps  2 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.3928736132199562 agent memory len 1093 steps  2 reward -1 next state  0 agent position  (1, 8)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.3928736132199562 agent memory len 1314 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.3928736132199562 agent memory len 923 steps  3 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.3928736132199562 agent memory len 1094 steps  3 reward -1 next state  1 agent position  (2, 8)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.3928736132199562 agent memory len 1315 steps  4 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.3928736132199562 agent memory len 924 steps  4 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.3928736132199562 agent memory len 1095 steps  4 reward -1 next state  2 agent position  (3, 8)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.3928736132199562 agent memory len 1316 steps  5 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3928736132199562 agent memory len 925 steps  5 reward -1 next state  1 agent position  (3, 5)
agent epsilon  0.3928736132199562 agent memory len 1096 steps  5 reward -1 next state  3 agent position  (3, 8)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.3928736132199562 agent memory len 1317 steps  6 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3928736132199562 agent memory len 926 steps  6 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.3928736132199562 agent memory len 1097 steps  6 reward -1 next state  4 agent position  (2, 8)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'D']
agent epsilon  0.3928736132199562 agent memory len 1318 steps  7 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3928736132199562 agent memory len 927 steps  7 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.3928736132199562 agent memory len 1098 steps  7 reward -1 next state  5 agent position  (3, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.3928736132199562 agent memory len 1319 steps  8 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.3928736132199562 agent memory len 928 steps  8 reward -1 next state  1 agent position  (5, 4)
agent epsilon  0.3928736132199562 agent memory len 1099 steps  8 reward -1 next state  5 agent position  (2, 8)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.3928736132199562 agent memory len 1320 steps  9 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3928736132199562 agent memory len 929 steps  9 reward 0 next state  1 agent position  (6, 4)
agent epsilon  0.3928736132199562 agent memory len 1100 steps  9 reward -1 next state  6 agent position  (3, 8)
 is_terminal [False, False, False]
random action 0
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['U', 'D', 'R']
agent epsilon  0.3928736132199562 agent memory len 1321 steps  10 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3928736132199562 agent memory len 930 steps  10 reward 10 next state  1 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1101 steps  10 reward -1 next state  7 agent position  (3, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1322 steps  11 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3928736132199562 agent memory len 930 steps  11 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1102 steps  11 reward -1 next state  7 agent position  (4, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1323 steps  12 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.3928736132199562 agent memory len 930 steps  12 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1103 steps  12 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1324 steps  13 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.3928736132199562 agent memory len 930 steps  13 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1104 steps  13 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1325 steps  14 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  14 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1105 steps  14 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1326 steps  15 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  15 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1106 steps  15 reward -1 next state  7 agent position  (5, 8)
 is_terminal [False, True, False]
actions ['D', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1327 steps  16 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  16 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1107 steps  16 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1328 steps  17 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  17 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1108 steps  17 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, True, False]
landmark captured 0
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  18 reward 10 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  18 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1109 steps  18 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  19 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  19 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1110 steps  19 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  20 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  20 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1111 steps  20 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  21 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  21 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1112 steps  21 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  22 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  22 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1113 steps  22 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  23 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  23 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1114 steps  23 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  24 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  24 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1115 steps  24 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  25 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  25 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1116 steps  25 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  26 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  26 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1117 steps  26 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  27 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  27 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1118 steps  27 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  28 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  28 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1119 steps  28 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  29 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  29 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1120 steps  29 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  30 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  30 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1121 steps  30 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  31 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  31 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1122 steps  31 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  32 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  32 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1123 steps  32 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  33 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  33 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1124 steps  33 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  34 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  34 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1125 steps  34 reward -1 next state  7 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  35 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  35 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1126 steps  35 reward -1 next state  7 agent position  (5, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  36 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  36 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1127 steps  36 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  37 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  37 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1128 steps  37 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  38 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  38 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1129 steps  38 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  39 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  39 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1130 steps  39 reward -1 next state  7 agent position  (6, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  40 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  40 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1131 steps  40 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  41 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  41 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1132 steps  41 reward -1 next state  7 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  42 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  42 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1133 steps  42 reward -1 next state  7 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  43 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  43 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1134 steps  43 reward -1 next state  7 agent position  (5, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  44 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  44 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1135 steps  44 reward -1 next state  7 agent position  (5, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  45 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  45 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1136 steps  45 reward -1 next state  7 agent position  (5, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  46 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  46 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1137 steps  46 reward -1 next state  7 agent position  (5, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  47 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  47 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1138 steps  47 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  48 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  48 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1139 steps  48 reward -1 next state  7 agent position  (3, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  49 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  49 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1140 steps  49 reward -1 next state  7 agent position  (3, 3)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  50 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  50 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1141 steps  50 reward -1 next state  7 agent position  (4, 3)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  51 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  51 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1142 steps  51 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  52 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  52 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1143 steps  52 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  53 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  53 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1144 steps  53 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  54 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  54 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1145 steps  54 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  55 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  55 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1146 steps  55 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  56 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  56 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1147 steps  56 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  57 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  57 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1148 steps  57 reward -1 next state  7 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  58 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  58 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1149 steps  58 reward -1 next state  7 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  59 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  59 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1150 steps  59 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  60 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  60 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1151 steps  60 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  61 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  61 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1152 steps  61 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  62 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  62 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1153 steps  62 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  63 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  63 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1154 steps  63 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  64 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  64 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1155 steps  64 reward -1 next state  7 agent position  (3, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  65 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  65 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1156 steps  65 reward -1 next state  7 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  66 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  66 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1157 steps  66 reward -1 next state  7 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  67 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  67 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1158 steps  67 reward -1 next state  7 agent position  (3, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  68 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  68 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1159 steps  68 reward -1 next state  7 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  69 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  69 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1160 steps  69 reward -1 next state  7 agent position  (3, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  70 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  70 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1161 steps  70 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  71 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  71 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1162 steps  71 reward -1 next state  7 agent position  (4, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  72 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  72 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1163 steps  72 reward -1 next state  7 agent position  (4, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  73 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  73 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1164 steps  73 reward -1 next state  7 agent position  (4, 4)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  74 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  74 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1165 steps  74 reward -1 next state  7 agent position  (5, 4)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.3928736132199562 agent memory len 1329 steps  75 reward -1 next state  9 agent position  (9, 2)
agent epsilon  0.3928736132199562 agent memory len 930 steps  75 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.3928736132199562 agent memory len 1166 steps  75 reward -1 next state  7 agent position  (5, 5)
max steps reached
total rewards -200
epsilon  0.3742006467597279
epsilon  0.3742006467597279
epsilon  0.3742006467597279
Episode number:  21
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.3742006467597279 agent memory len 1330 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3742006467597279 agent memory len 931 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.3742006467597279 agent memory len 1167 steps  1 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'S', 'D']
agent epsilon  0.3742006467597279 agent memory len 1331 steps  2 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.3742006467597279 agent memory len 932 steps  2 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.3742006467597279 agent memory len 1168 steps  2 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'U']
agent epsilon  0.3742006467597279 agent memory len 1332 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3742006467597279 agent memory len 933 steps  3 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.3742006467597279 agent memory len 1169 steps  3 reward -1 next state  2 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1333 steps  4 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3742006467597279 agent memory len 934 steps  4 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.3742006467597279 agent memory len 1170 steps  4 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1334 steps  5 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.3742006467597279 agent memory len 935 steps  5 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1171 steps  5 reward -1 next state  4 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1335 steps  6 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3742006467597279 agent memory len 936 steps  6 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.3742006467597279 agent memory len 1172 steps  6 reward -1 next state  5 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'D']
agent epsilon  0.3742006467597279 agent memory len 1336 steps  7 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3742006467597279 agent memory len 937 steps  7 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.3742006467597279 agent memory len 1173 steps  7 reward -1 next state  4 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'U', 'D']
agent epsilon  0.3742006467597279 agent memory len 1337 steps  8 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3742006467597279 agent memory len 938 steps  8 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.3742006467597279 agent memory len 1174 steps  8 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.3742006467597279 agent memory len 1338 steps  9 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.3742006467597279 agent memory len 939 steps  9 reward -1 next state  1 agent position  (3, 6)
agent epsilon  0.3742006467597279 agent memory len 1175 steps  9 reward -1 next state  3 agent position  (6, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.3742006467597279 agent memory len 1339 steps  10 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3742006467597279 agent memory len 940 steps  10 reward -1 next state  1 agent position  (4, 6)
agent epsilon  0.3742006467597279 agent memory len 1176 steps  10 reward 0 next state  4 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.3742006467597279 agent memory len 1340 steps  11 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.3742006467597279 agent memory len 941 steps  11 reward -1 next state  2 agent position  (5, 6)
agent epsilon  0.3742006467597279 agent memory len 1177 steps  11 reward -2 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'L', 'D']
agent epsilon  0.3742006467597279 agent memory len 1341 steps  12 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.3742006467597279 agent memory len 942 steps  12 reward -1 next state  2 agent position  (5, 5)
agent epsilon  0.3742006467597279 agent memory len 1178 steps  12 reward 0 next state  5 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.3742006467597279 agent memory len 1342 steps  13 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.3742006467597279 agent memory len 943 steps  13 reward 0 next state  2 agent position  (6, 5)
agent epsilon  0.3742006467597279 agent memory len 1179 steps  13 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.3742006467597279 agent memory len 1343 steps  14 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3742006467597279 agent memory len 944 steps  14 reward 0 next state  3 agent position  (6, 5)
agent epsilon  0.3742006467597279 agent memory len 1180 steps  14 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.3742006467597279 agent memory len 1344 steps  15 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.3742006467597279 agent memory len 945 steps  15 reward -1 next state  3 agent position  (5, 5)
agent epsilon  0.3742006467597279 agent memory len 1181 steps  15 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'S']
agent epsilon  0.3742006467597279 agent memory len 1345 steps  16 reward 0 next state  8 agent position  (8, 4)
agent epsilon  0.3742006467597279 agent memory len 946 steps  16 reward 0 next state  4 agent position  (6, 5)
agent epsilon  0.3742006467597279 agent memory len 1182 steps  16 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.3742006467597279 agent memory len 1346 steps  17 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.3742006467597279 agent memory len 947 steps  17 reward 0 next state  4 agent position  (6, 4)
agent epsilon  0.3742006467597279 agent memory len 1183 steps  17 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['R', 'R', 'L']
agent epsilon  0.3742006467597279 agent memory len 1347 steps  18 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.3742006467597279 agent memory len 948 steps  18 reward 0 next state  5 agent position  (6, 5)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  18 reward 10 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1348 steps  19 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.3742006467597279 agent memory len 949 steps  19 reward -1 next state  6 agent position  (6, 6)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  19 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['U', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1349 steps  20 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.3742006467597279 agent memory len 950 steps  20 reward 0 next state  6 agent position  (6, 5)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  20 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['U', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1350 steps  21 reward -1 next state  7 agent position  (7, 6)
agent epsilon  0.3742006467597279 agent memory len 951 steps  21 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  21 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1351 steps  22 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.3742006467597279 agent memory len 952 steps  22 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  22 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'D', 'S']
agent epsilon  0.3742006467597279 agent memory len 1352 steps  23 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.3742006467597279 agent memory len 953 steps  23 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  23 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1353 steps  24 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.3742006467597279 agent memory len 954 steps  24 reward -1 next state  8 agent position  (7, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  24 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['U', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1354 steps  25 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 955 steps  25 reward 0 next state  8 agent position  (7, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  25 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1355 steps  26 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.3742006467597279 agent memory len 956 steps  26 reward -1 next state  8 agent position  (7, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  26 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['U', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1356 steps  27 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 957 steps  27 reward -1 next state  8 agent position  (7, 1)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  27 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1357 steps  28 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 958 steps  28 reward -1 next state  8 agent position  (7, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  28 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['U', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1358 steps  29 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 959 steps  29 reward -1 next state  8 agent position  (7, 1)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  29 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['U', 'D', 'S']
agent epsilon  0.3742006467597279 agent memory len 1359 steps  30 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.3742006467597279 agent memory len 960 steps  30 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  30 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['U', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1360 steps  31 reward -1 next state  4 agent position  (4, 8)
agent epsilon  0.3742006467597279 agent memory len 961 steps  31 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  31 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['L', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1361 steps  32 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.3742006467597279 agent memory len 962 steps  32 reward 0 next state  7 agent position  (8, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  32 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1362 steps  33 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.3742006467597279 agent memory len 963 steps  33 reward 0 next state  7 agent position  (8, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  33 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1363 steps  34 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.3742006467597279 agent memory len 964 steps  34 reward 0 next state  7 agent position  (8, 1)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  34 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['U', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1364 steps  35 reward -1 next state  5 agent position  (5, 7)
agent epsilon  0.3742006467597279 agent memory len 965 steps  35 reward 0 next state  7 agent position  (8, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  35 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1365 steps  36 reward -1 next state  6 agent position  (6, 7)
agent epsilon  0.3742006467597279 agent memory len 966 steps  36 reward 0 next state  7 agent position  (8, 1)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  36 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1366 steps  37 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 967 steps  37 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  37 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1367 steps  38 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 968 steps  38 reward 0 next state  9 agent position  (8, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  38 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['U', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1368 steps  39 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 969 steps  39 reward 0 next state  9 agent position  (8, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  39 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'L', 'S']
agent epsilon  0.3742006467597279 agent memory len 1369 steps  40 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 970 steps  40 reward 0 next state  9 agent position  (8, 3)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  40 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['U', 'R', 'S']
agent epsilon  0.3742006467597279 agent memory len 1370 steps  41 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 971 steps  41 reward 0 next state  9 agent position  (8, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  41 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['D', 'U', 'S']
agent epsilon  0.3742006467597279 agent memory len 1371 steps  42 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  42 reward 10 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1372 steps  43 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  43 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1373 steps  44 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  44 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1374 steps  45 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  45 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1375 steps  46 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  46 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1376 steps  47 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  47 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  47 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1377 steps  48 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  48 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  48 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1378 steps  49 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  49 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  49 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1379 steps  50 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  50 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  50 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1380 steps  51 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  51 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1381 steps  52 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  52 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  52 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1382 steps  53 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  53 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  53 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1383 steps  54 reward -1 next state  7 agent position  (7, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  54 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  54 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1384 steps  55 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  55 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  55 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1385 steps  56 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  56 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  56 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1386 steps  57 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  57 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  57 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1387 steps  58 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  58 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1388 steps  59 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  59 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1389 steps  60 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  60 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1390 steps  61 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  61 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1391 steps  62 reward -2 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  62 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1392 steps  63 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  63 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1393 steps  64 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  64 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1394 steps  65 reward -1 next state  5 agent position  (5, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  65 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1395 steps  66 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  66 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1396 steps  67 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  67 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1397 steps  68 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  68 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1398 steps  69 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  69 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1399 steps  70 reward -1 next state  5 agent position  (5, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  70 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1400 steps  71 reward -1 next state  6 agent position  (6, 9)
agent epsilon  0.3742006467597279 agent memory len 972 steps  71 reward -1 next state  9 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1401 steps  72 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  72 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1402 steps  73 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  73 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1403 steps  74 reward -1 next state  6 agent position  (6, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  74 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.3742006467597279 agent memory len 1404 steps  75 reward -1 next state  7 agent position  (7, 8)
agent epsilon  0.3742006467597279 agent memory len 972 steps  75 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.3742006467597279 agent memory len 1184 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -174
epsilon  0.35643837162004377
epsilon  0.35643837162004377
epsilon  0.35643837162004377
Episode number:  22
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1405 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 973 steps  1 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.35643837162004377 agent memory len 1185 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1406 steps  2 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.35643837162004377 agent memory len 974 steps  2 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.35643837162004377 agent memory len 1186 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'D', 'L']
agent epsilon  0.35643837162004377 agent memory len 1407 steps  3 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.35643837162004377 agent memory len 975 steps  3 reward -1 next state  2 agent position  (3, 5)
agent epsilon  0.35643837162004377 agent memory len 1187 steps  3 reward -1 next state  3 agent position  (2, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1408 steps  4 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 976 steps  4 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.35643837162004377 agent memory len 1188 steps  4 reward -1 next state  4 agent position  (3, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1409 steps  5 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 977 steps  5 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.35643837162004377 agent memory len 1189 steps  5 reward -1 next state  5 agent position  (4, 8)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1410 steps  6 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 978 steps  6 reward 0 next state  1 agent position  (6, 5)
agent epsilon  0.35643837162004377 agent memory len 1190 steps  6 reward -1 next state  6 agent position  (5, 8)
 is_terminal [False, False, False]
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.35643837162004377 agent memory len 1411 steps  7 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 979 steps  7 reward 0 next state  0 agent position  (7, 5)
agent epsilon  0.35643837162004377 agent memory len 1191 steps  7 reward -1 next state  7 agent position  (5, 7)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1412 steps  8 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 980 steps  8 reward 0 next state  1 agent position  (8, 5)
agent epsilon  0.35643837162004377 agent memory len 1192 steps  8 reward -1 next state  8 agent position  (6, 7)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1413 steps  9 reward -2 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 981 steps  9 reward -1 next state  1 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1193 steps  9 reward 0 next state  9 agent position  (7, 7)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'S', 'D']
agent epsilon  0.35643837162004377 agent memory len 1414 steps  10 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.35643837162004377 agent memory len 982 steps  10 reward -1 next state  2 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1194 steps  10 reward 0 next state  9 agent position  (8, 7)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1415 steps  11 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.35643837162004377 agent memory len 983 steps  11 reward -2 next state  2 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1195 steps  11 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
actions ['L', 'L', 'R']
agent epsilon  0.35643837162004377 agent memory len 1416 steps  12 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 984 steps  12 reward -1 next state  1 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1196 steps  12 reward 0 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1417 steps  13 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 985 steps  13 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1197 steps  13 reward 0 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1418 steps  14 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 986 steps  14 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1198 steps  14 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
actions ['L', 'U', 'D']
agent epsilon  0.35643837162004377 agent memory len 1419 steps  15 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 987 steps  15 reward 0 next state  0 agent position  (8, 4)
agent epsilon  0.35643837162004377 agent memory len 1199 steps  15 reward -2 next state  8 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'R', 'D']
agent epsilon  0.35643837162004377 agent memory len 1420 steps  16 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 988 steps  16 reward 0 next state  1 agent position  (8, 5)
agent epsilon  0.35643837162004377 agent memory len 1200 steps  16 reward -2 next state  8 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'L', 'R']
agent epsilon  0.35643837162004377 agent memory len 1421 steps  17 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 989 steps  17 reward 0 next state  0 agent position  (8, 4)
agent epsilon  0.35643837162004377 agent memory len 1201 steps  17 reward -2 next state  8 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1422 steps  18 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 990 steps  18 reward -1 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1202 steps  18 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1423 steps  19 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 991 steps  19 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1203 steps  19 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1424 steps  20 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 992 steps  20 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1204 steps  20 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1425 steps  21 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 993 steps  21 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1205 steps  21 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1426 steps  22 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 994 steps  22 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1206 steps  22 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1427 steps  23 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 995 steps  23 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1207 steps  23 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.35643837162004377 agent memory len 1428 steps  24 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 996 steps  24 reward -2 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1208 steps  24 reward 0 next state  9 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'R', 'S']
agent epsilon  0.35643837162004377 agent memory len 1429 steps  25 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 997 steps  25 reward -1 next state  0 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1209 steps  25 reward 0 next state  9 agent position  (8, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'D']
agent epsilon  0.35643837162004377 agent memory len 1430 steps  26 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 998 steps  26 reward -1 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1210 steps  26 reward 0 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1431 steps  27 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 999 steps  27 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1211 steps  27 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1432 steps  28 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1000 steps  28 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1212 steps  28 reward -2 next state  9 agent position  (9, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.35643837162004377 agent memory len 1433 steps  29 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1001 steps  29 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1213 steps  29 reward 0 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1434 steps  30 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1002 steps  30 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1214 steps  30 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1435 steps  31 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1003 steps  31 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1215 steps  31 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1436 steps  32 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1004 steps  32 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1216 steps  32 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1437 steps  33 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1005 steps  33 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1217 steps  33 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1438 steps  34 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1006 steps  34 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1218 steps  34 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 1
actions ['L', 'L', 'D']
agent epsilon  0.35643837162004377 agent memory len 1439 steps  35 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1007 steps  35 reward -1 next state  0 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1219 steps  35 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1440 steps  36 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1008 steps  36 reward -2 next state  0 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1220 steps  36 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1441 steps  37 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 1009 steps  37 reward -2 next state  1 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1221 steps  37 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.35643837162004377 agent memory len 1442 steps  38 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1010 steps  38 reward -2 next state  0 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1222 steps  38 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'R', 'U']
agent epsilon  0.35643837162004377 agent memory len 1443 steps  39 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1011 steps  39 reward -1 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1223 steps  39 reward 0 next state  9 agent position  (8, 7)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1444 steps  40 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1012 steps  40 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1224 steps  40 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1445 steps  41 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1013 steps  41 reward -2 next state  0 agent position  (9, 6)
agent epsilon  0.35643837162004377 agent memory len 1225 steps  41 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'L', 'S']
agent epsilon  0.35643837162004377 agent memory len 1446 steps  42 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1014 steps  42 reward -1 next state  0 agent position  (9, 5)
agent epsilon  0.35643837162004377 agent memory len 1226 steps  42 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.35643837162004377 agent memory len 1447 steps  43 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1015 steps  43 reward -1 next state  0 agent position  (9, 4)
agent epsilon  0.35643837162004377 agent memory len 1227 steps  43 reward -2 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'D']
agent epsilon  0.35643837162004377 agent memory len 1448 steps  44 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 1016 steps  44 reward 0 next state  1 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1228 steps  44 reward -2 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1449 steps  45 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 1017 steps  45 reward -2 next state  1 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1229 steps  45 reward 0 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1450 steps  46 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1018 steps  46 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1230 steps  46 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1451 steps  47 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1019 steps  47 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1231 steps  47 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1452 steps  48 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1020 steps  48 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1232 steps  48 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1453 steps  49 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1021 steps  49 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1233 steps  49 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1454 steps  50 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1022 steps  50 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1234 steps  50 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'D', 'L']
agent epsilon  0.35643837162004377 agent memory len 1455 steps  51 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1023 steps  51 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1235 steps  51 reward 0 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1456 steps  52 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1024 steps  52 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1236 steps  52 reward -2 next state  9 agent position  (9, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['D', 'D', 'R']
agent epsilon  0.35643837162004377 agent memory len 1457 steps  53 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1025 steps  53 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1237 steps  53 reward 0 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1458 steps  54 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1026 steps  54 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1238 steps  54 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
actions ['L', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1459 steps  55 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1027 steps  55 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1239 steps  55 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.35643837162004377 agent memory len 1460 steps  56 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1028 steps  56 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1240 steps  56 reward -2 next state  9 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['U', 'U', 'U']
agent epsilon  0.35643837162004377 agent memory len 1461 steps  57 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1029 steps  57 reward 0 next state  0 agent position  (8, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  57 reward 10 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'U', 'S']
agent epsilon  0.35643837162004377 agent memory len 1462 steps  58 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1030 steps  58 reward 0 next state  0 agent position  (7, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['D', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1463 steps  59 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.35643837162004377 agent memory len 1031 steps  59 reward 0 next state  0 agent position  (8, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  59 reward -1 next state  8 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1464 steps  60 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.35643837162004377 agent memory len 1032 steps  60 reward 0 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  60 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['L', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1465 steps  61 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.35643837162004377 agent memory len 1033 steps  61 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  61 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['L', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1466 steps  62 reward -2 next state  2 agent position  (2, 0)
agent epsilon  0.35643837162004377 agent memory len 1034 steps  62 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  62 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1467 steps  63 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1035 steps  63 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  63 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['L', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1468 steps  64 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1036 steps  64 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  64 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'D', 'S']
agent epsilon  0.35643837162004377 agent memory len 1469 steps  65 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1037 steps  65 reward -2 next state  0 agent position  (9, 3)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  65 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['L', 'L', 'S']
agent epsilon  0.35643837162004377 agent memory len 1470 steps  66 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  66 reward 10 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  66 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1471 steps  67 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  67 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  67 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1472 steps  68 reward -1 next state  0 agent position  (0, 2)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  68 reward -1 next state  2 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  68 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1473 steps  69 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  69 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  69 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1474 steps  70 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  70 reward -1 next state  1 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  70 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1475 steps  71 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  71 reward -1 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  71 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1476 steps  72 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  72 reward -1 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  72 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1477 steps  73 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  73 reward -1 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  73 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1478 steps  74 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  74 reward -1 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  74 reward -1 next state  9 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.35643837162004377 agent memory len 1479 steps  75 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.35643837162004377 agent memory len 1038 steps  75 reward -1 next state  0 agent position  (9, 2)
agent epsilon  0.35643837162004377 agent memory len 1241 steps  75 reward -1 next state  9 agent position  (8, 8)
max steps reached
total rewards -282
epsilon  0.3395423728610988
epsilon  0.3395423728610988
epsilon  0.3395423728610988
Episode number:  23
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.3395423728610988 agent memory len 1480 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.3395423728610988 agent memory len 1039 steps  1 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.3395423728610988 agent memory len 1242 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1481 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.3395423728610988 agent memory len 1040 steps  2 reward -1 next state  0 agent position  (1, 6)
agent epsilon  0.3395423728610988 agent memory len 1243 steps  2 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.3395423728610988 agent memory len 1482 steps  3 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.3395423728610988 agent memory len 1041 steps  3 reward -1 next state  0 agent position  (1, 7)
agent epsilon  0.3395423728610988 agent memory len 1244 steps  3 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.3395423728610988 agent memory len 1483 steps  4 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1042 steps  4 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.3395423728610988 agent memory len 1245 steps  4 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.3395423728610988 agent memory len 1484 steps  5 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1043 steps  5 reward -1 next state  0 agent position  (3, 7)
agent epsilon  0.3395423728610988 agent memory len 1246 steps  5 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['S', 'U', 'U']
agent epsilon  0.3395423728610988 agent memory len 1485 steps  6 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1044 steps  6 reward -1 next state  0 agent position  (2, 7)
agent epsilon  0.3395423728610988 agent memory len 1247 steps  6 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.3395423728610988 agent memory len 1486 steps  7 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1045 steps  7 reward -1 next state  0 agent position  (3, 7)
agent epsilon  0.3395423728610988 agent memory len 1248 steps  7 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
actions ['S', 'L', 'D']
agent epsilon  0.3395423728610988 agent memory len 1487 steps  8 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.3395423728610988 agent memory len 1046 steps  8 reward -1 next state  0 agent position  (3, 6)
agent epsilon  0.3395423728610988 agent memory len 1249 steps  8 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.3395423728610988 agent memory len 1488 steps  9 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.3395423728610988 agent memory len 1047 steps  9 reward -1 next state  0 agent position  (3, 6)
agent epsilon  0.3395423728610988 agent memory len 1250 steps  9 reward -1 next state  3 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'D']
agent epsilon  0.3395423728610988 agent memory len 1489 steps  10 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.3395423728610988 agent memory len 1048 steps  10 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.3395423728610988 agent memory len 1251 steps  10 reward 0 next state  3 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1490 steps  11 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1049 steps  11 reward -1 next state  1 agent position  (3, 4)
agent epsilon  0.3395423728610988 agent memory len 1252 steps  11 reward 0 next state  3 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['U', 'U', 'S']
agent epsilon  0.3395423728610988 agent memory len 1491 steps  12 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.3395423728610988 agent memory len 1050 steps  12 reward -1 next state  1 agent position  (2, 4)
agent epsilon  0.3395423728610988 agent memory len 1253 steps  12 reward 0 next state  2 agent position  (7, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1492 steps  13 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.3395423728610988 agent memory len 1051 steps  13 reward -1 next state  1 agent position  (3, 4)
agent epsilon  0.3395423728610988 agent memory len 1254 steps  13 reward 0 next state  3 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1493 steps  14 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1052 steps  14 reward -1 next state  2 agent position  (4, 4)
agent epsilon  0.3395423728610988 agent memory len 1255 steps  14 reward 0 next state  4 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1494 steps  15 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.3395423728610988 agent memory len 1053 steps  15 reward -1 next state  3 agent position  (5, 4)
agent epsilon  0.3395423728610988 agent memory len 1256 steps  15 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'L']
agent epsilon  0.3395423728610988 agent memory len 1495 steps  16 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1054 steps  16 reward 0 next state  3 agent position  (6, 4)
agent epsilon  0.3395423728610988 agent memory len 1257 steps  16 reward 0 next state  6 agent position  (7, 8)
 is_terminal [False, False, False]
random action 0
actions ['L', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1496 steps  17 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1055 steps  17 reward 0 next state  2 agent position  (6, 3)
agent epsilon  0.3395423728610988 agent memory len 1258 steps  17 reward 0 next state  6 agent position  (7, 7)
 is_terminal [False, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1497 steps  18 reward -1 next state  7 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1056 steps  18 reward 0 next state  2 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1259 steps  18 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1498 steps  19 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1057 steps  19 reward -1 next state  3 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1260 steps  19 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'U', 'S']
agent epsilon  0.3395423728610988 agent memory len 1499 steps  20 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1058 steps  20 reward -1 next state  3 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1261 steps  20 reward -1 next state  6 agent position  (7, 6)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'S', 'R']
agent epsilon  0.3395423728610988 agent memory len 1500 steps  21 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1059 steps  21 reward -1 next state  3 agent position  (6, 2)
agent epsilon  0.3395423728610988 agent memory len 1262 steps  21 reward 0 next state  6 agent position  (7, 7)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1501 steps  22 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1060 steps  22 reward -1 next state  3 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1263 steps  22 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'L']
agent epsilon  0.3395423728610988 agent memory len 1502 steps  23 reward 0 next state  7 agent position  (7, 3)
agent epsilon  0.3395423728610988 agent memory len 1061 steps  23 reward -1 next state  3 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1264 steps  23 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, False, False]
random action 0
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['R', 'R', 'R']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  24 reward 10 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1062 steps  24 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1265 steps  24 reward 0 next state  7 agent position  (7, 7)
 is_terminal [True, False, False]
random action 1
actions ['S', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  25 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1063 steps  25 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1266 steps  25 reward 0 next state  7 agent position  (7, 8)
 is_terminal [True, False, False]
random action 1
actions ['S', 'D', 'L']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  26 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1064 steps  26 reward 0 next state  4 agent position  (8, 1)
agent epsilon  0.3395423728610988 agent memory len 1267 steps  26 reward 0 next state  8 agent position  (7, 7)
 is_terminal [True, False, False]
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  27 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1065 steps  27 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1268 steps  27 reward 0 next state  8 agent position  (7, 7)
 is_terminal [True, False, False]
actions ['S', 'L', 'R']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  28 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1066 steps  28 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1269 steps  28 reward 0 next state  8 agent position  (7, 8)
 is_terminal [True, False, False]
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'L', 'D']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  29 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1067 steps  29 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  29 reward 10 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  30 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1068 steps  30 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  30 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  31 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1069 steps  31 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  31 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  32 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1070 steps  32 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  32 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  33 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1071 steps  33 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  33 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  34 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1072 steps  34 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  34 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  35 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1073 steps  35 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  35 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  36 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1074 steps  36 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  36 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  37 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1075 steps  37 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  37 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  38 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1076 steps  38 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  38 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  39 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1077 steps  39 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  39 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  40 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1078 steps  40 reward -1 next state  4 agent position  (7, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  40 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  41 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1079 steps  41 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  41 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1080 steps  42 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1081 steps  43 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  44 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1082 steps  44 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1083 steps  45 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  46 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1084 steps  46 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  47 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1085 steps  47 reward 0 next state  4 agent position  (8, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  47 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1086 steps  48 reward 0 next state  4 agent position  (8, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  48 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  49 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1087 steps  49 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  49 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1088 steps  50 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  50 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  51 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1089 steps  51 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  51 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  52 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1090 steps  52 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  52 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  53 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1091 steps  53 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  53 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  54 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1092 steps  54 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  54 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  55 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1093 steps  55 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  55 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  56 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1094 steps  56 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  56 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  57 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1095 steps  57 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  57 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  58 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1096 steps  58 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  58 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1097 steps  59 reward -1 next state  4 agent position  (7, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  60 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1098 steps  60 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  61 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1099 steps  61 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  62 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1100 steps  62 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  63 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1101 steps  63 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  64 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1102 steps  64 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1103 steps  65 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1104 steps  66 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1105 steps  67 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  68 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1106 steps  68 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'R', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  69 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1107 steps  69 reward -1 next state  4 agent position  (7, 2)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1108 steps  70 reward -1 next state  4 agent position  (7, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  71 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1109 steps  71 reward 0 next state  4 agent position  (8, 1)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  71 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  72 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1110 steps  72 reward -1 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  72 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1111 steps  73 reward -2 next state  4 agent position  (8, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  73 reward -1 next state  8 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  74 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1112 steps  74 reward -1 next state  4 agent position  (9, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  74 reward -1 next state  9 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'L', 'S']
agent epsilon  0.3395423728610988 agent memory len 1503 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.3395423728610988 agent memory len 1113 steps  75 reward -2 next state  4 agent position  (9, 0)
agent epsilon  0.3395423728610988 agent memory len 1270 steps  75 reward -1 next state  9 agent position  (8, 8)
max steps reached
total rewards -188
epsilon  0.32347040168526264
epsilon  0.32347040168526264
epsilon  0.32347040168526264
Episode number:  24
 is_terminal [False, False, False]
actions ['D', 'D', 'L']
agent epsilon  0.32347040168526264 agent memory len 1504 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.32347040168526264 agent memory len 1114 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.32347040168526264 agent memory len 1271 steps  1 reward -1 next state  1 agent position  (0, 8)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.32347040168526264 agent memory len 1505 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.32347040168526264 agent memory len 1115 steps  2 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.32347040168526264 agent memory len 1272 steps  2 reward -1 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1506 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.32347040168526264 agent memory len 1116 steps  3 reward -1 next state  1 agent position  (2, 4)
agent epsilon  0.32347040168526264 agent memory len 1273 steps  3 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1507 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.32347040168526264 agent memory len 1117 steps  4 reward -1 next state  2 agent position  (3, 4)
agent epsilon  0.32347040168526264 agent memory len 1274 steps  4 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1508 steps  5 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.32347040168526264 agent memory len 1118 steps  5 reward -1 next state  3 agent position  (4, 4)
agent epsilon  0.32347040168526264 agent memory len 1275 steps  5 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1509 steps  6 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.32347040168526264 agent memory len 1119 steps  6 reward -1 next state  4 agent position  (5, 4)
agent epsilon  0.32347040168526264 agent memory len 1276 steps  6 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.32347040168526264 agent memory len 1510 steps  7 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.32347040168526264 agent memory len 1120 steps  7 reward 0 next state  4 agent position  (6, 4)
agent epsilon  0.32347040168526264 agent memory len 1277 steps  7 reward -2 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'D', 'D']
agent epsilon  0.32347040168526264 agent memory len 1511 steps  8 reward -1 next state  2 agent position  (2, 4)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  8 reward 10 next state  4 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1278 steps  8 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1512 steps  9 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  9 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1279 steps  9 reward -2 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1513 steps  10 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  10 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1280 steps  10 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1514 steps  11 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  11 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1281 steps  11 reward 0 next state  7 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1515 steps  12 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  12 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1282 steps  12 reward 0 next state  7 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1516 steps  13 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  13 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1283 steps  13 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1517 steps  14 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  14 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1284 steps  14 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1518 steps  15 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  15 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1285 steps  15 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1519 steps  16 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  16 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1286 steps  16 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1520 steps  17 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  17 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1287 steps  17 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1521 steps  18 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  18 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1288 steps  18 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1522 steps  19 reward -1 next state  3 agent position  (3, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  19 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1289 steps  19 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1523 steps  20 reward -1 next state  2 agent position  (2, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  20 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1290 steps  20 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1524 steps  21 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  21 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1291 steps  21 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1525 steps  22 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  22 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1292 steps  22 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1526 steps  23 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  23 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1293 steps  23 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1527 steps  24 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  24 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1294 steps  24 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1528 steps  25 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  25 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1295 steps  25 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1529 steps  26 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  26 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1296 steps  26 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1530 steps  27 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  27 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1297 steps  27 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1531 steps  28 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  28 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1298 steps  28 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1532 steps  29 reward -1 next state  2 agent position  (2, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  29 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1299 steps  29 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1533 steps  30 reward -1 next state  2 agent position  (2, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  30 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1300 steps  30 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1534 steps  31 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  31 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1301 steps  31 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1535 steps  32 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  32 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1302 steps  32 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1536 steps  33 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  33 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1303 steps  33 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1537 steps  34 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  34 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1304 steps  34 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1538 steps  35 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  35 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1305 steps  35 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1539 steps  36 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  36 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1306 steps  36 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1540 steps  37 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  37 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1307 steps  37 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1541 steps  38 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  38 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1308 steps  38 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1542 steps  39 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  39 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1309 steps  39 reward -1 next state  7 agent position  (6, 4)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1543 steps  40 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  40 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1310 steps  40 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1544 steps  41 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  41 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1311 steps  41 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1545 steps  42 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1312 steps  42 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1546 steps  43 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  43 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1313 steps  43 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1547 steps  44 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  44 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1314 steps  44 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1548 steps  45 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  45 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1315 steps  45 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1549 steps  46 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  46 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1316 steps  46 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1550 steps  47 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  47 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1317 steps  47 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1551 steps  48 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1318 steps  48 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1552 steps  49 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  49 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1319 steps  49 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1553 steps  50 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1320 steps  50 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1554 steps  51 reward -1 next state  1 agent position  (1, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  51 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1321 steps  51 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1555 steps  52 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  52 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1322 steps  52 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1556 steps  53 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  53 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1323 steps  53 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.32347040168526264 agent memory len 1557 steps  54 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  54 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1324 steps  54 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1558 steps  55 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  55 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1325 steps  55 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1559 steps  56 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  56 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1326 steps  56 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1560 steps  57 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  57 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1327 steps  57 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1561 steps  58 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  58 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1328 steps  58 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1562 steps  59 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1329 steps  59 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1563 steps  60 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  60 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1330 steps  60 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1564 steps  61 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  61 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1331 steps  61 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1565 steps  62 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  62 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1332 steps  62 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1566 steps  63 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  63 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1333 steps  63 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.32347040168526264 agent memory len 1567 steps  64 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  64 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1334 steps  64 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1568 steps  65 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1335 steps  65 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.32347040168526264 agent memory len 1569 steps  66 reward -1 next state  0 agent position  (0, 6)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  66 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1336 steps  66 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1570 steps  67 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1337 steps  67 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['R', 'S', 'R']
agent epsilon  0.32347040168526264 agent memory len 1571 steps  68 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  68 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  68 reward 10 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1572 steps  69 reward -2 next state  0 agent position  (0, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  69 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1573 steps  70 reward -1 next state  0 agent position  (0, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1574 steps  71 reward -1 next state  0 agent position  (0, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  71 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1575 steps  72 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  72 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1576 steps  73 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1577 steps  74 reward -1 next state  1 agent position  (1, 8)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  74 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.32347040168526264 agent memory len 1578 steps  75 reward -1 next state  1 agent position  (1, 7)
agent epsilon  0.32347040168526264 agent memory len 1121 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.32347040168526264 agent memory len 1338 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -189
epsilon  0.30818226979308
epsilon  0.30818226979308
epsilon  0.30818226979308
Episode number:  25
 is_terminal [False, False, False]
actions ['U', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1579 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1122 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.30818226979308 agent memory len 1339 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 1
actions ['U', 'L', 'D']
agent epsilon  0.30818226979308 agent memory len 1580 steps  2 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1123 steps  2 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.30818226979308 agent memory len 1340 steps  2 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1581 steps  3 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1124 steps  3 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.30818226979308 agent memory len 1341 steps  3 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['U', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1582 steps  4 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1125 steps  4 reward -1 next state  0 agent position  (3, 4)
agent epsilon  0.30818226979308 agent memory len 1342 steps  4 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1583 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.30818226979308 agent memory len 1126 steps  5 reward -1 next state  0 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1343 steps  5 reward -1 next state  4 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1584 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.30818226979308 agent memory len 1127 steps  6 reward -1 next state  0 agent position  (5, 4)
agent epsilon  0.30818226979308 agent memory len 1344 steps  6 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1585 steps  7 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1128 steps  7 reward 0 next state  0 agent position  (6, 4)
agent epsilon  0.30818226979308 agent memory len 1345 steps  7 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'U']
agent epsilon  0.30818226979308 agent memory len 1586 steps  8 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.30818226979308 agent memory len 1129 steps  8 reward 0 next state  0 agent position  (6, 3)
agent epsilon  0.30818226979308 agent memory len 1346 steps  8 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
actions ['U', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1587 steps  9 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1130 steps  9 reward 0 next state  0 agent position  (6, 3)
agent epsilon  0.30818226979308 agent memory len 1347 steps  9 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1588 steps  10 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1131 steps  10 reward -1 next state  0 agent position  (6, 2)
agent epsilon  0.30818226979308 agent memory len 1348 steps  10 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1589 steps  11 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1132 steps  11 reward -1 next state  0 agent position  (6, 1)
agent epsilon  0.30818226979308 agent memory len 1349 steps  11 reward 0 next state  6 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'U', 'U']
agent epsilon  0.30818226979308 agent memory len 1590 steps  12 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1133 steps  12 reward -1 next state  0 agent position  (5, 1)
agent epsilon  0.30818226979308 agent memory len 1350 steps  12 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'R', 'D']
agent epsilon  0.30818226979308 agent memory len 1591 steps  13 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.30818226979308 agent memory len 1134 steps  13 reward -1 next state  0 agent position  (5, 2)
agent epsilon  0.30818226979308 agent memory len 1351 steps  13 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.30818226979308 agent memory len 1592 steps  14 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1135 steps  14 reward -1 next state  0 agent position  (6, 2)
agent epsilon  0.30818226979308 agent memory len 1352 steps  14 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 2
actions ['U', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1593 steps  15 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.30818226979308 agent memory len 1136 steps  15 reward 0 next state  0 agent position  (6, 3)
agent epsilon  0.30818226979308 agent memory len 1353 steps  15 reward -1 next state  6 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'U']
agent epsilon  0.30818226979308 agent memory len 1594 steps  16 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.30818226979308 agent memory len 1137 steps  16 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.30818226979308 agent memory len 1354 steps  16 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'U', 'D']
agent epsilon  0.30818226979308 agent memory len 1595 steps  17 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.30818226979308 agent memory len 1138 steps  17 reward -1 next state  1 agent position  (5, 2)
agent epsilon  0.30818226979308 agent memory len 1355 steps  17 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1596 steps  18 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1139 steps  18 reward -1 next state  1 agent position  (5, 2)
agent epsilon  0.30818226979308 agent memory len 1356 steps  18 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['U', 'R', 'D']
agent epsilon  0.30818226979308 agent memory len 1597 steps  19 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.30818226979308 agent memory len 1140 steps  19 reward -1 next state  1 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1357 steps  19 reward 0 next state  5 agent position  (8, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.30818226979308 agent memory len 1598 steps  20 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1141 steps  20 reward 0 next state  1 agent position  (6, 3)
agent epsilon  0.30818226979308 agent memory len 1358 steps  20 reward 0 next state  6 agent position  (8, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'D', 'D']
agent epsilon  0.30818226979308 agent memory len 1599 steps  21 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.30818226979308 agent memory len 1142 steps  21 reward 0 next state  1 agent position  (7, 3)
agent epsilon  0.30818226979308 agent memory len 1359 steps  21 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'L', 'L']
agent epsilon  0.30818226979308 agent memory len 1600 steps  22 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1143 steps  22 reward -1 next state  2 agent position  (7, 2)
agent epsilon  0.30818226979308 agent memory len 1360 steps  22 reward 0 next state  7 agent position  (9, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1601 steps  23 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1144 steps  23 reward -1 next state  2 agent position  (7, 1)
agent epsilon  0.30818226979308 agent memory len 1361 steps  23 reward 0 next state  7 agent position  (9, 8)
 is_terminal [False, False, False]
random action 2
actions ['D', 'L', 'R']
agent epsilon  0.30818226979308 agent memory len 1602 steps  24 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1145 steps  24 reward -1 next state  2 agent position  (7, 0)
agent epsilon  0.30818226979308 agent memory len 1362 steps  24 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1603 steps  25 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1146 steps  25 reward -1 next state  2 agent position  (7, 1)
agent epsilon  0.30818226979308 agent memory len 1363 steps  25 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['U', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1604 steps  26 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1147 steps  26 reward -1 next state  2 agent position  (7, 2)
agent epsilon  0.30818226979308 agent memory len 1364 steps  26 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
random action 2
actions ['L', 'L', 'D']
agent epsilon  0.30818226979308 agent memory len 1605 steps  27 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1148 steps  27 reward -1 next state  1 agent position  (7, 1)
agent epsilon  0.30818226979308 agent memory len 1365 steps  27 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1606 steps  28 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1149 steps  28 reward -1 next state  2 agent position  (7, 0)
agent epsilon  0.30818226979308 agent memory len 1366 steps  28 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1607 steps  29 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1150 steps  29 reward -1 next state  2 agent position  (7, 1)
agent epsilon  0.30818226979308 agent memory len 1367 steps  29 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['U', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1608 steps  30 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1151 steps  30 reward -1 next state  2 agent position  (7, 2)
agent epsilon  0.30818226979308 agent memory len 1368 steps  30 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['D', 'L', 'S']
agent epsilon  0.30818226979308 agent memory len 1609 steps  31 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1152 steps  31 reward -1 next state  2 agent position  (7, 1)
agent epsilon  0.30818226979308 agent memory len 1369 steps  31 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
actions ['U', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1610 steps  32 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1153 steps  32 reward -1 next state  2 agent position  (7, 2)
agent epsilon  0.30818226979308 agent memory len 1370 steps  32 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['L', 'R', 'S']
agent epsilon  0.30818226979308 agent memory len 1611 steps  33 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.30818226979308 agent memory len 1154 steps  33 reward 0 next state  1 agent position  (7, 3)
agent epsilon  0.30818226979308 agent memory len 1371 steps  33 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'R', 'R']
agent epsilon  0.30818226979308 agent memory len 1612 steps  34 reward -1 next state  1 agent position  (1, 2)
agent epsilon  0.30818226979308 agent memory len 1155 steps  34 reward 10 next state  2 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1372 steps  34 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1613 steps  35 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.30818226979308 agent memory len 1155 steps  35 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1373 steps  35 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1614 steps  36 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.30818226979308 agent memory len 1155 steps  36 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1374 steps  36 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1615 steps  37 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  37 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1375 steps  37 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1616 steps  38 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  38 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1376 steps  38 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1617 steps  39 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  39 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1377 steps  39 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.30818226979308 agent memory len 1618 steps  40 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  40 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1378 steps  40 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1619 steps  41 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  41 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1379 steps  41 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1620 steps  42 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  42 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1380 steps  42 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1621 steps  43 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  43 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1381 steps  43 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1622 steps  44 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  44 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1382 steps  44 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1623 steps  45 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  45 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1383 steps  45 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1624 steps  46 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  46 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1384 steps  46 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1625 steps  47 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.30818226979308 agent memory len 1155 steps  47 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1385 steps  47 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1626 steps  48 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  48 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1386 steps  48 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1627 steps  49 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  49 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1387 steps  49 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1628 steps  50 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  50 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1388 steps  50 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1629 steps  51 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  51 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1389 steps  51 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1630 steps  52 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  52 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1390 steps  52 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1631 steps  53 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  53 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1391 steps  53 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1632 steps  54 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  54 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1392 steps  54 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1633 steps  55 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  55 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1393 steps  55 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1634 steps  56 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  56 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1394 steps  56 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1635 steps  57 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  57 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1395 steps  57 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1636 steps  58 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  58 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1396 steps  58 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1637 steps  59 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  59 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1397 steps  59 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1638 steps  60 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  60 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1398 steps  60 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1639 steps  61 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  61 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1399 steps  61 reward -2 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1640 steps  62 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  62 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1400 steps  62 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1641 steps  63 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  63 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1401 steps  63 reward 0 next state  7 agent position  (9, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1642 steps  64 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  64 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1402 steps  64 reward 0 next state  7 agent position  (9, 8)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1643 steps  65 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.30818226979308 agent memory len 1155 steps  65 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1403 steps  65 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1644 steps  66 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  66 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1404 steps  66 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.30818226979308 agent memory len 1645 steps  67 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  67 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1405 steps  67 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1646 steps  68 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  68 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1406 steps  68 reward -1 next state  7 agent position  (9, 6)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.30818226979308 agent memory len 1647 steps  69 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  69 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1407 steps  69 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.30818226979308 agent memory len 1648 steps  70 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  70 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1408 steps  70 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'D']
agent epsilon  0.30818226979308 agent memory len 1649 steps  71 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  71 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1409 steps  71 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1650 steps  72 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  72 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1410 steps  72 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1651 steps  73 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  73 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1411 steps  73 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1652 steps  74 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.30818226979308 agent memory len 1155 steps  74 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1412 steps  74 reward 0 next state  7 agent position  (9, 7)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.30818226979308 agent memory len 1653 steps  75 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.30818226979308 agent memory len 1155 steps  75 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.30818226979308 agent memory len 1413 steps  75 reward 0 next state  7 agent position  (9, 7)
max steps reached
total rewards -164
epsilon  0.29363974889158817
epsilon  0.29363974889158817
epsilon  0.29363974889158817
Episode number:  26
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1654 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.29363974889158817 agent memory len 1156 steps  1 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.29363974889158817 agent memory len 1414 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1655 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.29363974889158817 agent memory len 1157 steps  2 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.29363974889158817 agent memory len 1415 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1656 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.29363974889158817 agent memory len 1158 steps  3 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.29363974889158817 agent memory len 1416 steps  3 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.29363974889158817 agent memory len 1657 steps  4 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.29363974889158817 agent memory len 1159 steps  4 reward -1 next state  1 agent position  (2, 4)
agent epsilon  0.29363974889158817 agent memory len 1417 steps  4 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1658 steps  5 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.29363974889158817 agent memory len 1160 steps  5 reward -1 next state  1 agent position  (3, 4)
agent epsilon  0.29363974889158817 agent memory len 1418 steps  5 reward -1 next state  3 agent position  (5, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1659 steps  6 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.29363974889158817 agent memory len 1161 steps  6 reward -1 next state  1 agent position  (4, 4)
agent epsilon  0.29363974889158817 agent memory len 1419 steps  6 reward -1 next state  4 agent position  (6, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.29363974889158817 agent memory len 1660 steps  7 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.29363974889158817 agent memory len 1162 steps  7 reward -1 next state  1 agent position  (5, 4)
agent epsilon  0.29363974889158817 agent memory len 1420 steps  7 reward 0 next state  5 agent position  (7, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'L']
agent epsilon  0.29363974889158817 agent memory len 1661 steps  8 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.29363974889158817 agent memory len 1163 steps  8 reward 0 next state  1 agent position  (6, 4)
agent epsilon  0.29363974889158817 agent memory len 1421 steps  8 reward 0 next state  6 agent position  (7, 8)
 is_terminal [False, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['L', 'D', 'U']
agent epsilon  0.29363974889158817 agent memory len 1662 steps  9 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  9 reward 10 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1422 steps  9 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1663 steps  10 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  10 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1423 steps  10 reward 0 next state  7 agent position  (7, 8)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1664 steps  11 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  11 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1424 steps  11 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1665 steps  12 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  12 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1425 steps  12 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1666 steps  13 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  13 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1426 steps  13 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1667 steps  14 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  14 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1427 steps  14 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1668 steps  15 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  15 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1428 steps  15 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1669 steps  16 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  16 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1429 steps  16 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1670 steps  17 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  17 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1430 steps  17 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1671 steps  18 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  18 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1431 steps  18 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1672 steps  19 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  19 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1432 steps  19 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1673 steps  20 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  20 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1433 steps  20 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1674 steps  21 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  21 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1434 steps  21 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1675 steps  22 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  22 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1435 steps  22 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.29363974889158817 agent memory len 1676 steps  23 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  23 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1436 steps  23 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1677 steps  24 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  24 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1437 steps  24 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.29363974889158817 agent memory len 1678 steps  25 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  25 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1438 steps  25 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1679 steps  26 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  26 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1439 steps  26 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1680 steps  27 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  27 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1440 steps  27 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1681 steps  28 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  28 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1441 steps  28 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1682 steps  29 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  29 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1442 steps  29 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1683 steps  30 reward -1 next state  7 agent position  (7, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  30 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1443 steps  30 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1684 steps  31 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  31 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1444 steps  31 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1685 steps  32 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  32 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1445 steps  32 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1686 steps  33 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  33 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1446 steps  33 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1687 steps  34 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  34 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1447 steps  34 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1688 steps  35 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  35 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1448 steps  35 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1689 steps  36 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  36 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1449 steps  36 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1690 steps  37 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  37 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1450 steps  37 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1691 steps  38 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  38 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1451 steps  38 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1692 steps  39 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  39 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1452 steps  39 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'U']
agent epsilon  0.29363974889158817 agent memory len 1693 steps  40 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  40 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1453 steps  40 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1694 steps  41 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  41 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1454 steps  41 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1695 steps  42 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  42 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1455 steps  42 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1696 steps  43 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  43 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1456 steps  43 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1697 steps  44 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  44 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1457 steps  44 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1698 steps  45 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  45 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1458 steps  45 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1699 steps  46 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  46 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1459 steps  46 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1700 steps  47 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  47 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1460 steps  47 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1701 steps  48 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  48 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1461 steps  48 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1702 steps  49 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  49 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1462 steps  49 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1703 steps  50 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  50 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1463 steps  50 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1704 steps  51 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  51 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1464 steps  51 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1705 steps  52 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  52 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1465 steps  52 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1706 steps  53 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  53 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1466 steps  53 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1707 steps  54 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  54 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1467 steps  54 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['S', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1708 steps  55 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  55 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1468 steps  55 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1709 steps  56 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  56 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1469 steps  56 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1710 steps  57 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  57 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1470 steps  57 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1711 steps  58 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  58 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1471 steps  58 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1712 steps  59 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  59 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1472 steps  59 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1713 steps  60 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  60 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1473 steps  60 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1714 steps  61 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  61 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1474 steps  61 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1715 steps  62 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  62 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1475 steps  62 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1716 steps  63 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  63 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1476 steps  63 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1717 steps  64 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  64 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1477 steps  64 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1718 steps  65 reward -2 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  65 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1478 steps  65 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.29363974889158817 agent memory len 1719 steps  66 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  66 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1479 steps  66 reward -1 next state  7 agent position  (6, 5)
 is_terminal [False, True, False]
random action 2
actions ['L', 'S', 'S']
agent epsilon  0.29363974889158817 agent memory len 1720 steps  67 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  67 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1480 steps  67 reward -1 next state  7 agent position  (6, 5)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'D']
agent epsilon  0.29363974889158817 agent memory len 1721 steps  68 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  68 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1481 steps  68 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1722 steps  69 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  69 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1482 steps  69 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1723 steps  70 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  70 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1483 steps  70 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
actions ['L', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1724 steps  71 reward -2 next state  7 agent position  (7, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  71 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1484 steps  71 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1725 steps  72 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  72 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1485 steps  72 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1726 steps  73 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  73 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1486 steps  73 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'L']
agent epsilon  0.29363974889158817 agent memory len 1727 steps  74 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  74 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1487 steps  74 reward -1 next state  7 agent position  (7, 5)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'R']
agent epsilon  0.29363974889158817 agent memory len 1728 steps  75 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.29363974889158817 agent memory len 1164 steps  75 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.29363974889158817 agent memory len 1488 steps  75 reward -1 next state  7 agent position  (7, 6)
max steps reached
total rewards -252
epsilon  0.27980647510367246
epsilon  0.27980647510367246
epsilon  0.27980647510367246
Episode number:  27
 is_terminal [False, False, False]
random action 1
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.27980647510367246 agent memory len 1729 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.27980647510367246 agent memory len 1165 steps  1 reward -1 next state  0 agent position  (0, 5)
agent epsilon  0.27980647510367246 agent memory len 1489 steps  1 reward -1 next state  0 agent position  (0, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.27980647510367246 agent memory len 1730 steps  2 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.27980647510367246 agent memory len 1166 steps  2 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.27980647510367246 agent memory len 1490 steps  2 reward -2 next state  1 agent position  (0, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1731 steps  3 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.27980647510367246 agent memory len 1167 steps  3 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.27980647510367246 agent memory len 1491 steps  3 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1732 steps  4 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.27980647510367246 agent memory len 1168 steps  4 reward -1 next state  1 agent position  (3, 5)
agent epsilon  0.27980647510367246 agent memory len 1492 steps  4 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1733 steps  5 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.27980647510367246 agent memory len 1169 steps  5 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.27980647510367246 agent memory len 1493 steps  5 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1734 steps  6 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.27980647510367246 agent memory len 1170 steps  6 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.27980647510367246 agent memory len 1494 steps  6 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'L']
agent epsilon  0.27980647510367246 agent memory len 1735 steps  7 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.27980647510367246 agent memory len 1171 steps  7 reward 0 next state  1 agent position  (6, 5)
agent epsilon  0.27980647510367246 agent memory len 1495 steps  7 reward -1 next state  6 agent position  (4, 8)
 is_terminal [False, False, False]
actions ['D', 'L', 'L']
agent epsilon  0.27980647510367246 agent memory len 1736 steps  8 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.27980647510367246 agent memory len 1172 steps  8 reward 0 next state  1 agent position  (6, 4)
agent epsilon  0.27980647510367246 agent memory len 1496 steps  8 reward -1 next state  6 agent position  (4, 7)
 is_terminal [False, False, False]
actions ['D', 'L', 'D']
agent epsilon  0.27980647510367246 agent memory len 1737 steps  9 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.27980647510367246 agent memory len 1173 steps  9 reward 0 next state  1 agent position  (6, 3)
agent epsilon  0.27980647510367246 agent memory len 1497 steps  9 reward -1 next state  6 agent position  (5, 7)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'D']
agent epsilon  0.27980647510367246 agent memory len 1738 steps  10 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.27980647510367246 agent memory len 1174 steps  10 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.27980647510367246 agent memory len 1498 steps  10 reward -1 next state  6 agent position  (6, 7)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'U', 'R']
agent epsilon  0.27980647510367246 agent memory len 1739 steps  11 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.27980647510367246 agent memory len 1175 steps  11 reward -1 next state  1 agent position  (5, 2)
agent epsilon  0.27980647510367246 agent memory len 1499 steps  11 reward -1 next state  5 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.27980647510367246 agent memory len 1740 steps  12 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.27980647510367246 agent memory len 1176 steps  12 reward -1 next state  1 agent position  (6, 2)
agent epsilon  0.27980647510367246 agent memory len 1500 steps  12 reward -1 next state  6 agent position  (5, 8)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1741 steps  13 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.27980647510367246 agent memory len 1177 steps  13 reward -1 next state  1 agent position  (7, 2)
agent epsilon  0.27980647510367246 agent memory len 1501 steps  13 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'D']
agent epsilon  0.27980647510367246 agent memory len 1742 steps  14 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.27980647510367246 agent memory len 1178 steps  14 reward 0 next state  2 agent position  (8, 2)
agent epsilon  0.27980647510367246 agent memory len 1502 steps  14 reward 0 next state  8 agent position  (7, 8)
 is_terminal [False, False, False]
random action 1
actions ['D', 'U', 'L']
agent epsilon  0.27980647510367246 agent memory len 1743 steps  15 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.27980647510367246 agent memory len 1179 steps  15 reward -1 next state  2 agent position  (7, 2)
agent epsilon  0.27980647510367246 agent memory len 1503 steps  15 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, False, False]
actions ['R', 'D', 'L']
agent epsilon  0.27980647510367246 agent memory len 1744 steps  16 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.27980647510367246 agent memory len 1180 steps  16 reward 0 next state  3 agent position  (8, 2)
agent epsilon  0.27980647510367246 agent memory len 1504 steps  16 reward -1 next state  8 agent position  (7, 6)
 is_terminal [False, False, False]
random action 2
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['R', 'D', 'R']
agent epsilon  0.27980647510367246 agent memory len 1745 steps  17 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  17 reward 10 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1505 steps  17 reward 0 next state  9 agent position  (7, 7)
 is_terminal [False, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1746 steps  18 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  18 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1506 steps  18 reward -1 next state  9 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.27980647510367246 agent memory len 1747 steps  19 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  19 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1507 steps  19 reward -1 next state  9 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1748 steps  20 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  20 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1508 steps  20 reward 0 next state  9 agent position  (7, 7)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1749 steps  21 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  21 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1509 steps  21 reward -1 next state  9 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1750 steps  22 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  22 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1510 steps  22 reward 0 next state  9 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.27980647510367246 agent memory len 1751 steps  23 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  23 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1511 steps  23 reward -1 next state  9 agent position  (6, 7)
 is_terminal [False, True, False]
random action 0
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  24 reward 10 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  24 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1512 steps  24 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  25 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  25 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1513 steps  25 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  26 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  26 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1514 steps  26 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  27 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  27 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1515 steps  27 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  28 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  28 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1516 steps  28 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  29 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  29 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1517 steps  29 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  30 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  30 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1518 steps  30 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  31 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  31 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1519 steps  31 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  32 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  32 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1520 steps  32 reward -1 next state  9 agent position  (6, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  33 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  33 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1521 steps  33 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  34 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  34 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1522 steps  34 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  35 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  35 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1523 steps  35 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  36 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  36 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1524 steps  36 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  37 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  37 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1525 steps  37 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  38 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  38 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1526 steps  38 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  39 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  39 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1527 steps  39 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  40 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  40 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1528 steps  40 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  41 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  41 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1529 steps  41 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  42 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1530 steps  42 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  43 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1531 steps  43 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  44 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  44 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1532 steps  44 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  45 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1533 steps  45 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  46 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  46 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1534 steps  46 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  47 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  47 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1535 steps  47 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  48 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1536 steps  48 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  49 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  49 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1537 steps  49 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  50 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1538 steps  50 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  51 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  51 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1539 steps  51 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'S']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  52 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  52 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1540 steps  52 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  53 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  53 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1541 steps  53 reward -1 next state  9 agent position  (6, 5)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  54 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  54 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1542 steps  54 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  55 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  55 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1543 steps  55 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  56 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  56 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1544 steps  56 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  57 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  57 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1545 steps  57 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'D']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  58 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  58 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1546 steps  58 reward 0 next state  9 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  59 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1547 steps  59 reward -1 next state  9 agent position  (7, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  60 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  60 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1548 steps  60 reward 0 next state  9 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  61 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  61 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1549 steps  61 reward -1 next state  9 agent position  (7, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  62 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  62 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1550 steps  62 reward 0 next state  9 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  63 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  63 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1551 steps  63 reward -1 next state  9 agent position  (7, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  64 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  64 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1552 steps  64 reward 0 next state  9 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  65 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1553 steps  65 reward -1 next state  9 agent position  (7, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  66 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1554 steps  66 reward 0 next state  9 agent position  (7, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  67 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1555 steps  67 reward -1 next state  9 agent position  (7, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'U']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  68 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  68 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1556 steps  68 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  69 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  69 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1557 steps  69 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  70 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1558 steps  70 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  71 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  71 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1559 steps  71 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
random action 2
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  72 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  72 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1560 steps  72 reward -1 next state  9 agent position  (6, 8)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  73 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1561 steps  73 reward -1 next state  9 agent position  (6, 7)
 is_terminal [True, True, False]
actions ['S', 'S', 'L']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  74 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  74 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1562 steps  74 reward -1 next state  9 agent position  (6, 6)
 is_terminal [True, True, False]
actions ['S', 'S', 'R']
agent epsilon  0.27980647510367246 agent memory len 1752 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.27980647510367246 agent memory len 1181 steps  75 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.27980647510367246 agent memory len 1563 steps  75 reward -1 next state  9 agent position  (6, 7)
max steps reached
total rewards -182
epsilon  0.2666478580394326
epsilon  0.2666478580394326
epsilon  0.2666478580394326
Episode number:  28
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1753 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.2666478580394326 agent memory len 1182 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.2666478580394326 agent memory len 1564 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1754 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.2666478580394326 agent memory len 1183 steps  2 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.2666478580394326 agent memory len 1565 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['L', 'D', 'U']
agent epsilon  0.2666478580394326 agent memory len 1755 steps  3 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.2666478580394326 agent memory len 1184 steps  3 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.2666478580394326 agent memory len 1566 steps  3 reward -1 next state  3 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1756 steps  4 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.2666478580394326 agent memory len 1185 steps  4 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.2666478580394326 agent memory len 1567 steps  4 reward -1 next state  4 agent position  (2, 9)
 is_terminal [False, False, False]
random action 0
actions ['U', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1757 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.2666478580394326 agent memory len 1186 steps  5 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.2666478580394326 agent memory len 1568 steps  5 reward -1 next state  5 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'S']
agent epsilon  0.2666478580394326 agent memory len 1758 steps  6 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.2666478580394326 agent memory len 1187 steps  6 reward 0 next state  1 agent position  (6, 5)
agent epsilon  0.2666478580394326 agent memory len 1569 steps  6 reward -1 next state  6 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.2666478580394326 agent memory len 1759 steps  7 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.2666478580394326 agent memory len 1188 steps  7 reward 0 next state  1 agent position  (6, 4)
agent epsilon  0.2666478580394326 agent memory len 1570 steps  7 reward -1 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['D', 'D', 'D']
agent epsilon  0.2666478580394326 agent memory len 1760 steps  8 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  8 reward 10 next state  1 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1571 steps  8 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'R']
agent epsilon  0.2666478580394326 agent memory len 1761 steps  9 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  9 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1572 steps  9 reward -2 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1762 steps  10 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  10 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1573 steps  10 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
random action 0
actions ['L', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1763 steps  11 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  11 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1574 steps  11 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1764 steps  12 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  12 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1575 steps  12 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1765 steps  13 reward -1 next state  7 agent position  (7, 0)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  13 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1576 steps  13 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1766 steps  14 reward -1 next state  8 agent position  (8, 0)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  14 reward -1 next state  0 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1577 steps  14 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1767 steps  15 reward 0 next state  8 agent position  (8, 1)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  15 reward -1 next state  1 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1578 steps  15 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1768 steps  16 reward 0 next state  8 agent position  (8, 2)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  16 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1579 steps  16 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
random action 2
actions ['R', 'S', 'U']
agent epsilon  0.2666478580394326 agent memory len 1769 steps  17 reward 0 next state  8 agent position  (8, 3)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  17 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1580 steps  17 reward -1 next state  7 agent position  (5, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1770 steps  18 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  18 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1581 steps  18 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.2666478580394326 agent memory len 1771 steps  19 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  19 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1582 steps  19 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
actions ['R', 'S', 'L']
agent epsilon  0.2666478580394326 agent memory len 1772 steps  20 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  20 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1583 steps  20 reward -1 next state  7 agent position  (6, 7)
 is_terminal [False, True, False]
actions ['U', 'S', 'R']
agent epsilon  0.2666478580394326 agent memory len 1773 steps  21 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  21 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1584 steps  21 reward -1 next state  7 agent position  (6, 8)
 is_terminal [False, True, False]
random action 0
actions ['D', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1774 steps  22 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  22 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1585 steps  22 reward 0 next state  7 agent position  (7, 8)
 is_terminal [False, True, False]
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'S', 'D']
agent epsilon  0.2666478580394326 agent memory len 1775 steps  23 reward -2 next state  9 agent position  (9, 5)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  23 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  23 reward 10 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1776 steps  24 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  24 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  24 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1777 steps  25 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  25 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  25 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1778 steps  26 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  26 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  26 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1779 steps  27 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  27 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  27 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1780 steps  28 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  28 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  28 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1781 steps  29 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  29 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  29 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1782 steps  30 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  30 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  30 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1783 steps  31 reward -1 next state  9 agent position  (9, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  31 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  31 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1784 steps  32 reward -1 next state  8 agent position  (8, 6)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  32 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  32 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1785 steps  33 reward -1 next state  8 agent position  (8, 7)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  33 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  33 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1786 steps  34 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  34 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  34 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1787 steps  35 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  35 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  35 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1788 steps  36 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  36 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  36 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1789 steps  37 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  37 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  37 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1790 steps  38 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  38 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  38 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1791 steps  39 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  39 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  39 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1792 steps  40 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  40 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  40 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1793 steps  41 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  41 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  41 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1794 steps  42 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  42 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1795 steps  43 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  43 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1796 steps  44 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  44 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1797 steps  45 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  45 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1798 steps  46 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  46 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1799 steps  47 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  47 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  47 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1800 steps  48 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  48 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  48 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1801 steps  49 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  49 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  49 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1802 steps  50 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  50 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  50 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1803 steps  51 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  51 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1804 steps  52 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  52 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  52 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1805 steps  53 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  53 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  53 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1806 steps  54 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  54 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  54 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1807 steps  55 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  55 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  55 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1808 steps  56 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  56 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  56 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1809 steps  57 reward -1 next state  8 agent position  (8, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  57 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  57 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1810 steps  58 reward -1 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  58 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1811 steps  59 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  59 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1812 steps  60 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  60 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1813 steps  61 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  61 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1814 steps  62 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  62 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1815 steps  63 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  63 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1816 steps  64 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  64 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1817 steps  65 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  65 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1818 steps  66 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  66 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1819 steps  67 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  67 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1820 steps  68 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  68 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1821 steps  69 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  69 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1822 steps  70 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  70 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1823 steps  71 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  71 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1824 steps  72 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  72 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1825 steps  73 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  73 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1826 steps  74 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  74 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.2666478580394326 agent memory len 1827 steps  75 reward -2 next state  9 agent position  (9, 8)
agent epsilon  0.2666478580394326 agent memory len 1189 steps  75 reward -1 next state  8 agent position  (7, 4)
agent epsilon  0.2666478580394326 agent memory len 1586 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -232
epsilon  0.2541309943021904
epsilon  0.2541309943021904
epsilon  0.2541309943021904
Episode number:  29
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.2541309943021904 agent memory len 1828 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.2541309943021904 agent memory len 1190 steps  1 reward -1 next state  0 agent position  (0, 4)
agent epsilon  0.2541309943021904 agent memory len 1587 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.2541309943021904 agent memory len 1829 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.2541309943021904 agent memory len 1191 steps  2 reward -1 next state  0 agent position  (1, 4)
agent epsilon  0.2541309943021904 agent memory len 1588 steps  2 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.2541309943021904 agent memory len 1830 steps  3 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.2541309943021904 agent memory len 1192 steps  3 reward -1 next state  0 agent position  (2, 4)
agent epsilon  0.2541309943021904 agent memory len 1589 steps  3 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'L', 'D']
agent epsilon  0.2541309943021904 agent memory len 1831 steps  4 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.2541309943021904 agent memory len 1193 steps  4 reward -1 next state  0 agent position  (2, 3)
agent epsilon  0.2541309943021904 agent memory len 1590 steps  4 reward -1 next state  2 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['U', 'D', 'U']
agent epsilon  0.2541309943021904 agent memory len 1832 steps  5 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.2541309943021904 agent memory len 1194 steps  5 reward -1 next state  0 agent position  (3, 3)
agent epsilon  0.2541309943021904 agent memory len 1591 steps  5 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.2541309943021904 agent memory len 1833 steps  6 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.2541309943021904 agent memory len 1195 steps  6 reward -1 next state  0 agent position  (3, 4)
agent epsilon  0.2541309943021904 agent memory len 1592 steps  6 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['D', 'R', 'U']
agent epsilon  0.2541309943021904 agent memory len 1834 steps  7 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.2541309943021904 agent memory len 1196 steps  7 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.2541309943021904 agent memory len 1593 steps  7 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.2541309943021904 agent memory len 1835 steps  8 reward -2 next state  4 agent position  (4, 0)
agent epsilon  0.2541309943021904 agent memory len 1197 steps  8 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1594 steps  8 reward -1 next state  4 agent position  (4, 9)
 is_terminal [False, False, False]
random action 2
actions ['S', 'D', 'R']
agent epsilon  0.2541309943021904 agent memory len 1836 steps  9 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.2541309943021904 agent memory len 1198 steps  9 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1595 steps  9 reward -2 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 1
actions ['S', 'D', 'D']
agent epsilon  0.2541309943021904 agent memory len 1837 steps  10 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.2541309943021904 agent memory len 1199 steps  10 reward 0 next state  0 agent position  (6, 5)
agent epsilon  0.2541309943021904 agent memory len 1596 steps  10 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
actions ['D', 'U', 'D']
agent epsilon  0.2541309943021904 agent memory len 1838 steps  11 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.2541309943021904 agent memory len 1200 steps  11 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1597 steps  11 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
actions ['S', 'U', 'D']
agent epsilon  0.2541309943021904 agent memory len 1839 steps  12 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.2541309943021904 agent memory len 1201 steps  12 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1598 steps  12 reward 0 next state  4 agent position  (7, 9)
 is_terminal [False, False, False]
random action 0
random action 2
actions ['R', 'D', 'L']
agent epsilon  0.2541309943021904 agent memory len 1840 steps  13 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1202 steps  13 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1599 steps  13 reward 0 next state  5 agent position  (7, 8)
 is_terminal [False, False, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'U', 'D']
agent epsilon  0.2541309943021904 agent memory len 1841 steps  14 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1203 steps  14 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  14 reward 10 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1842 steps  15 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1204 steps  15 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  15 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['U', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1843 steps  16 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.2541309943021904 agent memory len 1205 steps  16 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  16 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['D', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1844 steps  17 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1206 steps  17 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  17 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1845 steps  18 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1207 steps  18 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  18 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1846 steps  19 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1208 steps  19 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  19 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1847 steps  20 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1209 steps  20 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  20 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1848 steps  21 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1210 steps  21 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  21 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1849 steps  22 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1211 steps  22 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  22 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1850 steps  23 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1212 steps  23 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  23 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1851 steps  24 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1213 steps  24 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  24 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1852 steps  25 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1214 steps  25 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  25 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'S', 'S']
agent epsilon  0.2541309943021904 agent memory len 1853 steps  26 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1215 steps  26 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  26 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1854 steps  27 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1216 steps  27 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  27 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1855 steps  28 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1217 steps  28 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  28 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1856 steps  29 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1218 steps  29 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  29 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1857 steps  30 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1219 steps  30 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  30 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1858 steps  31 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1220 steps  31 reward -1 next state  1 agent position  (4, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  31 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1859 steps  32 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1221 steps  32 reward -1 next state  1 agent position  (5, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  32 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1860 steps  33 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1222 steps  33 reward 0 next state  1 agent position  (6, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  33 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1861 steps  34 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.2541309943021904 agent memory len 1223 steps  34 reward 0 next state  0 agent position  (7, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  34 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['R', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1862 steps  35 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.2541309943021904 agent memory len 1224 steps  35 reward 0 next state  1 agent position  (6, 5)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  35 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
random action 1
actions ['D', 'L', 'S']
agent epsilon  0.2541309943021904 agent memory len 1863 steps  36 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.2541309943021904 agent memory len 1225 steps  36 reward 0 next state  1 agent position  (6, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  36 reward -1 next state  6 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1864 steps  37 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.2541309943021904 agent memory len 1226 steps  37 reward -1 next state  2 agent position  (5, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  37 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 1
actions ['R', 'L', 'S']
agent epsilon  0.2541309943021904 agent memory len 1865 steps  38 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.2541309943021904 agent memory len 1227 steps  38 reward -1 next state  3 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  38 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
random action 0
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1866 steps  39 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.2541309943021904 agent memory len 1228 steps  39 reward -1 next state  3 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  39 reward -1 next state  4 agent position  (8, 8)
 is_terminal [False, False, True]
actions ['R', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1867 steps  40 reward 0 next state  6 agent position  (6, 4)
agent epsilon  0.2541309943021904 agent memory len 1229 steps  40 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  40 reward -1 next state  5 agent position  (8, 8)
 is_terminal [False, False, True]
landmark captured 1
agent reached landmark-------------------------------- 0
actions ['D', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  41 reward 10 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1230 steps  41 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  41 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  42 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1231 steps  42 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  42 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  43 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1232 steps  43 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  43 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  44 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1233 steps  44 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  44 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  45 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1234 steps  45 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  45 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  46 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1235 steps  46 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  46 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  47 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1236 steps  47 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  47 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  48 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1237 steps  48 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  48 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  49 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1238 steps  49 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  49 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  50 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1239 steps  50 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  50 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  51 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1240 steps  51 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  51 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  52 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1241 steps  52 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  52 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  53 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1242 steps  53 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  53 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  54 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1243 steps  54 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  54 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  55 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1244 steps  55 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  55 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  56 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1245 steps  56 reward -1 next state  4 agent position  (5, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  56 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'L', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  57 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1246 steps  57 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  57 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  58 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1247 steps  58 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  58 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1248 steps  59 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  59 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  60 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1249 steps  60 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  60 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  61 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1250 steps  61 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  61 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  62 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1251 steps  62 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  62 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  63 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1252 steps  63 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  63 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  64 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1253 steps  64 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  64 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  65 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1254 steps  65 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  65 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1255 steps  66 reward -1 next state  4 agent position  (6, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  66 reward -1 next state  6 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1256 steps  67 reward -1 next state  4 agent position  (5, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  67 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  68 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1257 steps  68 reward -1 next state  4 agent position  (4, 2)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  68 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  69 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1258 steps  69 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  69 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1259 steps  70 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  70 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  71 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1260 steps  71 reward -1 next state  4 agent position  (5, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  71 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  72 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1261 steps  72 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  72 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1262 steps  73 reward -1 next state  4 agent position  (5, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  73 reward -1 next state  5 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'U', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  74 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1263 steps  74 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  74 reward -1 next state  4 agent position  (8, 8)
 is_terminal [True, False, True]
actions ['S', 'D', 'S']
agent epsilon  0.2541309943021904 agent memory len 1868 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.2541309943021904 agent memory len 1264 steps  75 reward -1 next state  4 agent position  (5, 4)
agent epsilon  0.2541309943021904 agent memory len 1600 steps  75 reward -1 next state  5 agent position  (8, 8)
max steps reached
total rewards -192
epsilon  0.24222458521285967
epsilon  0.24222458521285967
epsilon  0.24222458521285967
Episode number:  30
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'D']
agent epsilon  0.24222458521285967 agent memory len 1869 steps  1 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.24222458521285967 agent memory len 1265 steps  1 reward -2 next state  0 agent position  (0, 5)
agent epsilon  0.24222458521285967 agent memory len 1601 steps  1 reward -1 next state  0 agent position  (1, 9)
 is_terminal [False, False, False]
random action 0
actions ['D', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1870 steps  2 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1266 steps  2 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.24222458521285967 agent memory len 1602 steps  2 reward -1 next state  1 agent position  (2, 9)
 is_terminal [False, False, False]
random action 2
actions ['S', 'D', 'U']
agent epsilon  0.24222458521285967 agent memory len 1871 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1267 steps  3 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.24222458521285967 agent memory len 1603 steps  3 reward -1 next state  2 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1872 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1268 steps  4 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.24222458521285967 agent memory len 1604 steps  4 reward -1 next state  3 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1873 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1269 steps  5 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1605 steps  5 reward -1 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1874 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.24222458521285967 agent memory len 1270 steps  6 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.24222458521285967 agent memory len 1606 steps  6 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1875 steps  7 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.24222458521285967 agent memory len 1271 steps  7 reward 0 next state  0 agent position  (6, 5)
agent epsilon  0.24222458521285967 agent memory len 1607 steps  7 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.24222458521285967 agent memory len 1876 steps  8 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.24222458521285967 agent memory len 1272 steps  8 reward 0 next state  0 agent position  (7, 5)
agent epsilon  0.24222458521285967 agent memory len 1608 steps  8 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'L']
agent epsilon  0.24222458521285967 agent memory len 1877 steps  9 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.24222458521285967 agent memory len 1273 steps  9 reward 0 next state  0 agent position  (8, 5)
agent epsilon  0.24222458521285967 agent memory len 1609 steps  9 reward -1 next state  8 agent position  (6, 8)
 is_terminal [False, False, False]
random action 0
actions ['R', 'L', 'L']
agent epsilon  0.24222458521285967 agent memory len 1878 steps  10 reward -1 next state  3 agent position  (3, 1)
agent epsilon  0.24222458521285967 agent memory len 1274 steps  10 reward 0 next state  1 agent position  (8, 4)
agent epsilon  0.24222458521285967 agent memory len 1610 steps  10 reward -1 next state  8 agent position  (6, 7)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'L']
agent epsilon  0.24222458521285967 agent memory len 1879 steps  11 reward -1 next state  4 agent position  (4, 1)
agent epsilon  0.24222458521285967 agent memory len 1275 steps  11 reward -1 next state  1 agent position  (9, 4)
agent epsilon  0.24222458521285967 agent memory len 1611 steps  11 reward -1 next state  9 agent position  (6, 6)
 is_terminal [False, False, False]
random action 1
actions ['D', 'S', 'L']
agent epsilon  0.24222458521285967 agent memory len 1880 steps  12 reward -1 next state  5 agent position  (5, 1)
agent epsilon  0.24222458521285967 agent memory len 1276 steps  12 reward -1 next state  1 agent position  (9, 4)
agent epsilon  0.24222458521285967 agent memory len 1612 steps  12 reward 0 next state  9 agent position  (6, 5)
 is_terminal [False, False, False]
random action 2
actions ['R', 'L', 'S']
agent epsilon  0.24222458521285967 agent memory len 1881 steps  13 reward -1 next state  5 agent position  (5, 2)
agent epsilon  0.24222458521285967 agent memory len 1277 steps  13 reward 0 next state  2 agent position  (9, 3)
agent epsilon  0.24222458521285967 agent memory len 1613 steps  13 reward 0 next state  9 agent position  (6, 5)
 is_terminal [False, False, False]
random action 1
actions ['R', 'U', 'D']
agent epsilon  0.24222458521285967 agent memory len 1882 steps  14 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.24222458521285967 agent memory len 1278 steps  14 reward 0 next state  3 agent position  (8, 3)
agent epsilon  0.24222458521285967 agent memory len 1614 steps  14 reward 0 next state  8 agent position  (7, 5)
 is_terminal [False, False, False]
random action 1
actions ['U', 'U', 'S']
agent epsilon  0.24222458521285967 agent memory len 1883 steps  15 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.24222458521285967 agent memory len 1279 steps  15 reward 0 next state  3 agent position  (7, 3)
agent epsilon  0.24222458521285967 agent memory len 1615 steps  15 reward 0 next state  7 agent position  (7, 5)
 is_terminal [False, False, False]
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['U', 'R', 'R']
agent epsilon  0.24222458521285967 agent memory len 1884 steps  16 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  16 reward 10 next state  3 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1616 steps  16 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1885 steps  17 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  17 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1617 steps  17 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1886 steps  18 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  18 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1618 steps  18 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1887 steps  19 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  19 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1619 steps  19 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1888 steps  20 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  20 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1620 steps  20 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['U', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1889 steps  21 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  21 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1621 steps  21 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1890 steps  22 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  22 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1622 steps  22 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['U', 'S', 'R']
agent epsilon  0.24222458521285967 agent memory len 1891 steps  23 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  23 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1623 steps  23 reward 0 next state  7 agent position  (7, 7)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'L']
agent epsilon  0.24222458521285967 agent memory len 1892 steps  24 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  24 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1624 steps  24 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1893 steps  25 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  25 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1625 steps  25 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1894 steps  26 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  26 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1626 steps  26 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['L', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1895 steps  27 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  27 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1627 steps  27 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['D', 'S', 'U']
agent epsilon  0.24222458521285967 agent memory len 1896 steps  28 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  28 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1628 steps  28 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, True, False]
actions ['U', 'S', 'D']
agent epsilon  0.24222458521285967 agent memory len 1897 steps  29 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  29 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1629 steps  29 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1898 steps  30 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  30 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1630 steps  30 reward -1 next state  7 agent position  (7, 6)
 is_terminal [False, True, False]
random action 2
actions ['U', 'S', 'D']
agent epsilon  0.24222458521285967 agent memory len 1899 steps  31 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  31 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1631 steps  31 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
random action 0
random action 2
actions ['R', 'S', 'L']
agent epsilon  0.24222458521285967 agent memory len 1900 steps  32 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  32 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1632 steps  32 reward -1 next state  7 agent position  (8, 5)
 is_terminal [False, True, False]
actions ['U', 'S', 'R']
agent epsilon  0.24222458521285967 agent memory len 1901 steps  33 reward -1 next state  2 agent position  (2, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  33 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1633 steps  33 reward -1 next state  7 agent position  (8, 6)
 is_terminal [False, True, False]
actions ['D', 'S', 'R']
agent epsilon  0.24222458521285967 agent memory len 1902 steps  34 reward -1 next state  3 agent position  (3, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  34 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1634 steps  34 reward 0 next state  7 agent position  (8, 7)
 is_terminal [False, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['D', 'S', 'R']
agent epsilon  0.24222458521285967 agent memory len 1903 steps  35 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  35 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  35 reward 10 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1904 steps  36 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  36 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  36 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1905 steps  37 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  37 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  37 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1906 steps  38 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  38 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  38 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1907 steps  39 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  39 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  39 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1908 steps  40 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  40 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  40 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1909 steps  41 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  41 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  41 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1910 steps  42 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  42 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1911 steps  43 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  43 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1912 steps  44 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  44 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1913 steps  45 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  45 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1914 steps  46 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  46 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1915 steps  47 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  47 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  47 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1916 steps  48 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  48 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  48 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1917 steps  49 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  49 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  49 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1918 steps  50 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  50 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  50 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1919 steps  51 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  51 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1920 steps  52 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  52 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  52 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1921 steps  53 reward -1 next state  4 agent position  (4, 5)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  53 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  53 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1922 steps  54 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  54 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  54 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1923 steps  55 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  55 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  55 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1924 steps  56 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  56 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  56 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1925 steps  57 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  57 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  57 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1926 steps  58 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  58 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1927 steps  59 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  59 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1928 steps  60 reward -1 next state  5 agent position  (5, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  60 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1929 steps  61 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  61 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1930 steps  62 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  62 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1931 steps  63 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  63 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1932 steps  64 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  64 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1933 steps  65 reward -1 next state  4 agent position  (4, 6)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  65 reward -1 next state  6 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1934 steps  66 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  66 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1935 steps  67 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  67 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1936 steps  68 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  68 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1937 steps  69 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  69 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1938 steps  70 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  70 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1939 steps  71 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  71 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1940 steps  72 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  72 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1941 steps  73 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  73 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1942 steps  74 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  74 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.24222458521285967 agent memory len 1943 steps  75 reward -1 next state  4 agent position  (4, 7)
agent epsilon  0.24222458521285967 agent memory len 1280 steps  75 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.24222458521285967 agent memory len 1635 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -188
epsilon  0.23089885854694553
epsilon  0.23089885854694553
epsilon  0.23089885854694553
Episode number:  31
 is_terminal [False, False, False]
random action 0
actions ['R', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1944 steps  1 reward -1 next state  0 agent position  (0, 1)
agent epsilon  0.23089885854694553 agent memory len 1281 steps  1 reward -1 next state  1 agent position  (1, 5)
agent epsilon  0.23089885854694553 agent memory len 1636 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1945 steps  2 reward -1 next state  1 agent position  (1, 1)
agent epsilon  0.23089885854694553 agent memory len 1282 steps  2 reward -1 next state  1 agent position  (2, 5)
agent epsilon  0.23089885854694553 agent memory len 1637 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1946 steps  3 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.23089885854694553 agent memory len 1283 steps  3 reward -1 next state  1 agent position  (3, 5)
agent epsilon  0.23089885854694553 agent memory len 1638 steps  3 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 2
actions ['R', 'D', 'R']
agent epsilon  0.23089885854694553 agent memory len 1947 steps  4 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.23089885854694553 agent memory len 1284 steps  4 reward -1 next state  2 agent position  (4, 5)
agent epsilon  0.23089885854694553 agent memory len 1639 steps  4 reward -2 next state  4 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1948 steps  5 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.23089885854694553 agent memory len 1285 steps  5 reward -1 next state  2 agent position  (5, 5)
agent epsilon  0.23089885854694553 agent memory len 1640 steps  5 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['R', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1949 steps  6 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.23089885854694553 agent memory len 1286 steps  6 reward 0 next state  3 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1641 steps  6 reward -1 next state  6 agent position  (5, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.23089885854694553 agent memory len 1950 steps  7 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.23089885854694553 agent memory len 1287 steps  7 reward 0 next state  3 agent position  (7, 5)
agent epsilon  0.23089885854694553 agent memory len 1642 steps  7 reward -1 next state  7 agent position  (6, 9)
 is_terminal [False, False, False]
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['S', 'L', 'D']
agent epsilon  0.23089885854694553 agent memory len 1951 steps  8 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  8 reward 10 next state  3 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1643 steps  8 reward 0 next state  7 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'L']
agent epsilon  0.23089885854694553 agent memory len 1952 steps  9 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  9 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1644 steps  9 reward 0 next state  7 agent position  (7, 8)
 is_terminal [False, True, False]
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['R', 'S', 'D']
agent epsilon  0.23089885854694553 agent memory len 1953 steps  10 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  10 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  10 reward 10 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1954 steps  11 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  11 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  11 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1955 steps  12 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  12 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  12 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1956 steps  13 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  13 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  13 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1957 steps  14 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  14 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  14 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1958 steps  15 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  15 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  15 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1959 steps  16 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  16 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  16 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1960 steps  17 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  17 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  17 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1961 steps  18 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  18 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  18 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1962 steps  19 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  19 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  19 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1963 steps  20 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  20 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  20 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1964 steps  21 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  21 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  21 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1965 steps  22 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  22 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  22 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1966 steps  23 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  23 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  23 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1967 steps  24 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  24 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  24 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1968 steps  25 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  25 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  25 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1969 steps  26 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  26 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  26 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1970 steps  27 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  27 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  27 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1971 steps  28 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  28 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  28 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1972 steps  29 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  29 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  29 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1973 steps  30 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  30 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  30 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1974 steps  31 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  31 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  31 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1975 steps  32 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  32 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  32 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1976 steps  33 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  33 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  33 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1977 steps  34 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  34 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  34 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1978 steps  35 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  35 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  35 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1979 steps  36 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  36 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  36 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1980 steps  37 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  37 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  37 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1981 steps  38 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  38 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  38 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1982 steps  39 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  39 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  39 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1983 steps  40 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  40 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  40 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1984 steps  41 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  41 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  41 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1985 steps  42 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  42 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1986 steps  43 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  43 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1987 steps  44 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  44 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1988 steps  45 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  45 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1989 steps  46 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  46 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1990 steps  47 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  47 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  47 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1991 steps  48 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  48 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  48 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1992 steps  49 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  49 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  49 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1993 steps  50 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  50 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  50 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1994 steps  51 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  51 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1995 steps  52 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  52 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  52 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1996 steps  53 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  53 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  53 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1997 steps  54 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  54 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  54 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1998 steps  55 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  55 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  55 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 1999 steps  56 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  56 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  56 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2000 steps  57 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  57 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  57 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2001 steps  58 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  58 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2002 steps  59 reward -1 next state  7 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  59 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2003 steps  60 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  60 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2004 steps  61 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  61 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2005 steps  62 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  62 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2006 steps  63 reward -1 next state  8 agent position  (8, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  63 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2007 steps  64 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  64 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2008 steps  65 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  65 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2009 steps  66 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  66 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2010 steps  67 reward -1 next state  9 agent position  (9, 4)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  67 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2011 steps  68 reward -1 next state  9 agent position  (9, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  68 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2012 steps  69 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  69 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2013 steps  70 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  70 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2014 steps  71 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  71 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2015 steps  72 reward -1 next state  7 agent position  (7, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  72 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2016 steps  73 reward -1 next state  7 agent position  (7, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  73 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2017 steps  74 reward -1 next state  7 agent position  (7, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  74 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.23089885854694553 agent memory len 2018 steps  75 reward -1 next state  8 agent position  (8, 5)
agent epsilon  0.23089885854694553 agent memory len 1288 steps  75 reward -1 next state  5 agent position  (7, 4)
agent epsilon  0.23089885854694553 agent memory len 1645 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -197
epsilon  0.22012549408847562
epsilon  0.22012549408847562
epsilon  0.22012549408847562
Episode number:  32
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'D']
agent epsilon  0.22012549408847562 agent memory len 2019 steps  1 reward -2 next state  0 agent position  (0, 0)
agent epsilon  0.22012549408847562 agent memory len 1289 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.22012549408847562 agent memory len 1646 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.22012549408847562 agent memory len 2020 steps  2 reward -1 next state  0 agent position  (0, 0)
agent epsilon  0.22012549408847562 agent memory len 1290 steps  2 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.22012549408847562 agent memory len 1647 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.22012549408847562 agent memory len 2021 steps  3 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1291 steps  3 reward -1 next state  0 agent position  (3, 5)
agent epsilon  0.22012549408847562 agent memory len 1648 steps  3 reward -1 next state  3 agent position  (3, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'D']
agent epsilon  0.22012549408847562 agent memory len 2022 steps  4 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1292 steps  4 reward -1 next state  0 agent position  (4, 5)
agent epsilon  0.22012549408847562 agent memory len 1649 steps  4 reward -1 next state  4 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 2023 steps  5 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1293 steps  5 reward -1 next state  0 agent position  (5, 5)
agent epsilon  0.22012549408847562 agent memory len 1650 steps  5 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
random action 1
actions ['S', 'R', 'S']
agent epsilon  0.22012549408847562 agent memory len 2024 steps  6 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1294 steps  6 reward -1 next state  0 agent position  (5, 6)
agent epsilon  0.22012549408847562 agent memory len 1651 steps  6 reward -1 next state  5 agent position  (4, 9)
 is_terminal [False, False, False]
random action 0
actions ['L', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 2025 steps  7 reward -2 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1295 steps  7 reward -1 next state  0 agent position  (6, 6)
agent epsilon  0.22012549408847562 agent memory len 1652 steps  7 reward -1 next state  6 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 2026 steps  8 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1296 steps  8 reward -1 next state  0 agent position  (7, 6)
agent epsilon  0.22012549408847562 agent memory len 1653 steps  8 reward -1 next state  7 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['S', 'D', 'D']
agent epsilon  0.22012549408847562 agent memory len 2027 steps  9 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.22012549408847562 agent memory len 1297 steps  9 reward -1 next state  0 agent position  (8, 6)
agent epsilon  0.22012549408847562 agent memory len 1654 steps  9 reward -1 next state  8 agent position  (5, 9)
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'S']
agent epsilon  0.22012549408847562 agent memory len 2028 steps  10 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.22012549408847562 agent memory len 1298 steps  10 reward -1 next state  0 agent position  (9, 6)
agent epsilon  0.22012549408847562 agent memory len 1655 steps  10 reward -1 next state  9 agent position  (5, 9)
 is_terminal [False, False, False]
random action 2
actions ['S', 'L', 'S']
agent epsilon  0.22012549408847562 agent memory len 2029 steps  11 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.22012549408847562 agent memory len 1299 steps  11 reward -1 next state  0 agent position  (9, 5)
agent epsilon  0.22012549408847562 agent memory len 1656 steps  11 reward -1 next state  9 agent position  (5, 9)
 is_terminal [False, False, False]
random action 1
actions ['S', 'U', 'D']
agent epsilon  0.22012549408847562 agent memory len 2030 steps  12 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.22012549408847562 agent memory len 1300 steps  12 reward 0 next state  0 agent position  (8, 5)
agent epsilon  0.22012549408847562 agent memory len 1657 steps  12 reward -1 next state  8 agent position  (6, 9)
 is_terminal [False, False, False]
random action 0
actions ['S', 'L', 'D']
agent epsilon  0.22012549408847562 agent memory len 2031 steps  13 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.22012549408847562 agent memory len 1301 steps  13 reward 0 next state  0 agent position  (8, 4)
agent epsilon  0.22012549408847562 agent memory len 1658 steps  13 reward 0 next state  8 agent position  (7, 9)
 is_terminal [False, False, False]
random action 1
random action 2
landmark captured 1
agent reached landmark-------------------------------- 1
actions ['R', 'U', 'S']
agent epsilon  0.22012549408847562 agent memory len 2032 steps  14 reward -1 next state  2 agent position  (2, 1)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  14 reward 10 next state  1 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1659 steps  14 reward 0 next state  7 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2033 steps  15 reward -1 next state  2 agent position  (2, 2)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  15 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1660 steps  15 reward 0 next state  7 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2034 steps  16 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  16 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1661 steps  16 reward 0 next state  7 agent position  (7, 9)
 is_terminal [False, True, False]
actions ['R', 'S', 'D']
agent epsilon  0.22012549408847562 agent memory len 2035 steps  17 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  17 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1662 steps  17 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2036 steps  18 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  18 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1663 steps  18 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2037 steps  19 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  19 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1664 steps  19 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2038 steps  20 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  20 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1665 steps  20 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['D', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2039 steps  21 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  21 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1666 steps  21 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2040 steps  22 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  22 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1667 steps  22 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2041 steps  23 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  23 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1668 steps  23 reward 0 next state  7 agent position  (8, 9)
 is_terminal [False, True, False]
random action 2
landmark captured 2
agent reached landmark-------------------------------- 2
actions ['S', 'S', 'L']
agent epsilon  0.22012549408847562 agent memory len 2042 steps  24 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  24 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  24 reward 10 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2043 steps  25 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  25 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  25 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2044 steps  26 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  26 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  26 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2045 steps  27 reward -1 next state  3 agent position  (3, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  27 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  27 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2046 steps  28 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  28 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  28 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2047 steps  29 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  29 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  29 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2048 steps  30 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  30 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  30 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2049 steps  31 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  31 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  31 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2050 steps  32 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  32 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  32 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2051 steps  33 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  33 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  33 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2052 steps  34 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  34 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  34 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2053 steps  35 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  35 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  35 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2054 steps  36 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  36 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  36 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2055 steps  37 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  37 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  37 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2056 steps  38 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  38 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  38 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2057 steps  39 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  39 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  39 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2058 steps  40 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  40 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  40 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2059 steps  41 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  41 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  41 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2060 steps  42 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  42 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  42 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2061 steps  43 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  43 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  43 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2062 steps  44 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  44 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  44 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2063 steps  45 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  45 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  45 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2064 steps  46 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  46 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  46 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2065 steps  47 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  47 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  47 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2066 steps  48 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  48 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  48 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2067 steps  49 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  49 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  49 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2068 steps  50 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  50 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  50 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2069 steps  51 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  51 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  51 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2070 steps  52 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  52 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  52 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2071 steps  53 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  53 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  53 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2072 steps  54 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  54 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  54 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2073 steps  55 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  55 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  55 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2074 steps  56 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  56 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  56 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2075 steps  57 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  57 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  57 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2076 steps  58 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  58 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  58 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2077 steps  59 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  59 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  59 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2078 steps  60 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  60 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  60 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2079 steps  61 reward -1 next state  4 agent position  (4, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  61 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  61 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2080 steps  62 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  62 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  62 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2081 steps  63 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  63 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  63 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2082 steps  64 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  64 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  64 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2083 steps  65 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  65 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  65 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2084 steps  66 reward -1 next state  3 agent position  (3, 2)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  66 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  66 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2085 steps  67 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  67 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  67 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2086 steps  68 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  68 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  68 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2087 steps  69 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  69 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  69 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2088 steps  70 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  70 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  70 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2089 steps  71 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  71 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  71 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2090 steps  72 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  72 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  72 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2091 steps  73 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  73 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  73 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
actions ['S', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2092 steps  74 reward -1 next state  2 agent position  (2, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  74 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  74 reward -1 next state  7 agent position  (8, 8)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.22012549408847562 agent memory len 2093 steps  75 reward -1 next state  3 agent position  (3, 3)
agent epsilon  0.22012549408847562 agent memory len 1302 steps  75 reward -1 next state  3 agent position  (7, 4)
agent epsilon  0.22012549408847562 agent memory len 1669 steps  75 reward -1 next state  7 agent position  (8, 8)
max steps reached
total rewards -189
epsilon  0.20987755281470882
epsilon  0.20987755281470882
epsilon  0.20987755281470882
Episode number:  33
 is_terminal [False, False, False]
random action 2
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 2094 steps  1 reward -1 next state  1 agent position  (1, 0)
agent epsilon  0.20987755281470882 agent memory len 1303 steps  1 reward -1 next state  0 agent position  (1, 5)
agent epsilon  0.20987755281470882 agent memory len 1670 steps  1 reward -1 next state  1 agent position  (1, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 2095 steps  2 reward -1 next state  2 agent position  (2, 0)
agent epsilon  0.20987755281470882 agent memory len 1304 steps  2 reward -1 next state  0 agent position  (2, 5)
agent epsilon  0.20987755281470882 agent memory len 1671 steps  2 reward -1 next state  2 agent position  (2, 9)
 is_terminal [False, False, False]
random action 1
actions ['D', 'R', 'D']
agent epsilon  0.20987755281470882 agent memory len 2096 steps  3 reward -1 next state  3 agent position  (3, 0)
agent epsilon  0.20987755281470882 agent memory len 1305 steps  3 reward -1 next state  0 agent position  (2, 6)
agent epsilon  0.20987755281470882 agent memory len 1672 steps  3 reward -1 next state  2 agent position  (3, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 2097 steps  4 reward -1 next state  4 agent position  (4, 0)
agent epsilon  0.20987755281470882 agent memory len 1306 steps  4 reward -1 next state  0 agent position  (3, 6)
agent epsilon  0.20987755281470882 agent memory len 1673 steps  4 reward -1 next state  3 agent position  (4, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 2098 steps  5 reward -1 next state  5 agent position  (5, 0)
agent epsilon  0.20987755281470882 agent memory len 1307 steps  5 reward -1 next state  0 agent position  (4, 6)
agent epsilon  0.20987755281470882 agent memory len 1674 steps  5 reward -1 next state  4 agent position  (5, 9)
 is_terminal [False, False, False]
actions ['D', 'D', 'D']
agent epsilon  0.20987755281470882 agent memory len 2099 steps  6 reward -1 next state  6 agent position  (6, 0)
agent epsilon  0.20987755281470882 agent memory len 1308 steps  6 reward -1 next state  0 agent position  (5, 6)
agent epsilon  0.20987755281470882 agent memory len 1675 steps  6 reward -1 next state  5 agent position  (6, 9)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'L']
agent epsilon  0.20987755281470882 agent memory len 2100 steps  7 reward -1 next state  6 agent position  (6, 1)
agent epsilon  0.20987755281470882 agent memory len 1309 steps  7 reward -1 next state  1 agent position  (6, 6)
agent epsilon  0.20987755281470882 agent memory len 1676 steps  7 reward -1 next state  6 agent position  (6, 8)
 is_terminal [False, False, False]
actions ['R', 'S', 'L']
agent epsilon  0.20987755281470882 agent memory len 2101 steps  8 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1310 steps  8 reward -1 next state  2 agent position  (6, 6)
agent epsilon  0.20987755281470882 agent memory len 1677 steps  8 reward -1 next state  6 agent position  (6, 7)
 is_terminal [False, False, False]
actions ['R', 'D', 'L']
agent epsilon  0.20987755281470882 agent memory len 2102 steps  9 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1311 steps  9 reward -1 next state  3 agent position  (7, 6)
agent epsilon  0.20987755281470882 agent memory len 1678 steps  9 reward -1 next state  7 agent position  (6, 6)
 is_terminal [False, False, False]
actions ['L', 'L', 'L']
agent epsilon  0.20987755281470882 agent memory len 2103 steps  10 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1312 steps  10 reward 0 next state  2 agent position  (7, 5)
agent epsilon  0.20987755281470882 agent memory len 1679 steps  10 reward 0 next state  7 agent position  (6, 5)
 is_terminal [False, False, False]
random action 0
actions ['S', 'D', 'L']
agent epsilon  0.20987755281470882 agent memory len 2104 steps  11 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1313 steps  11 reward 0 next state  2 agent position  (8, 5)
agent epsilon  0.20987755281470882 agent memory len 1680 steps  11 reward 0 next state  8 agent position  (6, 4)
 is_terminal [False, False, False]
random action 1
actions ['R', 'D', 'L']
agent epsilon  0.20987755281470882 agent memory len 2105 steps  12 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1314 steps  12 reward -1 next state  3 agent position  (9, 5)
agent epsilon  0.20987755281470882 agent memory len 1681 steps  12 reward 0 next state  9 agent position  (6, 3)
 is_terminal [False, False, False]
random action 1
actions ['L', 'S', 'R']
agent epsilon  0.20987755281470882 agent memory len 2106 steps  13 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1315 steps  13 reward -1 next state  2 agent position  (9, 5)
agent epsilon  0.20987755281470882 agent memory len 1682 steps  13 reward 0 next state  9 agent position  (6, 4)
 is_terminal [False, False, False]
actions ['R', 'L', 'L']
agent epsilon  0.20987755281470882 agent memory len 2107 steps  14 reward 0 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1316 steps  14 reward -1 next state  3 agent position  (9, 4)
agent epsilon  0.20987755281470882 agent memory len 1683 steps  14 reward 0 next state  9 agent position  (6, 3)
 is_terminal [False, False, False]
random action 1
random action 2
actions ['L', 'U', 'D']
agent epsilon  0.20987755281470882 agent memory len 2108 steps  15 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1317 steps  15 reward 0 next state  2 agent position  (8, 4)
agent epsilon  0.20987755281470882 agent memory len 1684 steps  15 reward 0 next state  8 agent position  (7, 3)
 is_terminal [False, False, False]
random action 0
landmark captured 1
agent reached landmark-------------------------------- 2
actions ['S', 'L', 'R']
agent epsilon  0.20987755281470882 agent memory len 2109 steps  16 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1318 steps  16 reward 0 next state  2 agent position  (8, 3)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  16 reward 10 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'U', 'S']
agent epsilon  0.20987755281470882 agent memory len 2110 steps  17 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1319 steps  17 reward -1 next state  3 agent position  (7, 3)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  17 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.20987755281470882 agent memory len 2111 steps  18 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.20987755281470882 agent memory len 1320 steps  18 reward -1 next state  4 agent position  (7, 4)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  18 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
random action 0
actions ['R', 'R', 'S']
agent epsilon  0.20987755281470882 agent memory len 2112 steps  19 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.20987755281470882 agent memory len 1321 steps  19 reward -1 next state  5 agent position  (7, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  19 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2113 steps  20 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.20987755281470882 agent memory len 1322 steps  20 reward -1 next state  4 agent position  (7, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  20 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2114 steps  21 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1323 steps  21 reward -1 next state  3 agent position  (7, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  21 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'L', 'S']
agent epsilon  0.20987755281470882 agent memory len 2115 steps  22 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1324 steps  22 reward -1 next state  2 agent position  (7, 4)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  22 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'R', 'S']
agent epsilon  0.20987755281470882 agent memory len 2116 steps  23 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1325 steps  23 reward -1 next state  3 agent position  (7, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  23 reward -1 next state  7 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['L', 'D', 'S']
agent epsilon  0.20987755281470882 agent memory len 2117 steps  24 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1326 steps  24 reward -1 next state  2 agent position  (8, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  24 reward -1 next state  8 agent position  (7, 4)
 is_terminal [False, False, True]
random action 1
actions ['R', 'D', 'S']
agent epsilon  0.20987755281470882 agent memory len 2118 steps  25 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1327 steps  25 reward -1 next state  3 agent position  (9, 5)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  25 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['L', 'L', 'S']
agent epsilon  0.20987755281470882 agent memory len 2119 steps  26 reward -1 next state  6 agent position  (6, 2)
agent epsilon  0.20987755281470882 agent memory len 1328 steps  26 reward -1 next state  2 agent position  (9, 4)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  26 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
actions ['R', 'L', 'S']
agent epsilon  0.20987755281470882 agent memory len 2120 steps  27 reward -1 next state  6 agent position  (6, 3)
agent epsilon  0.20987755281470882 agent memory len 1329 steps  27 reward 0 next state  3 agent position  (9, 3)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  27 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, False, True]
landmark captured 0
agent reached landmark-------------------------------- 1
actions ['R', 'L', 'S']
agent epsilon  0.20987755281470882 agent memory len 2121 steps  28 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  28 reward 10 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  28 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['U', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2122 steps  29 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  29 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  29 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2123 steps  30 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  30 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  30 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2124 steps  31 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  31 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  31 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2125 steps  32 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  32 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  32 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2126 steps  33 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  33 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  33 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2127 steps  34 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  34 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  34 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2128 steps  35 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  35 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  35 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2129 steps  36 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  36 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  36 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2130 steps  37 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  37 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  37 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2131 steps  38 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  38 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  38 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2132 steps  39 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  39 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  39 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2133 steps  40 reward -1 next state  4 agent position  (4, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  40 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  40 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['D', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2134 steps  41 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  41 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  41 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2135 steps  42 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  42 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  42 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2136 steps  43 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  43 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  43 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2137 steps  44 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  44 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  44 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2138 steps  45 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  45 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  45 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2139 steps  46 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  46 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  46 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2140 steps  47 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  47 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  47 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2141 steps  48 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  48 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  48 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2142 steps  49 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  49 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  49 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2143 steps  50 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  50 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  50 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2144 steps  51 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  51 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  51 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2145 steps  52 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  52 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  52 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['S', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2146 steps  53 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  53 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  53 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2147 steps  54 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  54 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  54 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2148 steps  55 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  55 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  55 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2149 steps  56 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  56 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  56 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2150 steps  57 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  57 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  57 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2151 steps  58 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  58 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  58 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2152 steps  59 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  59 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  59 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['D', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2153 steps  60 reward -1 next state  6 agent position  (6, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  60 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  60 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2154 steps  61 reward -1 next state  6 agent position  (6, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  61 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  61 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['U', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2155 steps  62 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  62 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  62 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2156 steps  63 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  63 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  63 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2157 steps  64 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  64 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  64 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2158 steps  65 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  65 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  65 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2159 steps  66 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  66 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  66 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2160 steps  67 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  67 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  67 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2161 steps  68 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  68 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  68 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2162 steps  69 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  69 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  69 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2163 steps  70 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  70 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  70 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
random action 0
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2164 steps  71 reward -1 next state  5 agent position  (5, 3)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  71 reward -1 next state  3 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  71 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2165 steps  72 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  72 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  72 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2166 steps  73 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  73 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  73 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['L', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2167 steps  74 reward -1 next state  5 agent position  (5, 4)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  74 reward -1 next state  4 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  74 reward -1 next state  9 agent position  (7, 4)
 is_terminal [False, True, True]
actions ['R', 'S', 'S']
agent epsilon  0.20987755281470882 agent memory len 2168 steps  75 reward -1 next state  5 agent position  (5, 5)
agent epsilon  0.20987755281470882 agent memory len 1330 steps  75 reward -1 next state  5 agent position  (9, 2)
agent epsilon  0.20987755281470882 agent memory len 1685 steps  75 reward -1 next state  9 agent position  (7, 4)
max steps reached
total rewards -186
epsilon  0.20012940953454655
