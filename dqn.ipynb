{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import deque\n",
    "from config import *\n",
    "custom_map = [\n",
    "    'SFFFF',\n",
    "    'FFFFF',\n",
    "    'FFFFF',\n",
    "    'FFFFF',\n",
    "    'FFFFG'\n",
    "]\n",
    "\n",
    "s_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False)\n",
    "train_episodes=200\n",
    "test_episodes=100\n",
    "max_steps=300\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "batch_size=32\n",
    "print(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.learning_rate=0.001\n",
    "        self.epsilon=1\n",
    "        self.max_eps=1\n",
    "        self.min_eps=0.01\n",
    "        self.eps_decay = 0.001/3\n",
    "        self.gamma=0.9\n",
    "        self.state_size= state_size\n",
    "        self.action_size= action_size\n",
    "        self.epsilon_lst=[]\n",
    "        self.model = self.buildmodel()\n",
    "\n",
    "    def buildmodel(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "    def action(self, state, episode, test = False):\n",
    "        if np.random.rand() < self.epsilon and episode<=30 and not test:\n",
    "            print(\"random action\")\n",
    "            return np.random.randint(0,4)\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self, batch_size, episode):\n",
    "        minibatch=random.sample(self.memory, batch_size)\n",
    "        for new_state, reward, done, state, action in minibatch:\n",
    "            target= reward\n",
    "            if not done:\n",
    "                target=reward + self.gamma* np.amax(self.model.predict(new_state))\n",
    "            target_f= self.model.predict(state)\n",
    "            target_f[0][action]= target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * \\\n",
    "            np.exp(-DECAY_RATE*episode)\n",
    "        print(\"epsilon \", self.epsilon)\n",
    "        self.epsilon_lst.append(self.epsilon)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2=Sequential()\n",
    "# model2.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "# model2.add(Dense(32, activation='relu'))\n",
    "# model2.add(Dense(action_size, activation='linear'))\n",
    "# model2.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "# model2.summary()\n",
    "agent=Agent(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_arr=np.zeros(state_size)\n",
    "state_arr[23] = 1\n",
    "state= np.reshape(state_arr, [1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enemy_state_arr = np.zeros(state_size)\n",
    "enemy_state_arr[24] = 1\n",
    "enemy_state =  np.reshape(enemy_state_arr, [1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state is  3\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    0/200 and step:   28. Wins: 1/   0, reward 1.0\n",
      "state is  5\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    1/200 and step:   80. Wins: 2/   1, reward 1.0\n",
      "epsilon  0.9517171302557069\n",
      "state is  10\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    2/200 and step:   84. Wins: 3/   2, reward 1.0\n",
      "epsilon  0.9057890438555999\n",
      "state is  17\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    3/200 and step:   61. Wins: 4/   3, reward 1.0\n",
      "epsilon  0.8621008966608072\n",
      "state is  17\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    4/200 and step:   27. Wins: 5/   4, reward 1.0\n",
      "epsilon  0.820543445547202\n",
      "state is  11\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    5/200 and step:   20. Wins: 6/   5, reward 1.0\n",
      "epsilon  0.7810127752406908\n",
      "state is  17\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    6/200 and step:   27. Wins: 7/   6, reward 1.0\n",
      "epsilon  0.7434100384749007\n",
      "state is  11\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    7/200 and step:   20. Wins: 8/   7, reward 1.0\n",
      "epsilon  0.7076412088215263\n",
      "state is  17\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    8/200 and step:   44. Wins: 9/   8, reward 1.0\n",
      "epsilon  0.6736168455752829\n",
      "state is  12\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:    9/200 and step:   33. Wins: 10/   9, reward 1.0\n",
      "epsilon  0.6412518701055556\n",
      "state is  11\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   10/200 and step:    7. Wins: 11/  10, reward 1.0\n",
      "epsilon  0.6104653531155071\n",
      "state is  1\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   11/200 and step:   12. Wins: 12/  11, reward 1.0\n",
      "epsilon  0.5811803122766818\n",
      "state is  9\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   12/200 and step:   49. Wins: 13/  12, reward 1.0\n",
      "epsilon  0.5533235197330861\n",
      "state is  17\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   13/200 and step:   80. Wins: 14/  13, reward 1.0\n",
      "epsilon  0.5268253189934059\n",
      "state is  3\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   14/200 and step:   21. Wins: 15/  14, reward 1.0\n",
      "epsilon  0.5016194507534953\n",
      "state is  1\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   15/200 and step:   23. Wins: 16/  15, reward 1.0\n",
      "epsilon  0.47764288721360454\n",
      "state is  19\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   16/200 and step:   97. Wins: 17/  16, reward 1.0\n",
      "epsilon  0.45483567447604933\n",
      "state is  1\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   17/200 and step:   15. Wins: 18/  17, reward 1.0\n",
      "epsilon  0.4331407826292394\n",
      "state is  13\n",
      "random action\n",
      "random action\n",
      "Episode:   18/200 and step:    9. Wins: 19/  18, reward 1.0\n",
      "epsilon  0.4125039631431931\n",
      "state is  4\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   19/200 and step:   72. Wins: 20/  19, reward 1.0\n",
      "epsilon  0.3928736132199562\n",
      "state is  11\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   20/200 and step:   11. Wins: 21/  20, reward 1.0\n",
      "epsilon  0.3742006467597279\n",
      "state is  15\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   21/200 and step:   24. Wins: 22/  21, reward 1.0\n",
      "epsilon  0.35643837162004377\n",
      "state is  10\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   22/200 and step:   13. Wins: 23/  22, reward 1.0\n",
      "epsilon  0.3395423728610988\n",
      "state is  15\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   23/200 and step:   44. Wins: 24/  23, reward 1.0\n",
      "epsilon  0.32347040168526264\n",
      "state is  16\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   24/200 and step:    9. Wins: 25/  24, reward 1.0\n",
      "epsilon  0.30818226979308\n",
      "state is  12\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   25/200 and step:    9. Wins: 26/  25, reward 1.0\n",
      "epsilon  0.29363974889158817\n",
      "state is  14\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   26/200 and step:   45. Wins: 27/  26, reward 1.0\n",
      "epsilon  0.27980647510367246\n",
      "state is  0\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   27/200 and step:   12. Wins: 28/  27, reward 1.0\n",
      "epsilon  0.2666478580394326\n",
      "state is  5\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   28/200 and step:   21. Wins: 29/  28, reward 1.0\n",
      "epsilon  0.2541309943021904\n",
      "state is  5\n",
      "random action\n",
      "Episode:   29/200 and step:    9. Wins: 30/  29, reward 1.0\n",
      "epsilon  0.24222458521285967\n",
      "state is  3\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "Episode:   30/200 and step:   19. Wins: 31/  30, reward 1.0\n",
      "epsilon  0.23089885854694553\n",
      "state is  6\n",
      "Episode:   31/200 and step:   99. Wins: 32/  31, reward 0.0\n",
      "epsilon  0.22012549408847562\n",
      "state is  18\n",
      "Episode:   32/200 and step:    7. Wins: 33/  32, reward 1.0\n",
      "epsilon  0.20987755281470882\n",
      "state is  11\n",
      "Episode:   33/200 and step:    7. Wins: 34/  33, reward 1.0\n",
      "epsilon  0.20012940953454655\n",
      "state is  10\n",
      "Episode:   34/200 and step:    7. Wins: 35/  34, reward 1.0\n",
      "epsilon  0.19085668881220727\n",
      "state is  6\n",
      "Episode:   35/200 and step:    7. Wins: 36/  35, reward 1.0\n",
      "epsilon  0.1820362040159407\n",
      "state is  14\n",
      "Episode:   36/200 and step:    7. Wins: 37/  36, reward 1.0\n",
      "epsilon  0.17364589933937066\n",
      "state is  16\n",
      "Episode:   37/200 and step:    7. Wins: 38/  37, reward 1.0\n",
      "epsilon  0.16566479465049133\n",
      "state is  19\n",
      "Episode:   38/200 and step:   99. Wins: 39/  38, reward 0.0\n",
      "epsilon  0.1580729330304087\n",
      "state is  4\n",
      "Episode:   39/200 and step:    7. Wins: 40/  39, reward 1.0\n",
      "epsilon  0.15085133087064842\n",
      "state is  11\n",
      "Episode:   40/200 and step:    7. Wins: 41/  40, reward 1.0\n",
      "epsilon  0.1439819304042466\n",
      "state is  14\n",
      "Episode:   41/200 and step:    7. Wins: 42/  41, reward 1.0\n",
      "epsilon  0.13744755455192614\n",
      "state is  13\n",
      "Episode:   42/200 and step:   99. Wins: 43/  42, reward 0.0\n",
      "epsilon  0.13123186397045208\n",
      "state is  19\n",
      "Episode:   43/200 and step:   99. Wins: 44/  43, reward 0.0\n",
      "epsilon  0.125319316195762\n",
      "state is  16\n",
      "Episode:   44/200 and step:   99. Wins: 45/  44, reward 0.0\n",
      "epsilon  0.11969512677871053\n",
      "state is  10\n",
      "Episode:   45/200 and step:   99. Wins: 46/  45, reward 0.0\n",
      "epsilon  0.11434523231624569\n",
      "state is  9\n",
      "Episode:   46/200 and step:    7. Wins: 47/  46, reward 1.0\n",
      "epsilon  0.10925625528557566\n",
      "state is  18\n",
      "Episode:   47/200 and step:    7. Wins: 48/  47, reward 1.0\n",
      "epsilon  0.10441547059339411\n",
      "state is  6\n",
      "Episode:   48/200 and step:    7. Wins: 49/  48, reward 1.0\n",
      "epsilon  0.09981077375651834\n",
      "state is  4\n",
      "Episode:   49/200 and step:    8. Wins: 50/  49, reward 1.0\n",
      "epsilon  0.09543065063437678\n",
      "state is  8\n",
      "Episode:   50/200 and step:    7. Wins: 51/  50, reward 1.0\n",
      "epsilon  0.0912641486376598\n",
      "state is  1\n",
      "Episode:   51/200 and step:    7. Wins: 52/  51, reward 1.0\n",
      "epsilon  0.08730084934114159\n",
      "state is  3\n",
      "Episode:   52/200 and step:    7. Wins: 53/  52, reward 1.0\n",
      "epsilon  0.08353084243219053\n",
      "state is  19\n",
      "Episode:   53/200 and step:    8. Wins: 54/  53, reward 1.0\n",
      "epsilon  0.07994470092982527\n",
      "state is  2\n",
      "Episode:   54/200 and step:    7. Wins: 55/  54, reward 1.0\n",
      "epsilon  0.07653345761235225\n",
      "state is  7\n",
      "Episode:   55/200 and step:    7. Wins: 56/  55, reward 1.0\n",
      "epsilon  0.0732885825946405\n",
      "state is  10\n",
      "Episode:   56/200 and step:    7. Wins: 57/  56, reward 1.0\n",
      "epsilon  0.07020196199896576\n",
      "state is  4\n",
      "Episode:   57/200 and step:    8. Wins: 58/  57, reward 1.0\n",
      "epsilon  0.06726587766609007\n",
      "state is  19\n",
      "Episode:   58/200 and step:    8. Wins: 59/  58, reward 1.0\n",
      "epsilon  0.06447298785584314\n",
      "state is  2\n",
      "Episode:   59/200 and step:   99. Wins: 60/  59, reward 0.0\n",
      "epsilon  0.06181630888894806\n",
      "state is  5\n",
      "Episode:   60/200 and step:   99. Wins: 61/  60, reward 0.0\n",
      "epsilon  0.05928919768418531\n",
      "state is  9\n",
      "Episode:   61/200 and step:    7. Wins: 62/  61, reward 1.0\n",
      "epsilon  0.0568853351472295\n",
      "state is  1\n",
      "Episode:   62/200 and step:    7. Wins: 63/  62, reward 1.0\n",
      "epsilon  0.05459871036962222\n",
      "state is  8\n",
      "Episode:   63/200 and step:    7. Wins: 64/  63, reward 1.0\n",
      "epsilon  0.05242360559836977\n",
      "state is  19\n",
      "Episode:   64/200 and step:    7. Wins: 65/  64, reward 1.0\n",
      "epsilon  0.05035458193858255\n",
      "state is  9\n",
      "Episode:   65/200 and step:    7. Wins: 66/  65, reward 1.0\n",
      "epsilon  0.04838646575340479\n",
      "state is  16\n",
      "Episode:   66/200 and step:    7. Wins: 67/  66, reward 1.0\n",
      "epsilon  0.046514335727227595\n",
      "state is  10\n",
      "Episode:   67/200 and step:    7. Wins: 68/  67, reward 1.0\n",
      "epsilon  0.04473351055983658\n",
      "state is  1\n",
      "Episode:   68/200 and step:    7. Wins: 69/  68, reward 1.0\n",
      "epsilon  0.04303953726072281\n",
      "state is  10\n",
      "Episode:   69/200 and step:    7. Wins: 70/  69, reward 1.0\n",
      "epsilon  0.04142818001428726\n",
      "state is  17\n",
      "Episode:   70/200 and step:    7. Wins: 71/  70, reward 1.0\n",
      "epsilon  0.039895409588095315\n",
      "state is  10\n",
      "Episode:   71/200 and step:    7. Wins: 72/  71, reward 1.0\n",
      "epsilon  0.03843739325769703\n",
      "state is  9\n",
      "Episode:   72/200 and step:    7. Wins: 73/  72, reward 1.0\n",
      "epsilon  0.03705048522281963\n",
      "state is  11\n",
      "Episode:   73/200 and step:   99. Wins: 74/  73, reward 0.0\n",
      "epsilon  0.03573121749096778\n",
      "state is  5\n",
      "Episode:   74/200 and step:   99. Wins: 75/  74, reward 0.0\n",
      "epsilon  0.034476291205635994\n",
      "state is  5\n",
      "Episode:   75/200 and step:   99. Wins: 76/  75, reward 0.0\n",
      "epsilon  0.03328256839744902\n",
      "state is  13\n",
      "Episode:   76/200 and step:    7. Wins: 77/  76, reward 1.0\n",
      "epsilon  0.03214706413760394\n",
      "state is  9\n",
      "Episode:   77/200 and step:    7. Wins: 78/  77, reward 1.0\n",
      "epsilon  0.031066939073993396\n",
      "state is  2\n",
      "Episode:   78/200 and step:    7. Wins: 79/  78, reward 1.0\n",
      "epsilon  0.03003949233134634\n",
      "state is  1\n",
      "Episode:   79/200 and step:    7. Wins: 80/  79, reward 1.0\n",
      "epsilon  0.029062154757633053\n",
      "state is  8\n",
      "Episode:   80/200 and step:    7. Wins: 81/  80, reward 1.0\n",
      "epsilon  0.028132482499846838\n",
      "state is  18\n",
      "Episode:   81/200 and step:    7. Wins: 82/  81, reward 1.0\n",
      "epsilon  0.02724815089309858\n",
      "state is  5\n",
      "Episode:   82/200 and step:    7. Wins: 83/  82, reward 1.0\n",
      "epsilon  0.026406948647743622\n",
      "state is  16\n",
      "Episode:   83/200 and step:    7. Wins: 84/  83, reward 1.0\n",
      "epsilon  0.025606772320005942\n",
      "state is  12\n",
      "Episode:   84/200 and step:    7. Wins: 85/  84, reward 1.0\n",
      "epsilon  0.024845621052272927\n",
      "state is  4\n",
      "Episode:   85/200 and step:    7. Wins: 86/  85, reward 1.0\n",
      "epsilon  0.024121591569909263\n",
      "state is  13\n",
      "Episode:   86/200 and step:    7. Wins: 87/  86, reward 1.0\n",
      "epsilon  0.023432873422078924\n",
      "state is  15\n",
      "Episode:   87/200 and step:   99. Wins: 88/  87, reward 0.0\n",
      "epsilon  0.022777744454675064\n",
      "state is  14\n",
      "Episode:   88/200 and step:    8. Wins: 89/  88, reward 1.0\n",
      "epsilon  0.02215456650403775\n",
      "state is  11\n",
      "Episode:   89/200 and step:    7. Wins: 90/  89, reward 1.0\n",
      "epsilon  0.021561781300691488\n",
      "state is  0\n",
      "Episode:   90/200 and step:    7. Wins: 91/  90, reward 1.0\n",
      "epsilon  0.02099790657285988\n",
      "state is  19\n",
      "Episode:   91/200 and step:   99. Wins: 92/  91, reward 0.0\n",
      "epsilon  0.02046153234001413\n",
      "state is  12\n",
      "Episode:   92/200 and step:    7. Wins: 93/  92, reward 1.0\n",
      "epsilon  0.01995131738718724\n",
      "state is  18\n",
      "Episode:   93/200 and step:   99. Wins: 94/  93, reward 0.0\n",
      "epsilon  0.01946598591123807\n",
      "state is  19\n",
      "Episode:   94/200 and step:    8. Wins: 95/  94, reward 1.0\n",
      "epsilon  0.019004324330678855\n",
      "state is  19\n",
      "Episode:   95/200 and step:   99. Wins: 96/  95, reward 0.0\n",
      "epsilon  0.01856517825108943\n",
      "state is  1\n",
      "Episode:   96/200 and step:    7. Wins: 97/  96, reward 1.0\n",
      "epsilon  0.018147449578529824\n",
      "state is  6\n",
      "Episode:   97/200 and step:    7. Wins: 98/  97, reward 1.0\n",
      "epsilon  0.01775009377373351\n",
      "state is  5\n",
      "Episode:   98/200 and step:    7. Wins: 99/  98, reward 1.0\n",
      "epsilon  0.017372117240215094\n",
      "state is  10\n",
      "Episode:   99/200 and step:    7. Wins: 100/  99, reward 1.0\n",
      "epsilon  0.017012574839761596\n",
      "state is  0\n",
      "Episode:  100/200 and step:    7. Wins: 101/ 100, reward 1.0\n",
      "epsilon  0.016670567529094613\n",
      "state is  10\n",
      "Episode:  101/200 and step:    7. Wins: 102/ 101, reward 1.0\n",
      "epsilon  0.016345240111793814\n",
      "state is  12\n",
      "Episode:  102/200 and step:    7. Wins: 103/ 102, reward 1.0\n",
      "epsilon  0.016035779099860478\n",
      "state is  19\n",
      "Episode:  103/200 and step:    8. Wins: 104/ 103, reward 1.0\n",
      "epsilon  0.01574141067957372\n",
      "state is  0\n",
      "Episode:  104/200 and step:    7. Wins: 105/ 104, reward 1.0\n",
      "epsilon  0.015461398776553163\n",
      "state is  2\n",
      "Episode:  105/200 and step:    7. Wins: 106/ 105, reward 1.0\n",
      "epsilon  0.015195043215189571\n",
      "state is  0\n",
      "Episode:  106/200 and step:    7. Wins: 107/ 106, reward 1.0\n",
      "epsilon  0.01494167796784111\n",
      "state is  9\n",
      "Episode:  107/200 and step:   99. Wins: 108/ 107, reward 0.0\n",
      "epsilon  0.014700669489417359\n",
      "state is  1\n",
      "Episode:  108/200 and step:    7. Wins: 109/ 108, reward 1.0\n",
      "epsilon  0.01447141513318654\n",
      "state is  7\n",
      "Episode:  109/200 and step:    7. Wins: 110/ 109, reward 1.0\n",
      "epsilon  0.014253341643844817\n",
      "state is  17\n",
      "Episode:  110/200 and step:    7. Wins: 111/ 110, reward 1.0\n",
      "epsilon  0.014045903724079427\n",
      "state is  9\n",
      "Episode:  111/200 and step:   99. Wins: 112/ 111, reward 0.0\n",
      "epsilon  0.013848582671041366\n",
      "state is  19\n",
      "Episode:  112/200 and step:   99. Wins: 113/ 112, reward 0.0\n",
      "epsilon  0.0136608850793181\n",
      "state is  17\n",
      "Episode:  113/200 and step:    7. Wins: 114/ 113, reward 1.0\n",
      "epsilon  0.013482341607163007\n",
      "state is  3\n",
      "Episode:  114/200 and step:   99. Wins: 115/ 114, reward 0.0\n",
      "epsilon  0.01331250580289656\n",
      "state is  4\n",
      "Episode:  115/200 and step:   99. Wins: 116/ 115, reward 0.0\n",
      "epsilon  0.01315095298854457\n",
      "state is  1\n",
      "Episode:  116/200 and step:    7. Wins: 117/ 116, reward 1.0\n",
      "epsilon  0.012997279197922054\n",
      "state is  5\n",
      "Episode:  117/200 and step:    7. Wins: 118/ 117, reward 1.0\n",
      "epsilon  0.012851100166507357\n",
      "state is  6\n",
      "Episode:  118/200 and step:    7. Wins: 119/ 118, reward 1.0\n",
      "epsilon  0.012712050370580685\n",
      "state is  17\n",
      "Episode:  119/200 and step:    7. Wins: 120/ 119, reward 1.0\n",
      "epsilon  0.012579782113224414\n",
      "state is  4\n",
      "Episode:  120/200 and step:    7. Wins: 121/ 120, reward 1.0\n",
      "epsilon  0.012453964654899695\n",
      "state is  2\n",
      "Episode:  121/200 and step:    7. Wins: 122/ 121, reward 1.0\n",
      "epsilon  0.012334283386425329\n",
      "state is  10\n",
      "Episode:  122/200 and step:    7. Wins: 123/ 122, reward 1.0\n",
      "epsilon  0.012220439042290943\n",
      "state is  4\n",
      "Episode:  123/200 and step:    7. Wins: 124/ 123, reward 1.0\n",
      "epsilon  0.012112146952337331\n",
      "state is  5\n",
      "Episode:  124/200 and step:    7. Wins: 125/ 124, reward 1.0\n",
      "epsilon  0.012009136329932777\n",
      "state is  15\n",
      "Episode:  125/200 and step:    7. Wins: 126/ 125, reward 1.0\n",
      "epsilon  0.011911149594865432\n",
      "state is  17\n",
      "Episode:  126/200 and step:    7. Wins: 127/ 126, reward 1.0\n",
      "epsilon  0.011817941729258617\n",
      "state is  0\n",
      "Episode:  127/200 and step:    7. Wins: 128/ 127, reward 1.0\n",
      "epsilon  0.011729279664898506\n",
      "state is  9\n",
      "Episode:  128/200 and step:    8. Wins: 129/ 128, reward 1.0\n",
      "epsilon  0.011644941700442194\n",
      "state is  5\n",
      "Episode:  129/200 and step:   99. Wins: 130/ 129, reward 0.0\n",
      "epsilon  0.011564716947048855\n",
      "state is  4\n",
      "Episode:  130/200 and step:    7. Wins: 131/ 130, reward 1.0\n",
      "epsilon  0.011488404801047796\n",
      "state is  11\n",
      "Episode:  131/200 and step:    7. Wins: 132/ 131, reward 1.0\n",
      "epsilon  0.011415814442324795\n",
      "state is  2\n",
      "Episode:  132/200 and step:    7. Wins: 133/ 132, reward 1.0\n",
      "epsilon  0.011346764357172414\n",
      "state is  15\n",
      "Episode:  133/200 and step:    7. Wins: 134/ 133, reward 1.0\n",
      "epsilon  0.01128108188441119\n",
      "state is  3\n",
      "Episode:  134/200 and step:    7. Wins: 135/ 134, reward 1.0\n",
      "epsilon  0.011218602783646746\n",
      "state is  4\n",
      "Episode:  135/200 and step:    8. Wins: 136/ 135, reward 1.0\n",
      "epsilon  0.011159170824583262\n",
      "state is  4\n",
      "Episode:  136/200 and step:    8. Wins: 137/ 136, reward 1.0\n",
      "epsilon  0.011102637396366355\n",
      "state is  19\n",
      "Episode:  137/200 and step:    7. Wins: 138/ 137, reward 1.0\n",
      "epsilon  0.011048861135978534\n",
      "state is  18\n",
      "Episode:  138/200 and step:    7. Wins: 139/ 138, reward 1.0\n",
      "epsilon  0.010997707574758025\n",
      "state is  8\n",
      "Episode:  139/200 and step:    7. Wins: 140/ 139, reward 1.0\n",
      "epsilon  0.01094904880215708\n",
      "state is  0\n",
      "Episode:  140/200 and step:    7. Wins: 141/ 140, reward 1.0\n",
      "epsilon  0.010902763145898971\n",
      "state is  13\n",
      "Episode:  141/200 and step:    7. Wins: 142/ 141, reward 1.0\n",
      "epsilon  0.010858734867733932\n",
      "state is  0\n",
      "Episode:  142/200 and step:    7. Wins: 143/ 142, reward 1.0\n",
      "epsilon  0.010816853874033245\n",
      "state is  10\n",
      "Episode:  143/200 and step:    7. Wins: 144/ 143, reward 1.0\n",
      "epsilon  0.010777015440497823\n",
      "state is  8\n",
      "Episode:  144/200 and step:    7. Wins: 145/ 144, reward 1.0\n",
      "epsilon  0.010739119950292912\n",
      "state is  2\n",
      "Episode:  145/200 and step:    7. Wins: 146/ 145, reward 1.0\n",
      "epsilon  0.010703072644954124\n",
      "state is  6\n",
      "Episode:  146/200 and step:    7. Wins: 147/ 146, reward 1.0\n",
      "epsilon  0.010668783387441906\n",
      "state is  11\n",
      "Episode:  147/200 and step:    7. Wins: 148/ 147, reward 1.0\n",
      "epsilon  0.010636166436752002\n",
      "state is  13\n",
      "Episode:  148/200 and step:    7. Wins: 149/ 148, reward 1.0\n",
      "epsilon  0.010605140233518277\n",
      "state is  19\n",
      "Episode:  149/200 and step:    7. Wins: 150/ 149, reward 1.0\n",
      "epsilon  0.01057562719607182\n",
      "state is  17\n",
      "Episode:  150/200 and step:    8. Wins: 151/ 150, reward 1.0\n",
      "epsilon  0.010547553526446355\n",
      "state is  12\n",
      "Episode:  151/200 and step:    7. Wins: 152/ 151, reward 1.0\n",
      "epsilon  0.010520849025844903\n",
      "state is  10\n",
      "Episode:  152/200 and step:   99. Wins: 153/ 152, reward 0.0\n",
      "epsilon  0.010495446919106205\n",
      "state is  12\n",
      "Episode:  153/200 and step:   99. Wins: 154/ 153, reward 0.0\n",
      "epsilon  0.010471283687732046\n",
      "state is  7\n",
      "Episode:  154/200 and step:    7. Wins: 155/ 154, reward 1.0\n",
      "epsilon  0.010448298911057929\n",
      "state is  11\n",
      "Episode:  155/200 and step:    7. Wins: 156/ 155, reward 1.0\n",
      "epsilon  0.01042643511516993\n",
      "state is  12\n",
      "Episode:  156/200 and step:    7. Wins: 157/ 156, reward 1.0\n",
      "epsilon  0.010405637629189989\n",
      "state is  12\n",
      "Episode:  157/200 and step:    7. Wins: 158/ 157, reward 1.0\n",
      "epsilon  0.010385854448570228\n",
      "state is  13\n",
      "Episode:  158/200 and step:    7. Wins: 159/ 158, reward 1.0\n",
      "epsilon  0.010367036105054498\n",
      "state is  12\n",
      "Episode:  159/200 and step:    7. Wins: 160/ 159, reward 1.0\n",
      "epsilon  0.010349135542981974\n",
      "state is  8\n",
      "Episode:  160/200 and step:    7. Wins: 161/ 160, reward 1.0\n",
      "epsilon  0.010332108001623487\n",
      "state is  9\n",
      "Episode:  161/200 and step:    8. Wins: 162/ 161, reward 1.0\n",
      "epsilon  0.010315910903256391\n",
      "state is  7\n",
      "Episode:  162/200 and step:    7. Wins: 163/ 162, reward 1.0\n",
      "epsilon  0.01030050374669808\n",
      "state is  9\n",
      "Episode:  163/200 and step:    8. Wins: 164/ 163, reward 1.0\n",
      "epsilon  0.010285848006031922\n",
      "state is  17\n",
      "Episode:  164/200 and step:    7. Wins: 165/ 164, reward 1.0\n",
      "epsilon  0.01027190703427242\n",
      "state is  9\n",
      "Episode:  165/200 and step:    8. Wins: 166/ 165, reward 1.0\n",
      "epsilon  0.010258645971728651\n",
      "state is  17\n",
      "Episode:  166/200 and step:    7. Wins: 167/ 166, reward 1.0\n",
      "epsilon  0.010246031658836873\n",
      "state is  13\n",
      "Episode:  167/200 and step:    7. Wins: 168/ 167, reward 1.0\n",
      "epsilon  0.010234032553244354\n",
      "state is  15\n",
      "Episode:  168/200 and step:    7. Wins: 169/ 168, reward 1.0\n",
      "epsilon  0.01022261865093706\n",
      "state is  11\n",
      "Episode:  169/200 and step:    7. Wins: 170/ 169, reward 1.0\n",
      "epsilon  0.010211761411213984\n",
      "state is  10\n",
      "Episode:  170/200 and step:    7. Wins: 171/ 170, reward 1.0\n",
      "epsilon  0.010201433685320538\n",
      "state is  3\n",
      "Episode:  171/200 and step:    7. Wins: 172/ 171, reward 1.0\n",
      "epsilon  0.010191609648562514\n",
      "state is  13\n",
      "Episode:  172/200 and step:    7. Wins: 173/ 172, reward 1.0\n",
      "epsilon  0.010182264735730903\n",
      "state is  8\n",
      "Episode:  173/200 and step:    7. Wins: 174/ 173, reward 1.0\n",
      "epsilon  0.010173375579676082\n",
      "state is  9\n",
      "Episode:  174/200 and step:    7. Wins: 175/ 174, reward 1.0\n",
      "epsilon  0.010164919952877757\n",
      "state is  9\n",
      "Episode:  175/200 and step:    8. Wins: 176/ 175, reward 1.0\n",
      "epsilon  0.010156876711864594\n",
      "state is  18\n",
      "Episode:  176/200 and step:    7. Wins: 177/ 176, reward 1.0\n",
      "epsilon  0.010149225744344521\n",
      "state is  0\n",
      "Episode:  177/200 and step:   99. Wins: 178/ 177, reward 0.0\n",
      "epsilon  0.01014194791891353\n",
      "state is  15\n",
      "Episode:  178/200 and step:    7. Wins: 179/ 178, reward 1.0\n",
      "epsilon  0.010135025037217192\n",
      "state is  11\n",
      "Episode:  179/200 and step:    7. Wins: 180/ 179, reward 1.0\n",
      "epsilon  0.010128439788445296\n",
      "state is  11\n",
      "Episode:  180/200 and step:    7. Wins: 181/ 180, reward 1.0\n",
      "epsilon  0.010122175706045813\n",
      "state is  16\n",
      "Episode:  181/200 and step:    7. Wins: 182/ 181, reward 1.0\n",
      "epsilon  0.010116217126549927\n",
      "state is  6\n",
      "Episode:  182/200 and step:    7. Wins: 183/ 182, reward 1.0\n",
      "epsilon  0.010110549150405214\n",
      "state is  11\n",
      "Episode:  183/200 and step:   99. Wins: 184/ 183, reward 0.0\n",
      "epsilon  0.010105157604718994\n",
      "state is  1\n",
      "Episode:  184/200 and step:    7. Wins: 185/ 184, reward 1.0\n",
      "epsilon  0.010100029007818723\n",
      "state is  10\n",
      "Episode:  185/200 and step:    7. Wins: 186/ 185, reward 1.0\n",
      "epsilon  0.01009515053554078\n",
      "state is  3\n",
      "Episode:  186/200 and step:   99. Wins: 187/ 186, reward 0.0\n",
      "epsilon  0.010090509989163391\n",
      "state is  14\n",
      "Episode:  187/200 and step:   99. Wins: 188/ 187, reward 0.0\n",
      "epsilon  0.01008609576490346\n",
      "state is  19\n",
      "Episode:  188/200 and step:    7. Wins: 189/ 188, reward 1.0\n",
      "epsilon  0.010081896824901066\n",
      "state is  1\n",
      "Episode:  189/200 and step:    7. Wins: 190/ 189, reward 1.0\n",
      "epsilon  0.010077902669619077\n",
      "state is  9\n",
      "Episode:  190/200 and step:    7. Wins: 191/ 190, reward 1.0\n",
      "epsilon  0.010074103311588823\n",
      "state is  7\n",
      "Episode:  191/200 and step:   99. Wins: 192/ 191, reward 0.0\n",
      "epsilon  0.010070489250436234\n",
      "state is  15\n",
      "Episode:  192/200 and step:    7. Wins: 193/ 192, reward 1.0\n",
      "epsilon  0.010067051449125945\n",
      "state is  19\n",
      "Episode:  193/200 and step:    7. Wins: 194/ 193, reward 1.0\n",
      "epsilon  0.010063781311364012\n",
      "state is  16\n",
      "Episode:  194/200 and step:    7. Wins: 195/ 194, reward 1.0\n",
      "epsilon  0.01006067066010269\n",
      "state is  8\n",
      "Episode:  195/200 and step:    7. Wins: 196/ 195, reward 1.0\n",
      "epsilon  0.01005771171709356\n",
      "state is  12\n",
      "Episode:  196/200 and step:    7. Wins: 197/ 196, reward 1.0\n",
      "epsilon  0.010054897083437856\n",
      "state is  0\n",
      "Episode:  197/200 and step:    7. Wins: 198/ 197, reward 1.0\n",
      "epsilon  0.01005221972108536\n",
      "state is  18\n",
      "Episode:  198/200 and step:    7. Wins: 199/ 198, reward 1.0\n",
      "epsilon  0.010049672935235614\n",
      "state is  16\n",
      "Episode:  199/200 and step:    7. Wins: 200/ 199, reward 1.0\n",
      "epsilon  0.010047250357597435\n",
      " Train mean % score=  86.0\n"
     ]
    }
   ],
   "source": [
    "reward_lst=[]\n",
    "wins = 0\n",
    "for episode in range(train_episodes):\n",
    "    state= env.reset()\n",
    "    state = np.random.choice(state_size-5)\n",
    "    enemy_state = 24\n",
    "    print(\"state is \", state)\n",
    "    state_arr=np.zeros(state_size)\n",
    "    state_arr[[state, enemy_state]] = 1\n",
    "    state= np.reshape(state_arr, [1, state_size])\n",
    "    reward = 0\n",
    "    done = False\n",
    "    for t in range(max_steps):\n",
    "        # env.render()\n",
    "        action = agent.action(state, episode)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state_arr = np.zeros(state_size)\n",
    "        new_state_arr[[new_state, enemy_state]] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "\n",
    "        # next_state = new_state\n",
    "        # old_state = state\n",
    "        agent.add_memory(new_state, reward, done, state, action)\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            wins+=1\n",
    "            print(f'Episode: {episode:4}/{train_episodes} and step: {t:4}. Wins: {wins}/{episode:4}, reward {reward}')\n",
    "            break\n",
    "\n",
    "    # print(\"storing\", next_state, reward, done, old_state, action)\n",
    "    reward_lst.append(reward)\n",
    "\n",
    "    if len(agent.memory)> batch_size:\n",
    "        agent.replay(batch_size, episode)\n",
    "print(' Train mean % score= ', round(100*np.mean(reward_lst),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"./saved_models/dqn_agent10x10.h5\")\n",
    "agent.load(\"./saved_models/dqn_agent10x10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_state_arr = np.zeros(25)\n",
    "# new_state_arr[[24,24]] = 1\n",
    "# new_state = np.reshape(new_state_arr, [1, 25])\n",
    "# new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state is  7\n",
      "******* EPISODE  0  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  1  *******\n",
      "1.0\n",
      "state is  6\n",
      "******* EPISODE  2  *******\n",
      "1.0\n",
      "state is  7\n",
      "******* EPISODE  3  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  4  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  5  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  6  *******\n",
      "1.0\n",
      "state is  9\n",
      "******* EPISODE  7  *******\n",
      "1.0\n",
      "state is  10\n",
      "******* EPISODE  8  *******\n",
      "1.0\n",
      "state is  16\n",
      "******* EPISODE  9  *******\n",
      "1.0\n",
      "state is  4\n",
      "******* EPISODE  10  *******\n",
      "1.0\n",
      "state is  16\n",
      "******* EPISODE  11  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  12  *******\n",
      "1.0\n",
      "state is  17\n",
      "******* EPISODE  13  *******\n",
      "1.0\n",
      "state is  15\n",
      "******* EPISODE  14  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  15  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  16  *******\n",
      "1.0\n",
      "state is  15\n",
      "******* EPISODE  17  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  18  *******\n",
      "1.0\n",
      "state is  8\n",
      "******* EPISODE  19  *******\n",
      "1.0\n",
      "state is  18\n",
      "******* EPISODE  20  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  21  *******\n",
      "1.0\n",
      "state is  11\n",
      "******* EPISODE  22  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  23  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  24  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  25  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  26  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  27  *******\n",
      "1.0\n",
      "state is  6\n",
      "******* EPISODE  28  *******\n",
      "1.0\n",
      "state is  12\n",
      "******* EPISODE  29  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  30  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  31  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  32  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  33  *******\n",
      "1.0\n",
      "state is  6\n",
      "******* EPISODE  34  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  35  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  36  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  37  *******\n",
      "1.0\n",
      "state is  8\n",
      "******* EPISODE  38  *******\n",
      "1.0\n",
      "state is  15\n",
      "******* EPISODE  39  *******\n",
      "1.0\n",
      "state is  9\n",
      "******* EPISODE  40  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  41  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  42  *******\n",
      "1.0\n",
      "state is  10\n",
      "******* EPISODE  43  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  44  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  45  *******\n",
      "1.0\n",
      "state is  16\n",
      "******* EPISODE  46  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  47  *******\n",
      "1.0\n",
      "state is  15\n",
      "******* EPISODE  48  *******\n",
      "1.0\n",
      "state is  10\n",
      "******* EPISODE  49  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  50  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  51  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  52  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  53  *******\n",
      "1.0\n",
      "state is  1\n",
      "******* EPISODE  54  *******\n",
      "1.0\n",
      "state is  12\n",
      "******* EPISODE  55  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  56  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  57  *******\n",
      "1.0\n",
      "state is  1\n",
      "******* EPISODE  58  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  59  *******\n",
      "1.0\n",
      "state is  16\n",
      "******* EPISODE  60  *******\n",
      "1.0\n",
      "state is  12\n",
      "******* EPISODE  61  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  62  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  63  *******\n",
      "1.0\n",
      "state is  8\n",
      "******* EPISODE  64  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  65  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  66  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  67  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  68  *******\n",
      "1.0\n",
      "state is  17\n",
      "******* EPISODE  69  *******\n",
      "1.0\n",
      "state is  1\n",
      "******* EPISODE  70  *******\n",
      "1.0\n",
      "state is  18\n",
      "******* EPISODE  71  *******\n",
      "1.0\n",
      "state is  19\n",
      "******* EPISODE  72  *******\n",
      "1.0\n",
      "state is  9\n",
      "******* EPISODE  73  *******\n",
      "1.0\n",
      "state is  11\n",
      "******* EPISODE  74  *******\n",
      "1.0\n",
      "state is  16\n",
      "******* EPISODE  75  *******\n",
      "1.0\n",
      "state is  4\n",
      "******* EPISODE  76  *******\n",
      "1.0\n",
      "state is  18\n",
      "******* EPISODE  77  *******\n",
      "1.0\n",
      "state is  7\n",
      "******* EPISODE  78  *******\n",
      "1.0\n",
      "state is  17\n",
      "******* EPISODE  79  *******\n",
      "1.0\n",
      "state is  9\n",
      "******* EPISODE  80  *******\n",
      "1.0\n",
      "state is  18\n",
      "******* EPISODE  81  *******\n",
      "1.0\n",
      "state is  12\n",
      "******* EPISODE  82  *******\n",
      "1.0\n",
      "state is  9\n",
      "******* EPISODE  83  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  84  *******\n",
      "1.0\n",
      "state is  14\n",
      "******* EPISODE  85  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  86  *******\n",
      "1.0\n",
      "state is  17\n",
      "******* EPISODE  87  *******\n",
      "1.0\n",
      "state is  12\n",
      "******* EPISODE  88  *******\n",
      "1.0\n",
      "state is  2\n",
      "******* EPISODE  89  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  90  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  91  *******\n",
      "1.0\n",
      "state is  0\n",
      "******* EPISODE  92  *******\n",
      "1.0\n",
      "state is  3\n",
      "******* EPISODE  93  *******\n",
      "1.0\n",
      "state is  10\n",
      "******* EPISODE  94  *******\n",
      "1.0\n",
      "state is  1\n",
      "******* EPISODE  95  *******\n",
      "1.0\n",
      "state is  1\n",
      "******* EPISODE  96  *******\n",
      "1.0\n",
      "state is  17\n",
      "******* EPISODE  97  *******\n",
      "1.0\n",
      "state is  13\n",
      "******* EPISODE  98  *******\n",
      "1.0\n",
      "state is  5\n",
      "******* EPISODE  99  *******\n",
      "1.0\n",
      " Test mean % score=  100\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_wins=[]\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.random.choice(state_size-5)\n",
    "    enemy_state = 24\n",
    "    print(\"state is \", state)\n",
    "    state_arr=np.zeros(state_size)\n",
    "    state_arr[[state, enemy_state]] = 1\n",
    "    state= np.reshape(state_arr, [1, state_size])\n",
    "    done = False\n",
    "    reward=0\n",
    "    state_lst = []\n",
    "    state_lst.append(state)\n",
    "    print('******* EPISODE ',episode, ' *******')\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.action(state, episode, test = True)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state_arr = np.zeros(state_size)\n",
    "        new_state_arr[[new_state, enemy_state]] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "        state = new_state\n",
    "        state_lst.append(state)\n",
    "        if done:\n",
    "            print(reward)\n",
    "            # env.render()\n",
    "            break\n",
    "\n",
    "    test_wins.append(reward)\n",
    "env.close()\n",
    "\n",
    "print(' Test mean % score= ', int(100*np.mean(test_wins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.random.choice(state_size-5)\n",
    "enemy_state = 24\n",
    "state_arr=np.zeros(state_size)\n",
    "state_arr[[state, enemy_state]] = 1\n",
    "state= np.reshape(state_arr, [1, state_size])\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state is  2\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]]\n"
     ]
    }
   ],
   "source": [
    "state = np.random.choice(state_size-5)\n",
    "enemy_state = 24\n",
    "print(\"state is \", state)\n",
    "state_arr=np.zeros(state_size)\n",
    "state_arr[[state, enemy_state]] = 1\n",
    "state= np.reshape(state_arr, [1, state_size])\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, reward, done, info = env.step(action)\n",
    "print(new_state)\n",
    "new_state_arr = np.zeros(state_size)\n",
    "new_state_arr[[new_state, enemy_state]] = 1\n",
    "new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
