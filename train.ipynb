{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "import  numpy as np\n",
    "from    config          import *\n",
    "from    agent           import Agent\n",
    "from    enviroment      import Enviroment\n",
    "from    IPython.display import clear_output\n",
    "from    matplotlib      import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = GRID_SIZE\n",
    "num_col = grid_size\n",
    "\n",
    "possibleActions = POSSIBLE_ACTIONS\n",
    "\n",
    "action_space_dict = {\n",
    "    \"U\" : 0,\n",
    "    \"D\" : 1,\n",
    "    \"L\" : 2,\n",
    "    \"R\" : 3,\n",
    "    \"S\" : 4\n",
    "}\n",
    "TYPE              = \"random\"\n",
    "n_agents          = N_AGENTS\n",
    "allplayerpos      = PLAYER_POS[: n_agents]\n",
    "enemy_list_pos    = ENEMY_POS[: n_agents]\n",
    "batch_size        = BATCH_SIZE\n",
    "replay_memory_len = REPLAY_MEMORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(state_num):\n",
    "    return int(state_num/num_col), state_num%num_col\n",
    "\n",
    "def state_encode(row,col):\n",
    "    return row*num_col + col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    total_step = 0\n",
    "    rewards_list = []\n",
    "    timesteps_list = []\n",
    "    total_steps = 1\n",
    "    for episode in range(1, EPISODES):\n",
    "        print(\"Episode number: \", episode)\n",
    "\n",
    "        reward_all = 0\n",
    "        time_step = 1\n",
    "        i = 0\n",
    "\n",
    "        for agent in all_agents:\n",
    "            agent.terminal = False\n",
    "        \n",
    "        [states, enemy_states] = env.reset()\n",
    "        print(\"enemy states: \", enemy_states)\n",
    "        for agent in all_agents:\n",
    "            agent.set_pos(allplayerpos[i])\n",
    "            i = i + 1\n",
    "\n",
    "        done = [False for _ in range(n_agents)]\n",
    "\n",
    "        while not any(done):\n",
    "\n",
    "            # env.render(clear=True)\n",
    "            actions = []\n",
    "            for agent in all_agents:\n",
    "                actions.append(agent.act(np.concatenate((states,enemy_states)).ravel(), possibleActions))\n",
    "\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            for agent in all_agents:\n",
    "\n",
    "                next_states = np.array(next_states)\n",
    "                next_states = next_states.ravel()\n",
    "\n",
    "                agent.set_pos(decode_state(next_states[agent.index]))\n",
    "                print(\"action_space_dict[actions[agent.index]]\", action_space_dict[actions[agent.index]])\n",
    "                agent.store(np.concatenate((states,enemy_states)).ravel(), action_space_dict[actions[agent.index]], \\\n",
    "                rewards[agent.index], np.concatenate((next_states,enemy_states)).ravel(), done[agent.index])\n",
    "\n",
    "                if done[agent.index] == True:\n",
    "                    agent.terminal = True\n",
    "                    print(\"agent reached landmark\")\n",
    "                    print(\"updating target model\")\n",
    "                    agent.alighn_target_model()\n",
    "            print(\"actions\", actions)\n",
    "\n",
    "            print(\"agent epsilon \", all_agents[0].epsilon, \"agent memory len\",\\\n",
    "                len(all_agents[0].expirience_replay), \"steps \", time_step,\\\n",
    "                \"reward\", rewards[0], \"next state \", next_states[0], \"agent position \", all_agents[0].return_coordinates())\n",
    "\n",
    "            if time_step >= TIME_STEPS:\n",
    "                print(\"max steps reached\")\n",
    "                break\n",
    "\n",
    "\n",
    "            total_step += 1\n",
    "            time_step += 1\n",
    "            total_steps+1\n",
    "            states = next_states\n",
    "            reward_all += sum(rewards)\n",
    "\n",
    "            if time_step % REPLAY_STEPS == 0:\n",
    "                for agent in all_agents:\n",
    "                    agent.retrain()\n",
    "                    agent.save_model()\n",
    "\n",
    "        print(\"total rewards\", reward_all)\n",
    "        for agent in all_agents:\n",
    "            agent.decay_epsilon(episode)\n",
    "\n",
    "        rewards_list.append(reward_all)\n",
    "        timesteps_list.append(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "-\t-\tP\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\t-\t\n",
      "\n",
      "-\t-\t-\t-\tX\t\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_agents= []\n",
    "\n",
    "for index in range(n_agents):\n",
    "\n",
    "    all_agents.append(Agent(index, allplayerpos[index]))\n",
    "\n",
    "\n",
    "initial_states = []\n",
    "for agent in all_agents:\n",
    "    initial_states.append(state_encode(agent.x, agent.y))\n",
    "\n",
    "enemy_states = []\n",
    "for enemy_pos in enemy_list_pos:\n",
    "    enemy_states.append(state_encode(enemy_pos[0], enemy_pos[1]))\n",
    "\n",
    "env = Enviroment(initial_states = initial_states, enemy_states = enemy_states, type = \"random\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  1\n",
      "enemy states:  [4]\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 1 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 2 steps  2 reward 0 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  1.0 agent memory len 3 steps  3 reward 0 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  1.0 agent memory len 4 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  1.0 agent memory len 5 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  1.0 agent memory len 6 steps  6 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 7 steps  7 reward 0 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  1.0 agent memory len 8 steps  8 reward 10 next state  4 agent position  (0, 4)\n",
      "total rewards 6\n",
      "Episode number:  2\n",
      "enemy states:  [23]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 9 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 10 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 11 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 12 steps  4 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 13 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 14 steps  6 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 15 steps  7 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 16 steps  8 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 17 steps  9 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 18 steps  10 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 19 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 20 steps  12 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 21 steps  13 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.9517171302557069 agent memory len 22 steps  14 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 23 steps  15 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 24 steps  16 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 25 steps  17 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 26 steps  18 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.9517171302557069 agent memory len 27 steps  19 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 28 steps  20 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 29 steps  21 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 30 steps  22 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 31 steps  23 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9517171302557069 agent memory len 32 steps  24 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9517171302557069 agent memory len 33 steps  25 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9517171302557069 agent memory len 34 steps  26 reward -1 next state  14 agent position  (2, 4)\n",
      "max steps reached\n",
      "total rewards -26\n",
      "Episode number:  3\n",
      "enemy states:  [9]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 35 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 36 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.9057890438555999 agent memory len 37 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.9057890438555999 agent memory len 38 steps  4 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.9057890438555999 agent memory len 39 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 40 steps  6 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.9057890438555999 agent memory len 41 steps  7 reward 10 next state  9 agent position  (1, 4)\n",
      "total rewards 4\n",
      "Episode number:  4\n",
      "enemy states:  [7]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.8621008966608072 agent memory len 42 steps  1 reward 10 next state  7 agent position  (1, 2)\n",
      "total rewards 10\n",
      "Episode number:  5\n",
      "enemy states:  [9]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 43 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 44 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 45 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 46 steps  4 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 47 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 48 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.820543445547202 agent memory len 49 steps  7 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 50 steps  8 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 51 steps  9 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 52 steps  10 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 53 steps  11 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 54 steps  12 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 55 steps  13 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 56 steps  14 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.820543445547202 agent memory len 57 steps  15 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 58 steps  16 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 59 steps  17 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 60 steps  18 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 61 steps  19 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.820543445547202 agent memory len 62 steps  20 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.820543445547202 agent memory len 63 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.820543445547202 agent memory len 64 steps  22 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 65 steps  23 reward 0 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.820543445547202 agent memory len 66 steps  24 reward 0 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.820543445547202 agent memory len 67 steps  25 reward 0 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['U']\n",
      "agent epsilon  0.820543445547202 agent memory len 68 steps  26 reward 10 next state  9 agent position  (1, 4)\n",
      "max steps reached\n",
      "total rewards -22\n",
      "Episode number:  6\n",
      "enemy states:  [20]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 69 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 70 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 71 steps  3 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 72 steps  4 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 73 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 74 steps  6 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 75 steps  7 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 76 steps  8 reward -2 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 77 steps  9 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 78 steps  10 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 79 steps  11 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 80 steps  12 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 81 steps  13 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 82 steps  14 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 83 steps  15 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 84 steps  16 reward -2 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 85 steps  17 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 86 steps  18 reward -2 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7810127752406908 agent memory len 87 steps  19 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 88 steps  20 reward -2 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 89 steps  21 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7810127752406908 agent memory len 90 steps  22 reward -2 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 91 steps  23 reward -2 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7810127752406908 agent memory len 92 steps  24 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7810127752406908 agent memory len 93 steps  25 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7810127752406908 agent memory len 94 steps  26 reward -1 next state  5 agent position  (1, 0)\n",
      "max steps reached\n",
      "total rewards -33\n",
      "Episode number:  7\n",
      "enemy states:  [15]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 95 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 96 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 97 steps  3 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 98 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 99 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 100 steps  6 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 101 steps  7 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 102 steps  8 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 103 steps  9 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 104 steps  10 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 105 steps  11 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 106 steps  12 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 107 steps  13 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 108 steps  14 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 109 steps  15 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.7434100384749007 agent memory len 110 steps  16 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 111 steps  17 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 112 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 113 steps  19 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 114 steps  20 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.7434100384749007 agent memory len 115 steps  21 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.7434100384749007 agent memory len 116 steps  22 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 117 steps  23 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7434100384749007 agent memory len 118 steps  24 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.7434100384749007 agent memory len 119 steps  25 reward 10 next state  15 agent position  (3, 0)\n",
      "total rewards -10\n",
      "Episode number:  8\n",
      "enemy states:  [6]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.7076412088215263 agent memory len 120 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.7076412088215263 agent memory len 121 steps  2 reward 10 next state  6 agent position  (1, 1)\n",
      "total rewards 10\n",
      "Episode number:  9\n",
      "enemy states:  [22]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 122 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 123 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 124 steps  3 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 125 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 126 steps  5 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 127 steps  6 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6736168455752829 agent memory len 128 steps  7 reward -2 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6736168455752829 agent memory len 129 steps  8 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 130 steps  9 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 131 steps  10 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6736168455752829 agent memory len 132 steps  11 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 133 steps  12 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 134 steps  13 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 135 steps  14 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 136 steps  15 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6736168455752829 agent memory len 137 steps  16 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.6736168455752829 agent memory len 138 steps  17 reward 10 next state  22 agent position  (4, 2)\n",
      "total rewards -6\n",
      "Episode number:  10\n",
      "enemy states:  [12]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 139 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 140 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 141 steps  3 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 142 steps  4 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 143 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 144 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 145 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 146 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 147 steps  9 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 148 steps  10 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 149 steps  11 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 150 steps  12 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 151 steps  13 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 152 steps  14 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 153 steps  15 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 154 steps  16 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 155 steps  17 reward -2 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6412518701055556 agent memory len 156 steps  18 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6412518701055556 agent memory len 157 steps  19 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 158 steps  20 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 159 steps  21 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 160 steps  22 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 161 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6412518701055556 agent memory len 162 steps  24 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6412518701055556 agent memory len 163 steps  25 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6412518701055556 agent memory len 164 steps  26 reward -1 next state  4 agent position  (0, 4)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  11\n",
      "enemy states:  [18]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 165 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 166 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6104653531155071 agent memory len 167 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 168 steps  4 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 169 steps  5 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 170 steps  6 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 171 steps  7 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 172 steps  8 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 173 steps  9 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 174 steps  10 reward 0 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.6104653531155071 agent memory len 175 steps  11 reward -2 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.6104653531155071 agent memory len 176 steps  12 reward 0 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 177 steps  13 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 178 steps  14 reward 0 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.6104653531155071 agent memory len 179 steps  15 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.6104653531155071 agent memory len 180 steps  16 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.6104653531155071 agent memory len 181 steps  17 reward 10 next state  18 agent position  (3, 3)\n",
      "total rewards -6\n",
      "Episode number:  12\n",
      "enemy states:  [17]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 182 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 183 steps  2 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 184 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 185 steps  4 reward -2 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 186 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 187 steps  6 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 188 steps  7 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 189 steps  8 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 190 steps  9 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 191 steps  10 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 192 steps  11 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 193 steps  12 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 194 steps  13 reward -2 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 195 steps  14 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 196 steps  15 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 197 steps  16 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5811803122766818 agent memory len 198 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 199 steps  18 reward 0 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 200 steps  19 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 201 steps  20 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 202 steps  21 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5811803122766818 agent memory len 203 steps  22 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5811803122766818 agent memory len 204 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 205 steps  24 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5811803122766818 agent memory len 206 steps  25 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5811803122766818 agent memory len 207 steps  26 reward -1 next state  9 agent position  (1, 4)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  13\n",
      "enemy states:  [23]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 208 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 209 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 210 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 211 steps  4 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 212 steps  5 reward -2 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 213 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 214 steps  7 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 215 steps  8 reward -2 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 216 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 217 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 218 steps  11 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 219 steps  12 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 220 steps  13 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 221 steps  14 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5533235197330861 agent memory len 222 steps  15 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 223 steps  16 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 224 steps  17 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 225 steps  18 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 226 steps  19 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5533235197330861 agent memory len 227 steps  20 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 228 steps  21 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5533235197330861 agent memory len 229 steps  22 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 230 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 231 steps  24 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5533235197330861 agent memory len 232 steps  25 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5533235197330861 agent memory len 233 steps  26 reward -1 next state  4 agent position  (0, 4)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  14\n",
      "enemy states:  [23]\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 234 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 235 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 236 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 237 steps  4 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 238 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 239 steps  6 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 240 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 241 steps  8 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 242 steps  9 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 243 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 244 steps  11 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 245 steps  12 reward -2 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 246 steps  13 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 247 steps  14 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 248 steps  15 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 249 steps  16 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5268253189934059 agent memory len 250 steps  17 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 251 steps  18 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5268253189934059 agent memory len 252 steps  19 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 253 steps  20 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5268253189934059 agent memory len 254 steps  21 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 255 steps  22 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 256 steps  23 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5268253189934059 agent memory len 257 steps  24 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 258 steps  25 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5268253189934059 agent memory len 259 steps  26 reward -1 next state  14 agent position  (2, 4)\n",
      "max steps reached\n",
      "total rewards -29\n",
      "Episode number:  15\n",
      "enemy states:  [9]\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 260 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 261 steps  2 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 262 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 263 steps  4 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 264 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 265 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 266 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 267 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 268 steps  9 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 269 steps  10 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 270 steps  11 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 271 steps  12 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 272 steps  13 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.5016194507534953 agent memory len 273 steps  14 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.5016194507534953 agent memory len 274 steps  15 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 275 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.5016194507534953 agent memory len 276 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 277 steps  18 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 278 steps  19 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 279 steps  20 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 280 steps  21 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 281 steps  22 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 282 steps  23 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 283 steps  24 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.5016194507534953 agent memory len 284 steps  25 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.5016194507534953 agent memory len 285 steps  26 reward -1 next state  5 agent position  (1, 0)\n",
      "max steps reached\n",
      "total rewards -28\n",
      "Episode number:  16\n",
      "enemy states:  [8]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 286 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 287 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 288 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 289 steps  4 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 290 steps  5 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 291 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 292 steps  7 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 293 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 294 steps  9 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 295 steps  10 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 296 steps  11 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 297 steps  12 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 298 steps  13 reward -2 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 299 steps  14 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 300 steps  15 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 301 steps  16 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 302 steps  17 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 303 steps  18 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.47764288721360454 agent memory len 304 steps  19 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.47764288721360454 agent memory len 305 steps  20 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 306 steps  21 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.47764288721360454 agent memory len 307 steps  22 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.47764288721360454 agent memory len 308 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 309 steps  24 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.47764288721360454 agent memory len 310 steps  25 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.47764288721360454 agent memory len 311 steps  26 reward -1 next state  20 agent position  (4, 0)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  17\n",
      "enemy states:  [20]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 312 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 313 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 314 steps  3 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 315 steps  4 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 316 steps  5 reward -2 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 317 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 318 steps  7 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 319 steps  8 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 320 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 321 steps  10 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.45483567447604933 agent memory len 322 steps  11 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 323 steps  12 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 324 steps  13 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 325 steps  14 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 326 steps  15 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 327 steps  16 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 328 steps  17 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 329 steps  18 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 330 steps  19 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 331 steps  20 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 332 steps  21 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 333 steps  22 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.45483567447604933 agent memory len 334 steps  23 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.45483567447604933 agent memory len 335 steps  24 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.45483567447604933 agent memory len 336 steps  25 reward -2 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.45483567447604933 agent memory len 337 steps  26 reward -1 next state  6 agent position  (1, 1)\n",
      "max steps reached\n",
      "total rewards -28\n",
      "Episode number:  18\n",
      "enemy states:  [11]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 338 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 339 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.4331407826292394 agent memory len 340 steps  3 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.4331407826292394 agent memory len 341 steps  4 reward 0 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.4331407826292394 agent memory len 342 steps  5 reward 10 next state  11 agent position  (2, 1)\n",
      "total rewards 10\n",
      "Episode number:  19\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 343 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 344 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 345 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 346 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.4125039631431931 agent memory len 347 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.4125039631431931 agent memory len 348 steps  6 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 349 steps  7 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 350 steps  8 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.4125039631431931 agent memory len 351 steps  9 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 352 steps  10 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 353 steps  11 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.4125039631431931 agent memory len 354 steps  12 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 1\n",
      "Episode number:  20\n",
      "enemy states:  [7]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 355 steps  1 reward 0 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 356 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 357 steps  3 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 358 steps  4 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 359 steps  5 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 360 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.3928736132199562 agent memory len 361 steps  7 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 362 steps  8 reward -2 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 363 steps  9 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 364 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.3928736132199562 agent memory len 365 steps  11 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 366 steps  12 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 367 steps  13 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 368 steps  14 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 369 steps  15 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 370 steps  16 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 371 steps  17 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.3928736132199562 agent memory len 372 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.3928736132199562 agent memory len 373 steps  19 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 374 steps  20 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 375 steps  21 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 376 steps  22 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3928736132199562 agent memory len 377 steps  23 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 378 steps  24 reward -2 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.3928736132199562 agent memory len 379 steps  25 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3928736132199562 agent memory len 380 steps  26 reward -2 next state  22 agent position  (4, 2)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  21\n",
      "enemy states:  [12]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 381 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.3742006467597279 agent memory len 382 steps  2 reward 10 next state  12 agent position  (2, 2)\n",
      "total rewards 10\n",
      "Episode number:  22\n",
      "enemy states:  [22]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 383 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.35643837162004377 agent memory len 384 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 385 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 386 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.35643837162004377 agent memory len 387 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 388 steps  6 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.35643837162004377 agent memory len 389 steps  7 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 390 steps  8 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 391 steps  9 reward -2 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.35643837162004377 agent memory len 392 steps  10 reward -2 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.35643837162004377 agent memory len 393 steps  11 reward -2 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.35643837162004377 agent memory len 394 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 395 steps  13 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.35643837162004377 agent memory len 396 steps  14 reward 10 next state  22 agent position  (4, 2)\n",
      "total rewards -3\n",
      "Episode number:  23\n",
      "enemy states:  [9]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.3395423728610988 agent memory len 397 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 398 steps  2 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.3395423728610988 agent memory len 399 steps  3 reward 10 next state  9 agent position  (1, 4)\n",
      "total rewards 9\n",
      "Episode number:  24\n",
      "enemy states:  [23]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 400 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 401 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 402 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 403 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 404 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 405 steps  6 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 406 steps  7 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 407 steps  8 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 408 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 409 steps  10 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 410 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.32347040168526264 agent memory len 411 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 412 steps  13 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.32347040168526264 agent memory len 413 steps  14 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 414 steps  15 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 415 steps  16 reward -2 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.32347040168526264 agent memory len 416 steps  17 reward -2 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.32347040168526264 agent memory len 417 steps  18 reward 10 next state  23 agent position  (4, 3)\n",
      "total rewards -8\n",
      "Episode number:  25\n",
      "enemy states:  [11]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 418 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.30818226979308 agent memory len 419 steps  2 reward 0 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.30818226979308 agent memory len 420 steps  3 reward 10 next state  11 agent position  (2, 1)\n",
      "total rewards 10\n",
      "Episode number:  26\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 421 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 422 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.29363974889158817 agent memory len 423 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.29363974889158817 agent memory len 424 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 425 steps  5 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.29363974889158817 agent memory len 426 steps  6 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 6\n",
      "Episode number:  27\n",
      "enemy states:  [9]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 427 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 428 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 429 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 430 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 431 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 432 steps  6 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.27980647510367246 agent memory len 433 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 434 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.27980647510367246 agent memory len 435 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 436 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 437 steps  11 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 438 steps  12 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.27980647510367246 agent memory len 439 steps  13 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 440 steps  14 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 441 steps  15 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 442 steps  16 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 443 steps  17 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.27980647510367246 agent memory len 444 steps  18 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.27980647510367246 agent memory len 445 steps  19 reward 0 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.27980647510367246 agent memory len 446 steps  20 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.27980647510367246 agent memory len 447 steps  21 reward 10 next state  9 agent position  (1, 4)\n",
      "total rewards -8\n",
      "Episode number:  28\n",
      "enemy states:  [21]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 448 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 449 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 450 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 451 steps  4 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 452 steps  5 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 453 steps  6 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 454 steps  7 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 455 steps  8 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 456 steps  9 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.2666478580394326 agent memory len 457 steps  10 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 458 steps  11 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 459 steps  12 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 460 steps  13 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 461 steps  14 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 462 steps  15 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 463 steps  16 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.2666478580394326 agent memory len 464 steps  17 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 465 steps  18 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 466 steps  19 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 467 steps  20 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 468 steps  21 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 469 steps  22 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 470 steps  23 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2666478580394326 agent memory len 471 steps  24 reward 0 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2666478580394326 agent memory len 472 steps  25 reward 0 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.2666478580394326 agent memory len 473 steps  26 reward 10 next state  21 agent position  (4, 1)\n",
      "max steps reached\n",
      "total rewards -23\n",
      "Episode number:  29\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 474 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.2541309943021904 agent memory len 475 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 476 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.2541309943021904 agent memory len 477 steps  4 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 8\n",
      "Episode number:  30\n",
      "enemy states:  [11]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 478 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.24222458521285967 agent memory len 479 steps  2 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.24222458521285967 agent memory len 480 steps  3 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 481 steps  4 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.24222458521285967 agent memory len 482 steps  5 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.24222458521285967 agent memory len 483 steps  6 reward 10 next state  11 agent position  (2, 1)\n",
      "total rewards 9\n",
      "Episode number:  31\n",
      "enemy states:  [23]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.23089885854694553 agent memory len 484 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 485 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 486 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 487 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.23089885854694553 agent memory len 488 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 489 steps  6 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.23089885854694553 agent memory len 490 steps  7 reward 10 next state  23 agent position  (4, 3)\n",
      "total rewards 5\n",
      "Episode number:  32\n",
      "enemy states:  [7]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.22012549408847562 agent memory len 491 steps  1 reward 10 next state  7 agent position  (1, 2)\n",
      "total rewards 10\n",
      "Episode number:  33\n",
      "enemy states:  [14]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 492 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 493 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 494 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 495 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.20987755281470882 agent memory len 496 steps  5 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 497 steps  6 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 498 steps  7 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 499 steps  8 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 500 steps  9 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.20987755281470882 agent memory len 501 steps  10 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 502 steps  11 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 503 steps  12 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 504 steps  13 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 505 steps  14 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 506 steps  15 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 507 steps  16 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 508 steps  17 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 509 steps  18 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20987755281470882 agent memory len 510 steps  19 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 511 steps  20 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 512 steps  21 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 513 steps  22 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 514 steps  23 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 515 steps  24 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20987755281470882 agent memory len 516 steps  25 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20987755281470882 agent memory len 517 steps  26 reward -1 next state  23 agent position  (4, 3)\n",
      "max steps reached\n",
      "total rewards -18\n",
      "Episode number:  34\n",
      "enemy states:  [12]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 518 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 519 steps  2 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 520 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 521 steps  4 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 522 steps  5 reward -2 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 523 steps  6 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 524 steps  7 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.20012940953454655 agent memory len 525 steps  8 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 526 steps  9 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.20012940953454655 agent memory len 527 steps  10 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 528 steps  11 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20012940953454655 agent memory len 529 steps  12 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.20012940953454655 agent memory len 530 steps  13 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 531 steps  14 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20012940953454655 agent memory len 532 steps  15 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 533 steps  16 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20012940953454655 agent memory len 534 steps  17 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.20012940953454655 agent memory len 535 steps  18 reward -2 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 536 steps  19 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20012940953454655 agent memory len 537 steps  20 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 538 steps  21 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.20012940953454655 agent memory len 539 steps  22 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.20012940953454655 agent memory len 540 steps  23 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.20012940953454655 agent memory len 541 steps  24 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.20012940953454655 agent memory len 542 steps  25 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.20012940953454655 agent memory len 543 steps  26 reward -1 next state  24 agent position  (4, 4)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  35\n",
      "enemy states:  [10]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 544 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 545 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 546 steps  3 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 547 steps  4 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 548 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.19085668881220727 agent memory len 549 steps  6 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 550 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.19085668881220727 agent memory len 551 steps  8 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 552 steps  9 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.19085668881220727 agent memory len 553 steps  10 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 554 steps  11 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 555 steps  12 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 556 steps  13 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 557 steps  14 reward -2 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 558 steps  15 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 559 steps  16 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.19085668881220727 agent memory len 560 steps  17 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 561 steps  18 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.19085668881220727 agent memory len 562 steps  19 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 563 steps  20 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 564 steps  21 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 565 steps  22 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 566 steps  23 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.19085668881220727 agent memory len 567 steps  24 reward -2 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 568 steps  25 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.19085668881220727 agent memory len 569 steps  26 reward -1 next state  14 agent position  (2, 4)\n",
      "max steps reached\n",
      "total rewards -27\n",
      "Episode number:  36\n",
      "enemy states:  [21]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 570 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 571 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 572 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 573 steps  4 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1820362040159407 agent memory len 574 steps  5 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.1820362040159407 agent memory len 575 steps  6 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 576 steps  7 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 577 steps  8 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.1820362040159407 agent memory len 578 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 579 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 580 steps  11 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 581 steps  12 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 582 steps  13 reward -2 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 583 steps  14 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1820362040159407 agent memory len 584 steps  15 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 585 steps  16 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1820362040159407 agent memory len 586 steps  17 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1820362040159407 agent memory len 587 steps  18 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 588 steps  19 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 589 steps  20 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.1820362040159407 agent memory len 590 steps  21 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 591 steps  22 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 592 steps  23 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 593 steps  24 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1820362040159407 agent memory len 594 steps  25 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1820362040159407 agent memory len 595 steps  26 reward -1 next state  18 agent position  (3, 3)\n",
      "max steps reached\n",
      "total rewards -23\n",
      "Episode number:  37\n",
      "enemy states:  [24]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 596 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.17364589933937066 agent memory len 597 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.17364589933937066 agent memory len 598 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.17364589933937066 agent memory len 599 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 600 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 601 steps  6 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.17364589933937066 agent memory len 602 steps  7 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.17364589933937066 agent memory len 603 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.17364589933937066 agent memory len 604 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.17364589933937066 agent memory len 605 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.17364589933937066 agent memory len 606 steps  11 reward 0 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.17364589933937066 agent memory len 607 steps  12 reward 10 next state  24 agent position  (4, 4)\n",
      "total rewards 0\n",
      "Episode number:  38\n",
      "enemy states:  [18]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 608 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.16566479465049133 agent memory len 609 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 610 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 611 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.16566479465049133 agent memory len 612 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.16566479465049133 agent memory len 613 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.16566479465049133 agent memory len 614 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.16566479465049133 agent memory len 615 steps  8 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.16566479465049133 agent memory len 616 steps  9 reward 0 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.16566479465049133 agent memory len 617 steps  10 reward 0 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.16566479465049133 agent memory len 618 steps  11 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.16566479465049133 agent memory len 619 steps  12 reward 10 next state  18 agent position  (3, 3)\n",
      "total rewards 3\n",
      "Episode number:  39\n",
      "enemy states:  [17]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1580729330304087 agent memory len 620 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 621 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 622 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 623 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 624 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 625 steps  6 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1580729330304087 agent memory len 626 steps  7 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 627 steps  8 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 628 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 629 steps  10 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 630 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 631 steps  12 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 632 steps  13 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 633 steps  14 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 634 steps  15 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 635 steps  16 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 636 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 637 steps  18 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 638 steps  19 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1580729330304087 agent memory len 639 steps  20 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 640 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 641 steps  22 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 642 steps  23 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 643 steps  24 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1580729330304087 agent memory len 644 steps  25 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1580729330304087 agent memory len 645 steps  26 reward -1 next state  8 agent position  (1, 3)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  40\n",
      "enemy states:  [10]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 646 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 647 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 648 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.15085133087064842 agent memory len 649 steps  4 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 650 steps  5 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 651 steps  6 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 652 steps  7 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 653 steps  8 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 654 steps  9 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 655 steps  10 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 656 steps  11 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 657 steps  12 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 658 steps  13 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 659 steps  14 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 660 steps  15 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.15085133087064842 agent memory len 661 steps  16 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.15085133087064842 agent memory len 662 steps  17 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.15085133087064842 agent memory len 663 steps  18 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 664 steps  19 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 665 steps  20 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 666 steps  21 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 667 steps  22 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 668 steps  23 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 669 steps  24 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.15085133087064842 agent memory len 670 steps  25 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.15085133087064842 agent memory len 671 steps  26 reward -1 next state  8 agent position  (1, 3)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  41\n",
      "enemy states:  [10]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 672 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 673 steps  2 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1439819304042466 agent memory len 674 steps  3 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1439819304042466 agent memory len 675 steps  4 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1439819304042466 agent memory len 676 steps  5 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1439819304042466 agent memory len 677 steps  6 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.1439819304042466 agent memory len 678 steps  7 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.1439819304042466 agent memory len 679 steps  8 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 680 steps  9 reward 0 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 681 steps  10 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.1439819304042466 agent memory len 682 steps  11 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.1439819304042466 agent memory len 683 steps  12 reward 0 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['U']\n",
      "agent epsilon  0.1439819304042466 agent memory len 684 steps  13 reward 10 next state  10 agent position  (2, 0)\n",
      "total rewards 2\n",
      "Episode number:  42\n",
      "enemy states:  [20]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 685 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 686 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 687 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.13744755455192614 agent memory len 688 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 689 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 690 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 691 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 692 steps  8 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13744755455192614 agent memory len 693 steps  9 reward 0 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.13744755455192614 agent memory len 694 steps  10 reward 10 next state  20 agent position  (4, 0)\n",
      "total rewards 3\n",
      "Episode number:  43\n",
      "enemy states:  [17]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 695 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 696 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 697 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 698 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.13123186397045208 agent memory len 699 steps  5 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 700 steps  6 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.13123186397045208 agent memory len 701 steps  7 reward 0 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.13123186397045208 agent memory len 702 steps  8 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['U']\n",
      "agent epsilon  0.13123186397045208 agent memory len 703 steps  9 reward 10 next state  17 agent position  (3, 2)\n",
      "total rewards 9\n",
      "Episode number:  44\n",
      "enemy states:  [17]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 704 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 705 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 706 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 707 steps  4 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 708 steps  5 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 709 steps  6 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 710 steps  7 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 711 steps  8 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 712 steps  9 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.125319316195762 agent memory len 713 steps  10 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 714 steps  11 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 715 steps  12 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.125319316195762 agent memory len 716 steps  13 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 717 steps  14 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.125319316195762 agent memory len 718 steps  15 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 719 steps  16 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.125319316195762 agent memory len 720 steps  17 reward 10 next state  17 agent position  (3, 2)\n",
      "total rewards 0\n",
      "Episode number:  45\n",
      "enemy states:  [10]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 721 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 722 steps  2 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 723 steps  3 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 724 steps  4 reward -1 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 725 steps  5 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 726 steps  6 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 727 steps  7 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 728 steps  8 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 729 steps  9 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 730 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 731 steps  11 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 732 steps  12 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 733 steps  13 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 734 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 735 steps  15 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 736 steps  16 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.11969512677871053 agent memory len 737 steps  17 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 738 steps  18 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 739 steps  19 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 740 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11969512677871053 agent memory len 741 steps  21 reward -2 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 742 steps  22 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 743 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 744 steps  24 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11969512677871053 agent memory len 745 steps  25 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11969512677871053 agent memory len 746 steps  26 reward -1 next state  23 agent position  (4, 3)\n",
      "max steps reached\n",
      "total rewards -26\n",
      "Episode number:  46\n",
      "enemy states:  [15]\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 747 steps  1 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 748 steps  2 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 749 steps  3 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 750 steps  4 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 751 steps  5 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 752 steps  6 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 753 steps  7 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 754 steps  8 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 755 steps  9 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 756 steps  10 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.11434523231624569 agent memory len 757 steps  11 reward -1 next state  3 agent position  (0, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 758 steps  12 reward -1 next state  4 agent position  (0, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 759 steps  13 reward -1 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 760 steps  14 reward -1 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 761 steps  15 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 762 steps  16 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 763 steps  17 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 764 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 765 steps  19 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 766 steps  20 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 767 steps  21 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.11434523231624569 agent memory len 768 steps  22 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.11434523231624569 agent memory len 769 steps  23 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.11434523231624569 agent memory len 770 steps  24 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 771 steps  25 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.11434523231624569 agent memory len 772 steps  26 reward -1 next state  17 agent position  (3, 2)\n",
      "max steps reached\n",
      "total rewards -28\n",
      "Episode number:  47\n",
      "enemy states:  [19]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 773 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 774 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 775 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 776 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 777 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10925625528557566 agent memory len 778 steps  6 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10925625528557566 agent memory len 779 steps  7 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10925625528557566 agent memory len 780 steps  8 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 781 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 782 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10925625528557566 agent memory len 783 steps  11 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 784 steps  12 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10925625528557566 agent memory len 785 steps  13 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 786 steps  14 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10925625528557566 agent memory len 787 steps  15 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 788 steps  16 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10925625528557566 agent memory len 789 steps  17 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 790 steps  18 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 791 steps  19 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10925625528557566 agent memory len 792 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 793 steps  21 reward -2 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10925625528557566 agent memory len 794 steps  22 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10925625528557566 agent memory len 795 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 796 steps  24 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10925625528557566 agent memory len 797 steps  25 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10925625528557566 agent memory len 798 steps  26 reward -1 next state  21 agent position  (4, 1)\n",
      "max steps reached\n",
      "total rewards -26\n",
      "Episode number:  48\n",
      "enemy states:  [8]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 799 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 800 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 801 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 802 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 803 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 804 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 805 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 806 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 807 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 808 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 809 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 810 steps  12 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10441547059339411 agent memory len 811 steps  13 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 812 steps  14 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10441547059339411 agent memory len 813 steps  15 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 814 steps  16 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 815 steps  17 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 816 steps  18 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 817 steps  19 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 818 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10441547059339411 agent memory len 819 steps  21 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.10441547059339411 agent memory len 820 steps  22 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.10441547059339411 agent memory len 821 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 822 steps  24 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.10441547059339411 agent memory len 823 steps  25 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.10441547059339411 agent memory len 824 steps  26 reward -1 next state  22 agent position  (4, 2)\n",
      "max steps reached\n",
      "total rewards -24\n",
      "Episode number:  49\n",
      "enemy states:  [24]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 825 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09981077375651834 agent memory len 826 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.09981077375651834 agent memory len 827 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09981077375651834 agent memory len 828 steps  4 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 829 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 830 steps  6 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 831 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.09981077375651834 agent memory len 832 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 833 steps  9 reward -2 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.09981077375651834 agent memory len 834 steps  10 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09981077375651834 agent memory len 835 steps  11 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 836 steps  12 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 837 steps  13 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 838 steps  14 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 839 steps  15 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 840 steps  16 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 841 steps  17 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.09981077375651834 agent memory len 842 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.09981077375651834 agent memory len 843 steps  19 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09981077375651834 agent memory len 844 steps  20 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 845 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.09981077375651834 agent memory len 846 steps  22 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.09981077375651834 agent memory len 847 steps  23 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09981077375651834 agent memory len 848 steps  24 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09981077375651834 agent memory len 849 steps  25 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.09981077375651834 agent memory len 850 steps  26 reward -1 next state  22 agent position  (4, 2)\n",
      "max steps reached\n",
      "total rewards -31\n",
      "Episode number:  50\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 851 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.09543065063437678 agent memory len 852 steps  2 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 853 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.09543065063437678 agent memory len 854 steps  4 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 8\n",
      "Episode number:  51\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.0912641486376598 agent memory len 855 steps  1 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 856 steps  2 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0912641486376598 agent memory len 857 steps  3 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 858 steps  4 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.0912641486376598 agent memory len 859 steps  5 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 7\n",
      "Episode number:  52\n",
      "enemy states:  [13]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.08730084934114159 agent memory len 860 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 861 steps  2 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 862 steps  3 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 863 steps  4 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 864 steps  5 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 865 steps  6 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 866 steps  7 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.08730084934114159 agent memory len 867 steps  8 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 868 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 869 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 870 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 871 steps  12 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 872 steps  13 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 873 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 874 steps  15 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 875 steps  16 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 876 steps  17 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 877 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 878 steps  19 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 879 steps  20 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 880 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08730084934114159 agent memory len 881 steps  22 reward -2 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08730084934114159 agent memory len 882 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.08730084934114159 agent memory len 883 steps  24 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 884 steps  25 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08730084934114159 agent memory len 885 steps  26 reward -1 next state  15 agent position  (3, 0)\n",
      "max steps reached\n",
      "total rewards -26\n",
      "Episode number:  53\n",
      "enemy states:  [6]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 886 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 887 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 888 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 889 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 890 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 891 steps  6 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 892 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 893 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 894 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 895 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 896 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 897 steps  12 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 898 steps  13 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 899 steps  14 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 900 steps  15 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 901 steps  16 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 902 steps  17 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 903 steps  18 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 904 steps  19 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.08353084243219053 agent memory len 905 steps  20 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.08353084243219053 agent memory len 906 steps  21 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.08353084243219053 agent memory len 907 steps  22 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.08353084243219053 agent memory len 908 steps  23 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 909 steps  24 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 910 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.08353084243219053 agent memory len 911 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -22\n",
      "Episode number:  54\n",
      "enemy states:  [14]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 912 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07994470092982527 agent memory len 913 steps  2 reward 0 next state  8 agent position  (1, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07994470092982527 agent memory len 914 steps  3 reward 0 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.07994470092982527 agent memory len 915 steps  4 reward 10 next state  14 agent position  (2, 4)\n",
      "total rewards 9\n",
      "Episode number:  55\n",
      "enemy states:  [12]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 916 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 917 steps  2 reward 0 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 918 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 919 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 920 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 921 steps  6 reward -2 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 922 steps  7 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 923 steps  8 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 924 steps  9 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 925 steps  10 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 926 steps  11 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 927 steps  12 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 928 steps  13 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.07653345761235225 agent memory len 929 steps  14 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 930 steps  15 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 931 steps  16 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 932 steps  17 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 933 steps  18 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 934 steps  19 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 935 steps  20 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.07653345761235225 agent memory len 936 steps  21 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 937 steps  22 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07653345761235225 agent memory len 938 steps  23 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07653345761235225 agent memory len 939 steps  24 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.07653345761235225 agent memory len 940 steps  25 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07653345761235225 agent memory len 941 steps  26 reward 0 next state  11 agent position  (2, 1)\n",
      "max steps reached\n",
      "total rewards -16\n",
      "Episode number:  56\n",
      "enemy states:  [19]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 942 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 943 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0732885825946405 agent memory len 944 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 945 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0732885825946405 agent memory len 946 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 947 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 948 steps  7 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.0732885825946405 agent memory len 949 steps  8 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 950 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 951 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0732885825946405 agent memory len 952 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 953 steps  12 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.0732885825946405 agent memory len 954 steps  13 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 955 steps  14 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 956 steps  15 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 957 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 958 steps  17 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 959 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 960 steps  19 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 961 steps  20 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 962 steps  21 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 963 steps  22 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.0732885825946405 agent memory len 964 steps  23 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 965 steps  24 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 966 steps  25 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0732885825946405 agent memory len 967 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  57\n",
      "enemy states:  [8]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 968 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 969 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 970 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 971 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 972 steps  5 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 973 steps  6 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 974 steps  7 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 975 steps  8 reward -2 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 976 steps  9 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.07020196199896576 agent memory len 977 steps  10 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.07020196199896576 agent memory len 978 steps  11 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 979 steps  12 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 980 steps  13 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 981 steps  14 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 982 steps  15 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 983 steps  16 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 984 steps  17 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 985 steps  18 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 986 steps  19 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 987 steps  20 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.07020196199896576 agent memory len 988 steps  21 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 989 steps  22 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 990 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.07020196199896576 agent memory len 991 steps  24 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.07020196199896576 agent memory len 992 steps  25 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.07020196199896576 agent memory len 993 steps  26 reward -1 next state  17 agent position  (3, 2)\n",
      "max steps reached\n",
      "total rewards -24\n",
      "Episode number:  58\n",
      "enemy states:  [8]\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06726587766609007 agent memory len 994 steps  1 reward -2 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 995 steps  2 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 996 steps  3 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 997 steps  4 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 998 steps  5 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 999 steps  6 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1000 steps  7 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1001 steps  8 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1002 steps  9 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1003 steps  10 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1004 steps  11 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1005 steps  12 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1006 steps  13 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1007 steps  14 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1008 steps  15 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1009 steps  16 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1010 steps  17 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1011 steps  18 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1012 steps  19 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1013 steps  20 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1014 steps  21 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1015 steps  22 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1016 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1017 steps  24 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1018 steps  25 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06726587766609007 agent memory len 1019 steps  26 reward -1 next state  15 agent position  (3, 0)\n",
      "max steps reached\n",
      "total rewards -24\n",
      "Episode number:  59\n",
      "enemy states:  [22]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1020 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1021 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1022 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1023 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1024 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1025 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1026 steps  7 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.06447298785584314 agent memory len 1027 steps  8 reward 10 next state  22 agent position  (4, 2)\n",
      "total rewards 5\n",
      "Episode number:  60\n",
      "enemy states:  [13]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1028 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1029 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1030 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1031 steps  4 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1032 steps  5 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1033 steps  6 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1034 steps  7 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1035 steps  8 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1036 steps  9 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1037 steps  10 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1038 steps  11 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1039 steps  12 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1040 steps  13 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1041 steps  14 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1042 steps  15 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1043 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1044 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1045 steps  18 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1046 steps  19 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1047 steps  20 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1048 steps  21 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1049 steps  22 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1050 steps  23 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1051 steps  24 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1052 steps  25 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.06181630888894806 agent memory len 1053 steps  26 reward 0 next state  18 agent position  (3, 3)\n",
      "max steps reached\n",
      "total rewards -22\n",
      "Episode number:  61\n",
      "enemy states:  [15]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1054 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1055 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1056 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1057 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1058 steps  5 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1059 steps  6 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.05928919768418531 agent memory len 1060 steps  7 reward 10 next state  15 agent position  (3, 0)\n",
      "total rewards 7\n",
      "Episode number:  62\n",
      "enemy states:  [21]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1061 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1062 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1063 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1064 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1065 steps  5 reward 0 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1066 steps  6 reward 0 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['R']\n",
      "agent epsilon  0.0568853351472295 agent memory len 1067 steps  7 reward 10 next state  21 agent position  (4, 1)\n",
      "total rewards 7\n",
      "Episode number:  63\n",
      "enemy states:  [12]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1068 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.05459871036962222 agent memory len 1069 steps  2 reward 10 next state  12 agent position  (2, 2)\n",
      "total rewards 10\n",
      "Episode number:  64\n",
      "enemy states:  [19]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1070 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1071 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1072 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1073 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1074 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1075 steps  6 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1076 steps  7 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1077 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1078 steps  9 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1079 steps  10 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1080 steps  11 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1081 steps  12 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1082 steps  13 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1083 steps  14 reward -1 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1084 steps  15 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1085 steps  16 reward -1 next state  0 agent position  (0, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1086 steps  17 reward -1 next state  5 agent position  (1, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1087 steps  18 reward -1 next state  6 agent position  (1, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1088 steps  19 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1089 steps  20 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1090 steps  21 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1091 steps  22 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1092 steps  23 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1093 steps  24 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1094 steps  25 reward 0 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.05242360559836977 agent memory len 1095 steps  26 reward 0 next state  24 agent position  (4, 4)\n",
      "max steps reached\n",
      "total rewards -24\n",
      "Episode number:  65\n",
      "enemy states:  [7]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.05035458193858255 agent memory len 1096 steps  1 reward 10 next state  7 agent position  (1, 2)\n",
      "total rewards 10\n",
      "Episode number:  66\n",
      "enemy states:  [16]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1097 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1098 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1099 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['D']\n",
      "agent epsilon  0.04838646575340479 agent memory len 1100 steps  4 reward 10 next state  16 agent position  (3, 1)\n",
      "total rewards 9\n",
      "Episode number:  67\n",
      "enemy states:  [15]\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1101 steps  1 reward -1 next state  1 agent position  (0, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1102 steps  2 reward -1 next state  2 agent position  (0, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1103 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1104 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1105 steps  5 reward -1 next state  13 agent position  (2, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1106 steps  6 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1107 steps  7 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1108 steps  8 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1109 steps  9 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1110 steps  10 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1111 steps  11 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1112 steps  12 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1113 steps  13 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1114 steps  14 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1115 steps  15 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1116 steps  16 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1117 steps  17 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1118 steps  18 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1119 steps  19 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1120 steps  20 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1121 steps  21 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1122 steps  22 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1123 steps  23 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1124 steps  24 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1125 steps  25 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.046514335727227595 agent memory len 1126 steps  26 reward -1 next state  24 agent position  (4, 4)\n",
      "max steps reached\n",
      "total rewards -25\n",
      "Episode number:  68\n",
      "enemy states:  [10]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1127 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1128 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1129 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1130 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1131 steps  5 reward 0 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1132 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1133 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1134 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1135 steps  9 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1136 steps  10 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1137 steps  11 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1138 steps  12 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1139 steps  13 reward -2 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1140 steps  14 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1141 steps  15 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1142 steps  16 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1143 steps  17 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1144 steps  18 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1145 steps  19 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1146 steps  20 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1147 steps  21 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1148 steps  22 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1149 steps  23 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1150 steps  24 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1151 steps  25 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04473351055983658 agent memory len 1152 steps  26 reward -1 next state  23 agent position  (4, 3)\n",
      "max steps reached\n",
      "total rewards -23\n",
      "Episode number:  69\n",
      "enemy states:  [11]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1153 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1154 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.04303953726072281 agent memory len 1155 steps  3 reward 10 next state  11 agent position  (2, 1)\n",
      "total rewards 10\n",
      "Episode number:  70\n",
      "enemy states:  [17]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1156 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1157 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1158 steps  3 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1159 steps  4 reward 0 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1160 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1161 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1162 steps  7 reward 0 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1163 steps  8 reward 0 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['U']\n",
      "agent epsilon  0.04142818001428726 agent memory len 1164 steps  9 reward 10 next state  17 agent position  (3, 2)\n",
      "total rewards 7\n",
      "Episode number:  71\n",
      "enemy states:  [8]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1165 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1166 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1167 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1168 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1169 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1170 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1171 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1172 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1173 steps  9 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1174 steps  10 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1175 steps  11 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1176 steps  12 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1177 steps  13 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1178 steps  14 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1179 steps  15 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1180 steps  16 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1181 steps  17 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1182 steps  18 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1183 steps  19 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1184 steps  20 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1185 steps  21 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1186 steps  22 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1187 steps  23 reward -1 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1188 steps  24 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1189 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.039895409588095315 agent memory len 1190 steps  26 reward -1 next state  16 agent position  (3, 1)\n",
      "max steps reached\n",
      "total rewards -23\n",
      "Episode number:  72\n",
      "enemy states:  [11]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1191 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 4\n",
      "actions ['S']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1192 steps  2 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1193 steps  3 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "agent reached landmark\n",
      "updating target model\n",
      "actions ['L']\n",
      "agent epsilon  0.03843739325769703 agent memory len 1194 steps  4 reward 10 next state  11 agent position  (2, 1)\n",
      "total rewards 10\n",
      "Episode number:  73\n",
      "enemy states:  [13]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1195 steps  1 reward 0 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1196 steps  2 reward 0 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1197 steps  3 reward -1 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1198 steps  4 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1199 steps  5 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1200 steps  6 reward -1 next state  20 agent position  (4, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1201 steps  7 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1202 steps  8 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1203 steps  9 reward -1 next state  23 agent position  (4, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1204 steps  10 reward -1 next state  24 agent position  (4, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1205 steps  11 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1206 steps  12 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1207 steps  13 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1208 steps  14 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1209 steps  15 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1210 steps  16 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1211 steps  17 reward 0 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1212 steps  18 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1213 steps  19 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1214 steps  20 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1215 steps  21 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1216 steps  22 reward 0 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1217 steps  23 reward 0 next state  19 agent position  (3, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1218 steps  24 reward 0 next state  14 agent position  (2, 4)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1219 steps  25 reward 0 next state  9 agent position  (1, 4)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03705048522281963 agent memory len 1220 steps  26 reward 0 next state  14 agent position  (2, 4)\n",
      "max steps reached\n",
      "total rewards -8\n",
      "Episode number:  74\n",
      "enemy states:  [5]\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1221 steps  1 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1222 steps  2 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1223 steps  3 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1224 steps  4 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1225 steps  5 reward -1 next state  7 agent position  (1, 2)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1226 steps  6 reward -1 next state  12 agent position  (2, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1227 steps  7 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1228 steps  8 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1229 steps  9 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1230 steps  10 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1231 steps  11 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1232 steps  12 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1233 steps  13 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1234 steps  14 reward 0 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1235 steps  15 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1236 steps  16 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1237 steps  17 reward -1 next state  15 agent position  (3, 0)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1238 steps  18 reward 0 next state  10 agent position  (2, 0)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1239 steps  19 reward 0 next state  11 agent position  (2, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1240 steps  20 reward -1 next state  16 agent position  (3, 1)\n",
      "action_space_dict[actions[agent.index]] 1\n",
      "actions ['D']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1241 steps  21 reward -1 next state  21 agent position  (4, 1)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1242 steps  22 reward -1 next state  22 agent position  (4, 2)\n",
      "action_space_dict[actions[agent.index]] 0\n",
      "actions ['U']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1243 steps  23 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1244 steps  24 reward -1 next state  18 agent position  (3, 3)\n",
      "action_space_dict[actions[agent.index]] 2\n",
      "actions ['L']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1245 steps  25 reward -1 next state  17 agent position  (3, 2)\n",
      "action_space_dict[actions[agent.index]] 3\n",
      "actions ['R']\n",
      "agent epsilon  0.03573121749096778 agent memory len 1246 steps  26 reward -1 next state  18 agent position  (3, 3)\n",
      "max steps reached\n",
      "total rewards -20\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
